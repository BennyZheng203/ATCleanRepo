{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-Square Cut, Uncertainty Cut, Control Light Curve Cut, and Averaging\n",
    "\n",
    "This iPython notebook will help you ascertain the best chi-square cut for a certain supernova's ATLAS light curve, then average together the light curve after specifying the MJD bin size. After running a cell, the descriptions located above that cell will help you interpret the plots and make decisions about the supernova.\n",
    "\n",
    "This notebook takes into account ATLAS's periodic replacement of the difference image reference templates, which may cause step discontinuities in flux. Two template changes have been recorded at MJDs 58417 and 58882. More information can be found here: https://fallingstar-data.com/forcedphot/faq/.\n",
    "\n",
    "In order for this notebook to work correctly, the ATLAS light curve must already be downloaded and saved. It must also only include measurements for a single filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the ATLAS light curve, account for template changes, etc.\n",
    "\n",
    "Before analysis on the chi-squares of the light curve can be done, we correct for any potential flux in the template. We do this by calculating the median of any baseline flux with a chi-square less than or equal to 5 and then subtracting that median from the entire light curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules, set preliminary variables, etc.\n",
    "\n",
    "import sys, re\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import sigmacut\n",
    "\n",
    "# storing, accessing, and manipulating the light curve\n",
    "import pandas as pd\n",
    "from pdastro import pdastrostatsclass, AandB, AnotB, AorB, not_AandB\n",
    "\n",
    "# getting discovery date from TNS\n",
    "import requests, json\n",
    "from collections import OrderedDict\n",
    "from astropy.time import Time\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as matlib\n",
    "import warnings\n",
    "warnings.simplefilter('error', RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# plotting styles\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=13)\n",
    "plt.rc('ytick', labelsize=13)\n",
    "plt.rc('legend', fontsize=13)\n",
    "plt.rc('font', size=13)\n",
    "#plt.rcParams['font.family'] = 'sans-serif'\n",
    "#plt.rcParams['font.serif'] = 'Times'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.style.use('bmh')\n",
    "\n",
    "# ATLAS template changes\n",
    "global tchange1\n",
    "global tchange2\n",
    "tchange1 = 58417\n",
    "tchange2 = 58882\n",
    "\n",
    "# dictionary for storing light curve and other important information\n",
    "global lc_info\n",
    "lc_info = {}\n",
    "\n",
    "# dictionary for optionally storing control light curves\n",
    "global controls\n",
    "controls = {}\n",
    "\n",
    "# flag values for updating 'Mask' column with\n",
    "flag_chisquare = 0x1\n",
    "flag_uncertainty = 0x2\n",
    "flag_controls_bad = 0x400000\n",
    "flag_controls_questionable = 0x80000\n",
    "flag_controls_x2 = 0x100\n",
    "flag_controls_stn = 0x200\n",
    "flag_controls_Nclip = 0x400\n",
    "flag_controls_Ngood = 0x800\n",
    "flag_badday = 0x800000\n",
    "flag_ixclip = 0x1000\n",
    "flag_smallnum = 0x2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the target SN name:\n",
    "tnsname = '2020ohl'\n",
    "\n",
    "# Enter the SN light curve file name:\n",
    "filename = '/Users/sofiarest/Desktop/Supernovae/data/new_test/2020ohl/2020ohl.o.lc.txt'\n",
    "\n",
    "# Enter the filter for this light curve (must be 'o' or 'c'):\n",
    "filter = 'o'\n",
    "\n",
    "# Optionally, enter the SN's discovery date (if None is entered, it will be \n",
    "# fetched automatically from TNS using api_key):\n",
    "discdate = 59039\n",
    "api_key = None\n",
    "\n",
    "# Enter the number of minimum days between a template change date and the SN discovery date \n",
    "# in order to use this data as baseline (meaning before SN starts) flux for that template region:\n",
    "Ndays_min = 6\n",
    "\n",
    "# Optionally, manually enter the x limits for the flux (µJy) distribution histograms:\n",
    "fdf_xlim_lower = None\n",
    "fdf_xlim_upper = None\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the before and after plots of the light curve\n",
    "# when adjusting for template changes:\n",
    "xlim_lower = 58900\n",
    "xlim_upper = 59800\n",
    "ylim_lower = -300\n",
    "ylim_upper = 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL LIGHT CURVES SETTINGS FOR LOADING\n",
    "\n",
    "# Set to True if you are planning on applying the control light curve cut \n",
    "# and have already downloaded the control light curves:\n",
    "load_controls = True\n",
    "\n",
    "# Enter the number of control light curves to load:\n",
    "Ncontrols = 8\n",
    "\n",
    "# Enter the source directory of the control light curve files:\n",
    "controls_dir = '/Users/sofiarest/Desktop/Supernovae/data/new_test/2020ohl/controls/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get discovery date if needed, load in light curve, account for template changes, \n",
    "# and add uJy/duJy column\n",
    "\n",
    "def get_tns_data(tnsname, api_key):\n",
    "\ttry:\n",
    "\t\tget_obj = [(\"objname\",tnsname), (\"objid\",\"\"), (\"photometry\",\"1\"), (\"spectra\",\"1\")]\n",
    "\t\tget_url = 'https://www.wis-tns.org/api/get/object'\n",
    "\t\tjson_file = OrderedDict(get_obj)\n",
    "\t\tget_data = {'api_key':api_key,'data':json.dumps(json_file)}\n",
    "\t\tresponse = requests.post(get_url, data=get_data, headers={'User-Agent':'tns_marker{\"tns_id\":104739,\"type\": \"bot\", \"name\":\"Name and Redshift Retriever\"}'})\n",
    "\t\tjson_data = json.loads(response.text,object_pairs_hook=OrderedDict)\n",
    "\t\treturn json_data\n",
    "\texcept Exception as e:\n",
    "\t\treturn 'Error: \\n'+str(e)\n",
    "\n",
    "def get_discdate(tnsname, api_key):\n",
    "\tjson_data = get_tns_data(tnsname, api_key)\n",
    "\tdiscoverydate = json_data['data']['reply']['discoverydate']\n",
    "\tdate = list(discoverydate.partition(' '))[0]\n",
    "\ttime = list(discoverydate.partition(' '))[2]\n",
    "\tdisc_date_format = date+'T'+time\n",
    "\tdateobjects = Time(disc_date_format, format='isot', scale='utc')\n",
    "\tdisc_date = dateobjects.mjd\n",
    "\treturn disc_date\n",
    "\n",
    "def get_xth_percentile_flux(lc_type, percentile, indices=None):\n",
    "    if indices is None:\n",
    "        indices = lc_info[lc_type].getindices()\n",
    "    if len(indices)==0: \n",
    "        return None\n",
    "    else:\n",
    "        return np.percentile(lc_info[lc_type].t.loc[indices, 'uJy'], percentile)\n",
    "\n",
    "def save_lc(lc_type, filename, overwrite=False):\n",
    "    print('Saving light curve at %s' % filename)\n",
    "    lc_info[lc_type].write(filename,overwrite=overwrite)\n",
    "\n",
    "def plot_lc(add2title=None, dflux_colname='duJy', xlim_lower=None, xlim_upper=None, ylim_lower=None, ylim_upper=None):\n",
    "    fig = plt.figure(figsize=(12,8), tight_layout=True)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.axhline(linewidth=1,color='k')\n",
    "    plt.ylabel('Flux (µJy)')\n",
    "    plt.xlabel('MJD')\n",
    "    title = 'SN %s %s-band flux' % (lc_info['tnsname'],lc_info['filter'])\n",
    "    if not(add2title is None):\n",
    "        title += add2title\n",
    "    plt.title(title)\n",
    "    plt.axvline(x=tchange1,color='magenta', label='ATLAS template change')\n",
    "    plt.axvline(x=tchange2,color='magenta')\n",
    "\n",
    "    color = 'orange' if lc_info['filter'] == 'o' else 'cyan'\n",
    "\n",
    "    # set x and y limits\n",
    "    if xlim_lower is None: xlim_lower = lc_info['lc'].t['MJD'].min() * 0.999\n",
    "    if xlim_upper is None: xlim_upper = lc_info['lc'].t['MJD'].max() * 1.001\n",
    "    if ylim_lower is None: ylim_lower = lc_info['lc'].t['uJy'].min()\n",
    "    if ylim_upper is None: ylim_upper = lc_info['lc'].t['uJy'].max()\n",
    "    plt.xlim(xlim_lower,xlim_upper)\n",
    "    plt.ylim(ylim_lower,ylim_upper)\n",
    "\n",
    "    plt.errorbar(lc_info['lc'].t.loc[lc_info['baseline_revised_i'],'MJD'], lc_info['lc'].t.loc[lc_info['baseline_revised_i'],'uJy'], yerr=lc_info['lc'].t.loc[lc_info['baseline_revised_i'],dflux_colname], fmt='none',ecolor=color,elinewidth=1,c=color)\n",
    "    plt.scatter(lc_info['lc'].t.loc[lc_info['baseline_revised_i'],'MJD'], lc_info['lc'].t.loc[lc_info['baseline_revised_i'],'uJy'], s=45,color=color,marker='o',label='Baseline')\n",
    "    \n",
    "    plt.errorbar(lc_info['lc'].t.loc[lc_info['duringsn_i'],'MJD'], lc_info['lc'].t.loc[lc_info['duringsn_i'],'uJy'], lc_info['lc'].t.loc[lc_info['duringsn_i'],dflux_colname], fmt='none',ecolor='red',elinewidth=1,c='red')\n",
    "    plt.scatter(lc_info['lc'].t.loc[lc_info['duringsn_i'],'MJD'], lc_info['lc'].t.loc[lc_info['duringsn_i'],'uJy'], s=45,color='red',marker='o',label='During SN')\n",
    "    \n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=3)\n",
    "\n",
    "\n",
    "def plot_cut_lc(lc_type, mask, add2title=None, dflux_colname='duJy', xlim_lower=None, xlim_upper=None, ylim_lower=None, ylim_upper=None):\n",
    "    good_ix = lc_info[lc_type].ix_unmasked('Mask',maskval=mask)\n",
    "    bad_ix = AnotB(lc_info[lc_type].getindices(),good_ix)\n",
    "\n",
    "    fig, (cut, clean) = plt.subplots(1, 2, figsize=(16, 6.5), tight_layout=True)\n",
    "    title = 'SN %s %s-band' % (lc_info['tnsname'], lc_info['filter'])\n",
    "    if lc_type == 'avglc':\n",
    "        title += ', averaged'\n",
    "    if not(add2title is None):\n",
    "        title += ', '+add2title\n",
    "    plt.suptitle(title, fontsize=19, y=1)\n",
    "\n",
    "    color = 'orange' if lc_info['filter'] == 'o' else 'cyan'\n",
    "    \n",
    "    if ylim_lower is None: ylim_lower = -2000\n",
    "    if ylim_upper is None: \n",
    "        afterdiscdate_i = lc_info[lc_type].ix_inrange(colnames=['MJD'],lowlim=lc_info['discdate']) if lc_type == 'avglc' else lc_info['afterdiscdate_i']\n",
    "        ylim_upper = scale*get_xth_percentile_flux(lc_type, 95, afterdiscdate_i)\n",
    "    if xlim_lower is None: xlim_lower = lc_info['discdate'] - 200\n",
    "    if xlim_upper is None: xlim_upper = lc_info['discdate'] + 800\n",
    "    cut.set_ylim(ylim_lower, ylim_upper)\n",
    "    cut.set_xlim(xlim_lower,xlim_upper)\n",
    "    clean.set_ylim(ylim_lower, ylim_upper)\n",
    "    clean.set_xlim(xlim_lower,xlim_upper)\n",
    "\n",
    "    cut.spines.right.set_visible(False)\n",
    "    cut.spines.top.set_visible(False)\n",
    "    cut.errorbar(lc_info[lc_type].t.loc[good_ix,'MJD'], lc_info[lc_type].t.loc[good_ix,'uJy'], yerr=lc_info[lc_type].t.loc[good_ix,dflux_colname], fmt='none',ecolor=color,elinewidth=1,c=color)\n",
    "    cut.scatter(lc_info[lc_type].t.loc[good_ix,'MJD'], lc_info[lc_type].t.loc[good_ix,'uJy'], s=50,color=color,marker='o',label='Kept measurements')\n",
    "    cut.errorbar(lc_info[lc_type].t.loc[bad_ix,'MJD'], lc_info[lc_type].t.loc[bad_ix,'uJy'], yerr=lc_info[lc_type].t.loc[bad_ix,dflux_colname], fmt='none',mfc='white',ecolor=color,elinewidth=1,c=color)\n",
    "    cut.scatter(lc_info[lc_type].t.loc[bad_ix,'MJD'], lc_info[lc_type].t.loc[bad_ix,'uJy'], s=50,facecolors='white',edgecolors=color,marker='o',label='Cut measurements')\n",
    "    cut.set_title('All measurements')\n",
    "    cut.axhline(linewidth=1,color='k')\n",
    "    cut.set_xlabel('MJD')\n",
    "    cut.set_ylabel('Flux (uJy)')\n",
    "\n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.5, 0),ncol=2)\n",
    "\n",
    "    clean.spines.right.set_visible(False)\n",
    "    clean.spines.top.set_visible(False)\n",
    "    clean.errorbar(lc_info[lc_type].t.loc[good_ix,'MJD'], lc_info[lc_type].t.loc[good_ix,'uJy'], yerr=lc_info[lc_type].t.loc[good_ix,dflux_colname], fmt='none',ecolor=color,elinewidth=1,c=color)\n",
    "    clean.scatter(lc_info[lc_type].t.loc[good_ix,'MJD'], lc_info[lc_type].t.loc[good_ix,'uJy'], s=50,color=color,marker='o',label='Kept measurements')\n",
    "    clean.set_title('Kept measurements only')\n",
    "    clean.axhline(linewidth=1,color='k')\n",
    "    clean.set_xlabel('MJD')\n",
    "    clean.set_ylabel('Flux (uJy)')\n",
    "    clean.set_ylim(ylim_lower, ylim_upper)\n",
    "\n",
    "def plot_control_lcs(add2title=None, dflux_colname='duJy', xlim_lower=None, xlim_upper=None, ylim_lower=None, ylim_upper=None):\n",
    "    color = 'orange' if lc_info[\"filter\"] == 'o' else 'cyan'\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8), tight_layout=True)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.axhline(linewidth=1,color='k')\n",
    "    plt.ylabel('Flux (µJy)')\n",
    "    plt.xlabel('MJD')\n",
    "    title = f'SN {lc_info[\"tnsname\"]} and control light curves {lc_info[\"filter\"]}-band flux'\n",
    "    if not(add2title is None):\n",
    "        title += add2title\n",
    "    plt.title(title)\n",
    "    plt.axvline(x=tchange1, color='magenta', label='ATLAS template change')\n",
    "    plt.axvline(x=tchange2, color='magenta')\n",
    "\n",
    "    # set x and y limits\n",
    "    if xlim_lower is None: xlim_lower = lc_info['lc'].t['MJD'].min() * 0.999\n",
    "    if xlim_upper is None: xlim_upper = lc_info['lc'].t['MJD'].max() * 1.001\n",
    "    if ylim_lower is None: ylim_lower = lc_info['lc'].t['uJy'].min()\n",
    "    if ylim_upper is None: ylim_upper = lc_info['lc'].t['uJy'].max()\n",
    "    plt.xlim(xlim_lower,xlim_upper)\n",
    "    plt.ylim(ylim_lower,ylim_upper)\n",
    "\n",
    "    for control_index in range(1,Ncontrols+1):\n",
    "        plt.errorbar(controls[control_index].t['MJD'], controls[control_index].t['uJy'], yerr=controls[control_index].t[dflux_colname], fmt='none',ecolor='blue',elinewidth=1,c='blue')\n",
    "        if control_index == 1:\n",
    "            plt.scatter(controls[control_index].t['MJD'], controls[control_index].t['uJy'], s=45,color='blue',marker='o',label=f'{Ncontrols} control light curves')\n",
    "        else:\n",
    "            plt.scatter(controls[control_index].t['MJD'], controls[control_index].t['uJy'], s=45,color='blue',marker='o')\n",
    "\n",
    "    plt.errorbar(lc_info['lc'].t.loc[lc_info['baseline_revised_i'],'MJD'], lc_info['lc'].t.loc[lc_info['baseline_revised_i'],'uJy'], yerr=lc_info['lc'].t.loc[lc_info['baseline_revised_i'],dflux_colname], fmt='none',ecolor=color,elinewidth=1,c=color)\n",
    "    plt.scatter(lc_info['lc'].t.loc[lc_info['baseline_revised_i'],'MJD'],lc_info['lc'].t.loc[lc_info['baseline_revised_i'],'uJy'], s=45,color=color,marker='o',label='Baseline')\n",
    "\t\n",
    "    plt.errorbar(lc_info['lc'].t.loc[lc_info['duringsn_i'],'MJD'], lc_info['lc'].t.loc[lc_info['duringsn_i'],'uJy'], lc_info['lc'].t.loc[lc_info['duringsn_i'],dflux_colname], fmt='none',ecolor='red',elinewidth=1,c='red')\n",
    "    plt.scatter(lc_info['lc'].t.loc[lc_info['duringsn_i'],'MJD'], lc_info['lc'].t.loc[lc_info['duringsn_i'],'uJy'], s=45,color='red',marker='o',label='During SN')\n",
    "\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=2)\n",
    "\n",
    "def verify_mjds(Ncontrols):\n",
    "    # sort sn lc by mjd\n",
    "    mjd_sorted_i = lc_info['lc'].ix_sort_by_cols('MJD')\n",
    "    lc_info['lc'].t = lc_info['lc'].t.loc[mjd_sorted_i]\n",
    "    sn_sorted = lc_info['lc'].t.loc[mjd_sorted_i,'MJD'].to_numpy()\n",
    "\n",
    "    for control_index in range(1,Ncontrols+1):\n",
    "        # sort control lc by mjd\n",
    "        mjd_sorted_i = controls[control_index].ix_sort_by_cols('MJD')\n",
    "        control_sorted = controls[control_index].t.loc[mjd_sorted_i,'MJD'].to_numpy()\n",
    "        \n",
    "        # compare control lc to sn lc and, if out of agreement, fix\n",
    "        if (len(sn_sorted) != len(control_sorted)) or (np.array_equal(sn_sorted, control_sorted) is False):\n",
    "            print('\\nMJDs out of agreement for control light curve %03d, fixing...' % control_index)\n",
    "\n",
    "            mjds_onlysn = AnotB(sn_sorted, control_sorted)\n",
    "            mjds_onlycontrol = AnotB(control_sorted, sn_sorted)\n",
    "\n",
    "            # for the mjds only in sn, add row with that mjd to control lc, with all values of other columns NaN\n",
    "            if len(mjds_onlysn) > 0:\n",
    "                print('# Adding %d NaN rows to control light curve...' % len(mjds_onlysn))\n",
    "                for mjd in mjds_onlysn:\n",
    "                    controls[control_index].newrow({'MJD':mjd,'Mask':0})\n",
    "            \n",
    "            # remove indices of rows in control lc for which there is no mjd in the sn lc\n",
    "            if len(mjds_onlycontrol) > 0:\n",
    "                print('# Removing %d control light curve rows without matching SN rows...' % len(mjds_onlycontrol))\n",
    "                indices2skip = []\n",
    "                for mjd in mjds_onlycontrol:\n",
    "                    ix = controls[control_index].ix_equal('MJD',mjd)\n",
    "                    if len(ix)!=1:\n",
    "                        raise RuntimeError(f'# Couldn\\'t find MJD={mjd} in column MJD, but should be there!')\n",
    "                    indices2skip.extend(ix)\n",
    "                indices = AnotB(controls[control_index].getindices(),indices2skip)\n",
    "            else:\n",
    "                indices = controls[control_index].getindices()\n",
    "            \n",
    "            ix_sorted = controls[control_index].ix_sort_by_cols('MJD',indices=indices)\n",
    "            controls[control_index].t = controls[control_index].t.loc[ix_sorted]\n",
    "    print('\\nFinished sorting SN and control light curves')\n",
    "\n",
    "def load_control_lcs(controls_dir, Ncontrols): \n",
    "\tfor control_index in range(1,Ncontrols+1):\n",
    "\t\tcontrols[control_index] = pdastrostatsclass()\n",
    "\t\tfilename = controls_dir + lc_info['tnsname'] + '_i%03d.'%control_index + lc_info['filter'] + '.lc.txt'\n",
    "\t\tprint('Loading control light curve at ',filename)\n",
    "\t\tcontrols[control_index].load_spacesep(filename,delim_whitespace=True)\n",
    "\n",
    "\t\t# clear any previous control light curve flags\n",
    "\t\tcontrols[control_index].t['Mask'] = np.bitwise_and(controls[control_index].t['Mask'],(flag_chisquare|flag_uncertainty))\n",
    "\n",
    "def get_Ndays(SN_region_index):\n",
    "\treturn 200 if SN_region_index == 2 else 40\n",
    "\n",
    "def set_baseline_region(regions, region_index, start_override=None, end_override=None):\n",
    "\tif not(start_override is None) and not(end_override) is None:\n",
    "\t\tregions['b_t%d'%region_index] = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=start_override, uplim=end_override)\n",
    "\telif not(start_override is None):\n",
    "\t\tregions['b_t%d'%region_index] = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=start_override, uplim=lc_info['lc'].t.loc[regions['b_t%d'%region_index][-1],'MJD'])\n",
    "\telif not(end_override is None):\n",
    "\t\tregions['b_t%d'%region_index] = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=lc_info['lc'].t.loc[regions['b_t%d'%region_index][0],'MJD'], uplim=end_override)\n",
    "\treturn regions\n",
    "\n",
    "def set_baseline_regions(regions):\n",
    "\tfor region_index in range(0,3):\n",
    "\t\tstart_override = input('Override template region b_t%d START MJD (n to skip): ' % region_index)\n",
    "\t\tend_override = input('Override template region b_t%d END MJD (n to skip): ' % region_index)\n",
    "\n",
    "\t\tif start_override.isdigit() and end_override.isdigit():\n",
    "\t\t\tprint('# Overriding START for template region b_t%d with %f and END with %f: ' % (region_index,float(start_override),float(end_override)))\n",
    "\t\t\tregions = set_baseline_region(regions, region_index, start_override=float(start_override), end_override=float(end_override))\n",
    "\t\telif start_override.isdigit():\n",
    "\t\t\tprint('# Overriding START for template region b_t%d with %f: ' % (region_index,float(start_override)))\n",
    "\t\t\tregions = set_baseline_region(regions, region_index, start_override=float(start_override))\n",
    "\t\telif end_override.isdigit():\n",
    "\t\t\tprint('# Overriding END for template region b_t%d with %f: ' % (region_index,float(end_override)))\n",
    "\t\t\tregions = set_baseline_region(regions, region_index, end_override=float(end_override))\n",
    "\t\telse:\n",
    "\t\t\tprint('# Skipping region... ')\n",
    "\n",
    "\tlc_info['baseline_revised_i'] = np.concatenate([regions['b_t0'],regions['b_t1'],regions['b_t2']])\n",
    "\tlc_info['duringsn_i'] = AnotB(lc_info['lc'].getindices(),lc_info['baseline_revised_i'])\n",
    "\n",
    "\treturn regions\n",
    "\n",
    "def get_baseline_regions(Ndays_min):\n",
    "\tprint('Getting region indices around SN... ')\n",
    "\tregions = {}\n",
    "\tregions['t0']   = lc_info['lc'].ix_inrange(colnames=['MJD'],                  uplim=tchange1)\n",
    "\tregions['t1']   = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=tchange1, uplim=tchange2)\n",
    "\tregions['t2']   = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=tchange2)\n",
    "\tregions['b_t0'] = AandB(regions['t0'], lc_info['baseline_i'])\n",
    "\tregions['b_t1'] = AandB(regions['t1'], lc_info['baseline_i'])\n",
    "\tregions['b_t2'] = AandB(regions['t2'], lc_info['baseline_i'])\n",
    "\n",
    "\t# find region SN starts in \n",
    "\t\"\"\"\n",
    "\tSN_region_index = None\n",
    "\tfor region_index in range(0,3):\n",
    "\t\tregion_i = regions['t%d' % region_index]\n",
    "\t\tif lc_info['discdate'] >= lc_info['lc'].t.loc[region_i[0],'MJD'] and lc_info['discdate'] <= lc_info['lc'].t.loc[region_i[-1],'MJD']:\n",
    "\t\t\tSN_region_index = region_index\n",
    "\tif SN_region_index is None:\n",
    "\t\tprint('# ERROR: could not find region with SN discovery date; something is wrong, exiting... ')\n",
    "\t\tsys.exit()\n",
    "\telse:\n",
    "\t\tprint('# SN discovery date located in template region t%d' % SN_region_index)\n",
    "\t\"\"\"\n",
    "\t# find region SN starts in \n",
    "\tSN_region_index = None\n",
    "\tif lc_info['discdate']<= tchange1:\n",
    "\t\tSN_region_index = 0\n",
    "\telif lc_info['discdate'] > tchange1 and lc_info['discdate'] <= tchange2:\n",
    "\t\tSN_region_index = 1\n",
    "\telif lc_info['discdate'] > tchange2:\n",
    "\t\tSN_region_index = 2\n",
    "\tif SN_region_index is None:\n",
    "\t\traise RuntimeError('# ERROR: Something went wrong--could not find region with SN discovery date!')\n",
    "\telse:\n",
    "\t\tprint('# SN discovery date located in template region t%d' % SN_region_index)\n",
    "\n",
    "\t# for region with tail end of the SN, get last Ndays days and classify as baseline\n",
    "\tadjust_region_index = SN_region_index\n",
    "\tif adjust_region_index < 2 and len(regions['b_t%d'%adjust_region_index]) >= Ndays_min:\n",
    "\t\tadjust_region_index += 1\n",
    "\tif len(regions['b_t%d'%adjust_region_index]) < Ndays_min:\n",
    "\t\tprint('# Getting baseline flux for template region t%d by obtaining last %d days of region... ' % (adjust_region_index, get_Ndays(adjust_region_index)))\n",
    "\t\tregions['b_t%d'%adjust_region_index] = lc_info['lc'].ix_inrange(colnames=['MJD'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlowlim=lc_info['lc'].t.loc[regions['t%d'%adjust_region_index][-1],'MJD']- get_Ndays(adjust_region_index),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tuplim=lc_info['lc'].t.loc[regions['t%d'%adjust_region_index][-1],'MJD'])\n",
    "\tif adjust_region_index < 1: regions['b_t1'] = regions['t1']\n",
    "\tif adjust_region_index < 2: regions['b_t2'] = regions['t2']\n",
    "\n",
    "\tfor region_index in range(0,3):\n",
    "\t\tif len(regions['t%d'%region_index]) > 0:\n",
    "\t\t\tprint('# TEMPLATE REGION t%d MJD RANGE: %0.2f - %0.2f' % (region_index, lc_info['lc'].t.loc[regions['t%d'%region_index][0],'MJD'], lc_info['lc'].t.loc[regions['t%d'%region_index][-1],'MJD']))\n",
    "\t\telse:\n",
    "\t\t\tprint('# TEMPLATE REGION t%d MJD RANGE: not found' % region_index)\n",
    "\t\tif len(regions['b_t%d'%region_index]) > 0:\n",
    "\t\t\tprint('# TEMPLATE REGION b_t%d BASELINE MJD RANGE: %0.2f - %0.2f' % (region_index, lc_info['lc'].t.loc[regions['b_t%d'%region_index][0],'MJD'], lc_info['lc'].t.loc[regions['b_t%d'%region_index][-1],'MJD']))\n",
    "\t\telse:\n",
    "\t\t\tprint('# TEMPLATE REGION b_t%d BASELINE MJD RANGE: not found' % region_index)\n",
    "\n",
    "\t# check to make sure baseline flux is still consistent by getting median of first and last halves of affected region\n",
    "\tfirst_i = regions['b_t%d'%adjust_region_index][0]\n",
    "\tmid_i   = regions['b_t%d'%adjust_region_index][int(len(regions['b_t%d'%adjust_region_index])/2)]\n",
    "\tlast_i  = regions['b_t%d'%adjust_region_index][-1]\n",
    "\tmedian1 = np.median(lc_info['lc'].t.loc[lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=lc_info['lc'].t.loc[first_i,'MJD'], uplim=lc_info['lc'].t.loc[mid_i,'MJD']), 'uJy'])\n",
    "\tmedian2 = np.median(lc_info['lc'].t.loc[lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=lc_info['lc'].t.loc[mid_i+1,'MJD'], uplim=lc_info['lc'].t.loc[last_i,'MJD']), 'uJy'])\n",
    "\tprint('# Checking that baseline flux is consistent throughout adjusted region...\\n## Median of first half: %0.2f\\n## Median of second half: %0.2f' % (median1,median2))\n",
    "\n",
    "\tlc_info['baseline_revised_i'] = np.concatenate([regions['b_t0'],regions['b_t1'],regions['b_t2']])\n",
    "\tlc_info['duringsn_i'] = AnotB(lc_info['lc'].getindices(),lc_info['baseline_revised_i'])\n",
    "\treturn regions\n",
    "\n",
    "def count_valid_regions(regions):\n",
    "\tcount = 0\n",
    "\tfor region_index in range(0,3):\n",
    "\t\tif len(regions['b_t%d'%region_index]) > 0:\n",
    "\t\t\tcount += 1\n",
    "\treturn count\n",
    "\n",
    "def plot_fdf(ax, region_i, b_goodx2_i, b_badx2_i, median=None, xlim_lower=None, xlim_upper=None):\n",
    "\tax.set_title('Region (MJD): %d-%d' % (lc_info['lc'].t.loc[region_i[0],'MJD'], lc_info['lc'].t.loc[region_i[-1],'MJD']), size=15)\n",
    "\tax.set_xlabel('Flux (µJy)')\n",
    "\tplt.axvline(x=median,color='magenta')\n",
    "\tplt.gca().spines['right'].set_visible(False)\n",
    "\tplt.gca().spines['top'].set_visible(False)\n",
    "\tif len(lc_info['baseline_i'])>0: \n",
    "\t\tif xlim_lower is None: \n",
    "\t\t\txlim_lower = min(lc_info['lc'].t.loc[lc_info['baseline_revised_i'], 'uJy'])\n",
    "\t\tif xlim_upper is None:\n",
    "\t\t\txlim_upper = max(lc_info['lc'].t.loc[lc_info['baseline_revised_i'], 'uJy'])\n",
    "\t\tax.hist(lc_info['lc'].t.loc[AandB(region_i,b_goodx2_i), 'uJy'], bins=30, color='orange', alpha=0.5, range=(xlim_lower,xlim_upper), density=True)\n",
    "\t\tax.hist(lc_info['lc'].t.loc[AandB(region_i,b_badx2_i), 'uJy'], bins=30, color='blue', alpha=0.5, range=(xlim_lower,xlim_upper), density=True)\n",
    "\telse:\n",
    "\t\tax.hist(lc_info['lc'].t.loc[AandB(region_i,b_goodx2_i), 'uJy'], bins=30, color='orange', alpha=0.5, density=True)\n",
    "\t\tax.hist(lc_info['lc'].t.loc[AandB(region_i,b_badx2_i), 'uJy'], bins=30, color='blue', alpha=0.5, density=True)\n",
    "\n",
    "def begin_correct4template(Ndays_min, xlim_lower=None, xlim_upper=None, ylim_lower=None, ylim_upper=None):\n",
    "\t# automatically define baseline regions\n",
    "\tregions = get_baseline_regions(Ndays_min)\n",
    "\n",
    "\tif ylim_lower is None: ylim_lower = -400\n",
    "\tif ylim_upper is None: ylim_upper = 400 #ylim_upper = 1.3*get_xth_percentile_flux('lc', 95, lc_info['baseline_i'])\n",
    "\tplot_lc(add2title=' before correcting for template', xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)\n",
    "\n",
    "\treturn regions\n",
    "\n",
    "# correct control light curves for atlas template changes by \n",
    "# getting median of same baseline regions as SN, then applying to entire region\n",
    "def controls_correct_for_template(control_index, regions, region_index):\n",
    "\t\tb_goodx2_i = controls[control_index].ix_inrange(colnames=['chi/N'], uplim=5)\n",
    "\n",
    "\t\tlowlim = lc_info['lc'].t.loc[regions[f'b_t{region_index}'][0], 'MJD']\n",
    "\t\tuplim= lc_info['lc'].t.loc[regions[f'b_t{region_index}'][-1], 'MJD']\n",
    "\t\tb_region_i = controls[control_index].ix_inrange(colnames=['MJD'], lowlim=lowlim, uplim=uplim, exclude_uplim=True)\n",
    "\n",
    "\t\tif len(b_region_i) > 0:\n",
    "\t\t\tprint(f'### Adjusting for template change in region b_t{region_index} from {lowlim:0.2f}-{uplim:0.2f}...')\n",
    "\t\t\tprint(f'### Baseline median before: {np.median(controls[control_index].t.loc[b_region_i,\"uJy\"])}')\n",
    "\t\t\t\t\t\t\n",
    "\t\t\tif len(AandB(b_region_i,b_goodx2_i)) > 0:\n",
    "\t\t\t\tmedian = np.median(controls[control_index].t.loc[AandB(b_region_i,b_goodx2_i),'uJy'])\n",
    "\t\t\telse:\n",
    "\t\t\t\tmedian = np.median(controls[control_index].t.loc[b_region_i,'uJy'])\n",
    "\n",
    "\t\t\tlowlim = lc_info['lc'].t.loc[regions[f't{region_index}'][0], 'MJD']\n",
    "\t\t\tuplim = lc_info['lc'].t.loc[regions[f't{region_index}'][-1], 'MJD']\n",
    "\t\t\tt_region_i = controls[control_index].ix_inrange(colnames=['MJD'], lowlim=lowlim, uplim=uplim, exclude_uplim=True)\n",
    "\n",
    "\t\t\tprint(f'### Subtracting median {median:0.1f} uJy of baseline flux with chi-square ≤ 5 from light curve flux due to potential flux in the template...')\n",
    "\t\t\tcontrols[control_index].t.loc[t_region_i,'uJy'] -= median\n",
    "\t\t\tprint(f'### Baseline median now: {np.median(controls[control_index].t.loc[b_region_i,\"uJy\"])}')\n",
    "\t\telse:\n",
    "\t\t\tprint(f'### No baseline region for region b_t{region_index}, skipping...')\n",
    "\n",
    "def correct4template(regions, fdf_xlim_lower=None, fdf_xlim_upper=None, xlim_lower=None, xlim_upper=None, ylim_lower=None, ylim_upper=None):\n",
    "\tif ylim_lower is None: ylim_lower = -400\n",
    "\tif ylim_upper is None: ylim_upper = 400 #ylim_upper = 1.3*get_xth_percentile_flux('lc', 95, lc_info['baseline_i'])\n",
    "\n",
    "\t# user can manually override baseline region endpoints\n",
    "\tif input('Override baseline region endpoint(s)? (y/n)') == 'y':\n",
    "\t\tregions = set_baseline_regions(regions)\n",
    "\t\tplot_lc(add2title=' before correcting for template', xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)\n",
    "\n",
    "\tb_goodx2_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=5,indices=lc_info['baseline_revised_i'])\n",
    "\tb_badx2_i = AnotB(lc_info['baseline_revised_i'],b_goodx2_i)\n",
    "\tfig = plt.figure(figsize=(10, 5), tight_layout=True)\n",
    "\tplt.suptitle('SN %s %s-band baseline flux distribution before correcting for template' % (lc_info['tnsname'], lc_info['filter']), fontsize=17, y=1)\n",
    "\t\n",
    "\tfor region_index in range(0,3):\n",
    "\t\tregion_i = regions['b_t%d'%region_index]\n",
    "\t\tif len(region_i) > 0:\n",
    "\t\t\tprint('\\nAdjusting for template change in region b_t%d from %0.2f-%0.2f ' % (region_index, lc_info['lc'].t.loc[region_i[0],'MJD'], lc_info['lc'].t.loc[region_i[-1],'MJD']))\n",
    "\t\t\tprint('# Baseline median before: ', np.median(lc_info['lc'].t.loc[region_i,'uJy']))\n",
    "\t\t\tif len(AandB(region_i,b_goodx2_i)) > 0:\n",
    "\t\t\t\tmedian = np.median(lc_info['lc'].t.loc[AandB(region_i,b_goodx2_i),'uJy'])\n",
    "\t\t\telse:\n",
    "\t\t\t\tmedian = np.median(lc_info['lc'].t.loc[region_i,'uJy'])\n",
    "\t\t\t\n",
    "\t\t\tax = fig.add_subplot(1, count_valid_regions(regions), region_index+1)\n",
    "\t\t\tplot_fdf(ax, region_i, b_goodx2_i, b_badx2_i, median=median, xlim_lower=fdf_xlim_lower, xlim_upper=fdf_xlim_upper)\n",
    "\t\t\t\n",
    "\t\t\tprint(f'# Subtracting median {median:0.1f} uJy of baseline flux with chi-square ≤ 5 from light curve flux due to potential flux in the template...')\n",
    "\t\t\tlc_info['lc'].t.loc[regions['t%d'%region_index],'uJy'] -= median\n",
    "\t\t\tprint('# Baseline median now: ', np.median(lc_info['lc'].t.loc[region_i,'uJy']))\n",
    "\n",
    "\t\t\tif load_controls:\n",
    "\t\t\t\tprint(f'\\n# Correcting control light curves for potential flux in template...')\n",
    "\t\t\t\tfor control_index in range(1, Ncontrols+1):\n",
    "\t\t\t\t\tprint(f'\\n## Control index: {control_index}')\n",
    "\t\t\t\t\tcontrols_correct_for_template(control_index, regions, region_index)\n",
    "\t\telse:\n",
    "\t\t\tprint('No baseline region for region b_t%d, skipping... ' % region_index)\n",
    "\n",
    "\torange = mpatches.Patch(color='orange', alpha=0.5, label='Data with chi-square≤%d' % 5)\n",
    "\tblue = mpatches.Patch(color='blue', alpha=0.5, label='Data with chi-square>%d' % 5)\n",
    "\tmagenta = mpatches.Patch(color='magenta', alpha=0.5, label='Median of baseline flux with chi-square≤%d' % 5)\n",
    "\tfig.legend(handles=[orange, blue, magenta], loc='upper center', bbox_to_anchor=(0.5, 0), frameon=True)\n",
    "\n",
    "\tplot_lc(add2title=' after correcting for template', xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)\n",
    "\n",
    "def drop_extra_columns(lc_type):\n",
    "\tdropcols=[]\n",
    "\tif 'Noffsetlc' in lc_info[lc_type].t.columns: dropcols.append('Noffsetlc')\n",
    "\tif '__tmp_SN' in lc_info[lc_type].t.columns: dropcols.append('__tmp_SN')\n",
    "\tfor col in lc_info[lc_type].t.columns:\n",
    "\t\tif re.search('^c\\d_',col): \n",
    "\t\t\tdropcols.append(col)\n",
    "\tif len(dropcols)>0: \n",
    "\t\tprint('Dropping extra columns: ',dropcols)\n",
    "\t\tlc_info[lc_type].t.drop(columns=dropcols,inplace=True)\n",
    "\n",
    "def load_lc(filename):\n",
    "\tlc_info['lc'] = pdastrostatsclass()\n",
    "\ttry:\n",
    "\t\tprint('Loading SN %s light curve at %s...' % (lc_info['tnsname'], filename))\n",
    "\t\tlc_info['lc'].load_spacesep(filename,delim_whitespace=True)\n",
    "\texcept Exception as e:\n",
    "\t\tprint('Could not load light curve for SN %s at %s: %s' % (lc_info['tnsname'], filename, str(e)))\n",
    "\t\tsys.exit()\n",
    "\t\n",
    "\tlc_info['baseline_i'] = lc_info['lc'].ix_inrange(colnames=['MJD'],uplim=lc_info['discdate']-20,exclude_uplim=True)\n",
    "\tif len(lc_info['baseline_i'])<=0:\n",
    "\t\tprint('Baseline length is 0--cannot find best chi-square cut! Exiting...')\n",
    "\t\tsys.exit()\n",
    "\tlc_info['afterdiscdate_i'] = AnotB(lc_info['lc'].getindices(), lc_info['baseline_i']) \n",
    "\n",
    "lc_info['tnsname'] = tnsname\n",
    "\n",
    "if filter != 'o' and filter != 'c': \n",
    "\tprint('Filter must be \"o\" or \"c\"!')\n",
    "\tsys.exit()\n",
    "lc_info['filter'] = filter\n",
    "\n",
    "if discdate is None:\n",
    "\tprint('Obtaining SN %s discovery date from TNS...' % lc_info['tnsname'])\n",
    "\tdiscdate = get_discdate(lc_info['tnsname'], api_key)\n",
    "\tprint('Discovery date: ',discdate)\n",
    "lc_info['discdate'] = discdate - 20\n",
    "\n",
    "load_lc(filename)\n",
    "regions = begin_correct4template(Ndays_min, xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)\n",
    "\n",
    "print('\\n')\n",
    "load_control_lcs(controls_dir, Ncontrols)\n",
    "verify_mjds(Ncontrols)\n",
    "plot_control_lcs(add2title=' before correcting for template', xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using plot above, can manually redefine baseline endpoints within each template region; \n",
    "# then, adjust for template changes using baseline endpoints\n",
    "\n",
    "correct4template(regions, fdf_xlim_lower=fdf_xlim_lower, fdf_xlim_upper=fdf_xlim_upper, xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)\n",
    "plot_control_lcs(add2title=' after correcting for template', xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)\n",
    "\n",
    "drop_extra_columns('lc') # comment out this line if dropping columns is giving you unexpected trouble\n",
    "\n",
    "# add flux/dflux column\n",
    "print('Adding uJy/duJy column to light curve...')\n",
    "lc_info['lc'].t['uJy/duJy'] = lc_info['lc'].t['uJy']/lc_info['lc'].t['duJy']\n",
    "lc_info['lc'].t = lc_info['lc'].t.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ATLAS light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the scaling parameter for the SN plot's upper y limit \n",
    "# (ylim_upper = scale * 95th percentile flux):\n",
    "scale = 3\n",
    "\n",
    "# Optionally, manually enter the SN plot's x and y limits to override automatic scaling:\n",
    "xlim_lower = 58900\n",
    "xlim_upper = 59800\n",
    "ylim_lower = -300\n",
    "ylim_upper = 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the light curve\n",
    "\n",
    "if xlim_lower is None: xlim_lower = lc_info['discdate'] - 200\n",
    "if xlim_upper is None: xlim_upper = lc_info['discdate'] + 800\n",
    "if ylim_lower is None: ylim_lower = -2000\n",
    "if ylim_upper is None: ylim_upper = scale*get_xth_percentile_flux('lc', 97, lc_info['duringsn_i'])\n",
    "plot_lc(xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Uncertainty Cut\n",
    "\n",
    "The following uncertainty cut implements a static cut that applies the same way to each light curve. The purpose of this cut is to identify and clean out the most egregious outliers with large uncertainties and small chi-square values not cut in the dynamic chi-square cut. The value of this cut was determined after calculating the typical uncertainty of bright stars just below the saturation limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may change the following static uncertainty cut value to your liking;\n",
    "# however, the default value is set to 160.\n",
    "lc_info['uncertainty_cut'] = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart 'Mask' column and update with uncertainty cut flag\n",
    "\n",
    "def update_mask_col(lc, flag, indices):\n",
    "    if len(indices) > 1:\n",
    "        flag_arr = np.full(lc.loc[indices,'Mask'].shape, flag)\n",
    "        lc.loc[indices,'Mask'] = np.bitwise_or(lc.loc[indices,'Mask'], flag_arr)\n",
    "    elif len(indices) == 1:\n",
    "        lc.loc[indices[0],'Mask'] = int(lc.loc[indices[0],'Mask']) | flag\n",
    "    else:\n",
    "        print('WARNING: must pass at least 1 index to update_mask_col()! No indices masked...')\n",
    "\n",
    "# remove old mask column\n",
    "if 'Mask' in lc_info['lc'].t.columns: \n",
    "    lc_info['lc'].t.drop(columns=['Mask'],inplace=True)\n",
    "\n",
    "# create new mask column and update it with uncertainty cut\n",
    "lc_info['lc'].t['Mask'] = 0\n",
    "kept_i = lc_info['lc'].ix_inrange(colnames=['duJy'],uplim=lc_info['uncertainty_cut'])\n",
    "cut_i = AnotB(lc_info['lc'].getindices(), kept_i)\n",
    "update_mask_col(lc_info['lc'].t, flag_uncertainty, cut_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ATLAS light curve with uncertainty cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, manually enter the plot's x and y limits:\n",
    "xlim_lower = 58900\n",
    "xlim_upper = 59800\n",
    "ylim_lower = -300\n",
    "ylim_upper = 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the light curve\n",
    "\n",
    "plot_cut_lc('lc', flag_uncertainty, add2title='uncertainty cut', xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating True Uncertainties\n",
    "This section attempts to account for an extra noise source in the data by estimating the true typical uncertainty, deriving the additional systematic uncertainty, and lastly applying this extra noise to a new uncertainty column. This new uncertainty column will be used in the cuts following this section.\n",
    "\n",
    "Here is the exact procedure we use:\n",
    "1. Keep the uncertainty cut at 160 duJy and apply a preliminary chi-square cut at 20. Filter out any measurements flagged by these two cuts.\n",
    "2. Calculate the true typical uncertainty $\\text{sigma\\_true\\_typical}$ by taking a 3σ cut of the unflagged baseline flux and getting the standard deviation.\n",
    "3. If $\\text{sigma\\_true\\_typical}$ is greater than the median uncertainty of the unflagged baseline flux, $\\text{median}(∂µJy)$, proceed with estimating the extra noise to add. Otherwise, keep the old chi-square cut and skip this procedure. \n",
    "4. Calculate the extra noise source using the following formula, where the median uncertainty, $\\text{median}(∂µJy)$, is taken from the unflagged baseline flux:\n",
    "    - $\\text{sigma\\_extra}^2 = \\text{sigma\\_true\\_typical}^2 - \\text{sigma\\_poisson}^2 = \\text{sigma\\_true\\_typical}^2 - \\text{median}(∂µJy)^2 $\n",
    "5. Apply the extra noise source to the existing uncertainty using the following formula:\n",
    "    - $\\text{new }∂µJy = \\sqrt{(\\text{old }∂µJy)^2 + \\text{sigma\\_extra}^2}$\n",
    "6. Repeat steps 1-5 for each control light curve. For cuts following this procedure, use the new uncertainty column with the extra noise added instead of the old uncertainty column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter a preliminary chi-square cut (keep at a high number):\n",
    "prelim_x2_cut = 20\n",
    "\n",
    "# Optionally, manually enter the SN plot's x and y limits to override automatic scaling:\n",
    "xlim_lower = 58900\n",
    "xlim_upper = 59800\n",
    "ylim_lower = -300\n",
    "ylim_upper = 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_dflux(lc, b_clean_ix):\n",
    "    return np.median(lc.t.loc[b_clean_ix, 'duJy'])\n",
    "\n",
    "def get_sigma_true_typical(lc, b_clean_ix):\n",
    "    lc.calcaverage_sigmacutloop('uJy', indices=b_clean_ix, Nsigma=3.0, median_firstiteration=True, verbose=1)\n",
    "    return lc.statparams['stdev']\n",
    "\n",
    "def get_sigma_extra(sigma_true_typical, median_dflux):\n",
    "    sigma_extra = np.sqrt(sigma_true_typical*sigma_true_typical - median_dflux)\n",
    "    print('# Sigma extra calculated: %0.4f' % sigma_extra)\n",
    "    return sigma_extra\n",
    "\n",
    "def add_sigma_extra(sigma_extra, control_index):\n",
    "    print('# Adding sigma_extra to new duJy column...')\n",
    "    if control_index == 0:\n",
    "        lc = lc_info['lc']\n",
    "    else:\n",
    "        lc = controls[control_index]\n",
    "    lc.t['duJy_new'] = np.sqrt(lc.t['duJy']*lc.t['duJy'] + sigma_extra*sigma_extra)\n",
    "\n",
    "def estimate_true_uncertainties(prelim_x2_cut, control_index, xlim_lower=None, xlim_upper=None, ylim_lower=None, ylim_upper=None):\n",
    "    if control_index == 0:\n",
    "        print('\\nEstimating true uncertainties for SN light curve...')\n",
    "        lc = lc_info['lc']\n",
    "        clean_ix = AandB(lc.ix_unmasked('Mask',maskval=flag_uncertainty), lc.ix_inrange(colnames=['chi/N'],uplim=prelim_x2_cut,exclude_uplim=True))\n",
    "        b_clean_ix = AandB(lc_info['baseline_revised_i'], clean_ix)\n",
    "    else:\n",
    "        print(f'\\nEstimating true uncertainties for control light curve {control_index:03d}...')\n",
    "        lc = controls[control_index]\n",
    "        b_clean_ix =  AandB(lc.ix_unmasked('Mask',maskval=flag_uncertainty), lc.ix_inrange(colnames=['chi/N'],uplim=prelim_x2_cut,exclude_uplim=True))\n",
    "    \n",
    "    sigma_true_typical = get_sigma_true_typical(lc, b_clean_ix)\n",
    "    median_dflux = get_median_dflux(lc, b_clean_ix)\n",
    "    print(f'# Median uncertainty: {median_dflux:0.2f}; true typical uncertainty: {sigma_true_typical:0.2f}')\n",
    "\n",
    "    if sigma_true_typical > median_dflux:\n",
    "        print(f'# True typical uncertainty greater the current median uncertainty. Proceeding with true uncertainties estimation.')\n",
    "\n",
    "        # use new uncertainty column from now on\n",
    "        dflux_colnames[control_index] = 'duJy_new'\n",
    "\n",
    "        sigma_extra = get_sigma_extra(sigma_true_typical, median_dflux)\n",
    "        add_sigma_extra(sigma_extra, control_index)\n",
    "        #print(lc_info['lc'].t[['duJy', 'duJy_new']])\n",
    "        \n",
    "        if control_index == 0:\n",
    "            # fix uJy/duJy column\n",
    "            print('# Recalculating uJy/duJy column with duJy_new column...')\n",
    "            lc_info['lc'].t['uJy/duJy'] = lc_info['lc'].t['uJy']/lc_info['lc'].t['duJy_new']\n",
    "            lc_info['lc'].t = lc_info['lc'].t.replace([np.inf, -np.inf], np.nan)\n",
    "            \n",
    "            if xlim_lower is None: xlim_lower = lc_info['discdate'] - 200\n",
    "            if xlim_upper is None: xlim_upper = lc_info['discdate'] + 800\n",
    "            if ylim_lower is None: ylim_lower = -2000\n",
    "            if ylim_upper is None: ylim_upper = scale*get_xth_percentile_flux('lc', 97, lc_info['duringsn_i'])\n",
    "            # plot the light curve before adding extra noise\n",
    "            plot_lc(add2title= ' before adding extra noise', xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)\n",
    "            # plot the light curve after adding extra noise\n",
    "            plot_lc(add2title= ' after adding extra noise', dflux_colname=dflux_colnames[control_index], xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)\n",
    "    else:\n",
    "        print(f'# True typical uncertainty less than or equal to the current median uncertainty. Skipping true uncertainties estimation.')\n",
    "\n",
    "# list for storing the new dflux column name ('duJy' vs. 'duJy_new')\n",
    "global dflux_colnames\n",
    "dflux_colnames = ['duJy'] * (Ncontrols+1)\n",
    "\n",
    "estimate_true_uncertainties(prelim_x2_cut, 0, xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)\n",
    "for control_index in range(1,Ncontrols+1):\n",
    "    estimate_true_uncertainties(prelim_x2_cut, control_index, xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the light curve with the new 'Mask' and 'new duJy' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the light curve with the new 'Mask' column?\n",
    "save = False\n",
    "\n",
    "# Overwrite the old light curve file?\n",
    "overwrite_old_lc = False\n",
    "\n",
    "# If not overwriting old light curve file, enter new filename:\n",
    "filename_new = '/Users/sofiarest/Google Drive/My Drive/College/STScI Research Paper/atlaslc_chisquare/brightsne/2019vxm/NEW_2019vxm_i000.o.lc.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save light curve\n",
    "if save:\n",
    "    print('Saving light curve with updated mask column...')\n",
    "    if overwrite_old_lc:\n",
    "        print('Overwriting old light curve file at %s... ' % filename)\n",
    "        save_lc('lc',filename,overwrite=True)\n",
    "    else:\n",
    "        print('Writing new file at %s... ' % filename_new)\n",
    "        save_lc('lc',filename_new,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Chi-Square Cut\n",
    "\n",
    "### Plot the flux/dflux and chi-square distributions\n",
    "\n",
    "The following two histograms display the flux/dflux and chi-square distributions of the target SN. Both histograms show probability density so as to ease comparison between the groups plotted within each histogram.\n",
    "\n",
    "- The first histogram focuses on the baseline flux/dflux (µJy/dµJy) measurements, where we can expect the flux to equal 0. In orange, we plot flux/dflux (µJy/dµJy) measurements with a chi-square value less than or equal to `x2bound`, which is currently set to 5 below; in blue, we plot flux/dflux (µJy/dµJy) measurements with a chi-square value greater than `x2bound`. \n",
    "- The second histogram focuses on the baseline chi-square measurements. In green, we plot chi-square measurements with an abs(µJy/dµJy) value less than or equal to `stnbound`, which is currently set to 3 below; in red, we plot chi-square measurements with an abs(µJy/dµJy) value greater than `stnbound`. \n",
    "\n",
    "Ideally, all measurements with a chi-square value less than or equal to `x2bound` should have an abs(µJy/dµJy) value less than or equal to `stnbound`, and measurements with a chi-square value greater than `x2bound` should have an abs(µJy/dµJy) value greater than `stnbound`. Our goal is to separate good measurements from bad measurements using a chi-square cut; in order for our cut to be effective, these histograms should hopefully showcase this relation between the target SN's flux/dflux and chi-square measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the bound that should separate a good chi-square measurement from a bad one:\n",
    "x2bound = 5.0\n",
    "\n",
    "# Enter the bound that should separate a good abs(flux/dflux) measurement from a bad one:\n",
    "stnbound = 3.0\n",
    "\n",
    "# Optionally, manually enter the histograms' x limits here:\n",
    "# flux/dflux histogram x limits:\n",
    "fdf_xlim_lower = None \n",
    "fdf_xlim_upper = None\n",
    "# chi-square histogram x limits:\n",
    "x2_xlim_lower = None\n",
    "x2_xlim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot flux/dflux and chi-square distribution histograms\n",
    "\n",
    "def plot_hists(x2bound, stnbound, fdf_xlim_lower=None, fdf_xlim_upper=None, x2_xlim_lower=None, x2_xlim_upper=None):\n",
    "    b_goodstn_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stnbound,uplim=stnbound,indices=lc_info['baseline_i'])\n",
    "    b_badstn_i = AnotB(lc_info['baseline_i'],b_goodstn_i)\n",
    "    b_goodx2_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=x2bound,indices=lc_info['baseline_i'])\n",
    "    b_badx2_i = AnotB(lc_info['baseline_i'],b_goodx2_i)\n",
    "\n",
    "    fig, (stn, x2) = plt.subplots(1, 2, figsize=(10, 6.5), tight_layout=True)\n",
    "    plt.suptitle('SN %s %s-band, baseline only' % (lc_info['tnsname'], lc_info['filter']), fontsize=17, y=1)\n",
    "\n",
    "    stn.set_title('µJy/dµJy distribution')\n",
    "    stn.set_xlabel('µJy/dµJy')\n",
    "    stn.spines.right.set_visible(False)\n",
    "    stn.spines.top.set_visible(False)\n",
    "    orange = mpatches.Patch(color='orange', label='Data with chi-square<%0.2f' % x2bound)\n",
    "    blue = mpatches.Patch(color='blue', label='Data with chi-square≥%0.2f' % x2bound)\n",
    "    stn.legend(handles=[orange, blue], loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=1)\n",
    "    if len(lc_info['baseline_i'])>0: \n",
    "        if fdf_xlim_lower is None: \n",
    "            fdf_xlim_lower = min(lc_info['lc'].t.loc[lc_info['baseline_i'], 'uJy/duJy'])\n",
    "        if fdf_xlim_upper is None: \n",
    "            fdf_xlim_upper = max(lc_info['lc'].t.loc[lc_info['baseline_i'], 'uJy/duJy'])\n",
    "        stn.hist(lc_info['lc'].t.loc[b_goodx2_i, 'uJy/duJy'], bins=30, color='orange', alpha=0.5, range=(fdf_xlim_lower,fdf_xlim_upper), density=True)\n",
    "        stn.hist(lc_info['lc'].t.loc[b_badx2_i, 'uJy/duJy'], bins=30, color='blue', alpha=0.5, range=(fdf_xlim_lower,fdf_xlim_upper), density=True)\n",
    "    else:\n",
    "        stn.hist(lc_info['lc'].t.loc[b_goodx2_i, 'uJy/duJy'], bins=30, color='orange', alpha=0.5, density=True)\n",
    "        stn.hist(lc_info['lc'].t.loc[b_badx2_i, 'uJy/duJy'], bins=30, color='blue', alpha=0.5, density=True)\n",
    "\n",
    "    x2.set_title('Chi-square distribution')\n",
    "    x2.set_xlabel('Chi-square')\n",
    "    x2.spines.right.set_visible(False)\n",
    "    x2.spines.top.set_visible(False)\n",
    "    red = mpatches.Patch(color='green', label='Data with µJy/dµJy<%0.2f' % stnbound)\n",
    "    green = mpatches.Patch(color='red', label='Data with µJy/dµJy≥%0.2f' % stnbound)\n",
    "    x2.legend(handles=[red, green], loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=1)\n",
    "    if len(lc_info['baseline_i'])>0:\n",
    "        if x2_xlim_lower is None: \n",
    "            x2_xlim_lower = min(lc_info['lc'].t.loc[lc_info['baseline_i'], 'chi/N'])\n",
    "        if x2_xlim_upper is None: \n",
    "            x2_xlim_upper = max(lc_info['lc'].t.loc[lc_info['baseline_i'], 'chi/N'])\n",
    "        x2.hist(lc_info['lc'].t.loc[b_goodstn_i, 'chi/N'], bins=30, color='green', alpha=0.5, range=(x2_xlim_lower,x2_xlim_upper), density=True)\n",
    "        x2.hist(lc_info['lc'].t.loc[b_badstn_i, 'chi/N'], bins=30, color='red', alpha=0.5, range=(x2_xlim_lower,x2_xlim_upper), density=True)\n",
    "    else:\n",
    "        x2.hist(lc_info['lc'].t.loc[b_goodstn_i, 'chi/N'], bins=30, color='green', alpha=0.5, density=True)\n",
    "        x2.hist(lc_info['lc'].t.loc[b_badstn_i, 'chi/N'], bins=30, color='red', alpha=0.5, density=True)\n",
    "\n",
    "plot_hists(x2bound, stnbound, fdf_xlim_lower=fdf_xlim_lower, fdf_xlim_upper=fdf_xlim_upper, x2_xlim_lower=x2_xlim_lower, x2_xlim_upper=x2_xlim_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate best chi-square cut based on contamination and loss\n",
    "\n",
    "The following cells use two factors, <strong>contamination</strong> and <strong>loss</strong>, to attempt to calculate an optimal PSF chi-square cut for the target SN, with flux/dflux as the deciding factor of what constitutes a good measurement vs. a bad measurement. We aim to separate good measurements from bad using the calculated chi-square cut by removing as much contamination as possible with the smallest loss possible. Since we can assume that the expected value of the baseline flux is 0, we look only at the baseline measurements before the SN occurs in order to determine the best chi-square cut for the SN itself.\n",
    "\n",
    "First, we decide what will determine a good measurement vs. a bad measurement using a factor outside of the chi-square values. Our chosen factor is the <strong>absolute value of flux (µJy) divided by dflux (dµJy)</strong>. The recommended boundary is a value of 3, such that any measurements with a value of abs(µJy/dµJy) less than or equal to 3 are regarded as \"good\" measurements, and any measurements with a value of abs(µJy/dµJy) greater than 3 are regarded as \"bad\" measurements. You can set this boundary to a different number by changing the value of `stn_cut` below.\n",
    "\n",
    "Next, we set the upper and lower bounds of our final chi-square cut. We start at a low value of 3 (which can be changed by setting the value of `cut_start` below) and end at 50 (this value is inclusive and can be changed by setting the value of `cut_stop` below) with a step size of 1 (`cut_step` below). <strong>For chi-square cuts falling on or between `cut_start` and `cut_stop` in increments of `cut_step`, we can begin to calculate contamination and loss percentages.</strong>\n",
    "\n",
    "We define contamination to be the number of bad kept measurements over the total number of kept measurements for that chi-square cut (<strong>contamination = Nbad,kept/Nkept</strong>). For our final chi-square cut, we can also set a limit on what maximum percent contamination we want to have--the recommended value is <strong>10%</strong> but can be changed by setting the value of `contam_lim` below.\n",
    "\n",
    "We define loss to be the number of good cut measurements over the total number of good measurements for that chi-square cut (<strong>loss = Ngood,cut/Ngood</strong>). For our final chi-square cut, we can also set a limit on what maximum percent loss we want to have--the recommended value is <strong>10%</strong> but can be changed by setting the value of `loss_lim` below.\n",
    "\n",
    "Finally, we define which limit (`contam_lim` or `loss_lim`) to prioritize in the event that an optimal chi-square cut fitting both limits is not found. The default prioritized limit is `loss_lim` but can be changed by setting the value of `lim_to_prioritize` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the abs(uJy/duJy) boundary that will determine a \"good\" measurement vs. \"bad\" measurement:\n",
    "stn_cut = 3\n",
    "\n",
    "# Enter the bounds for the final chi-square cut (minimum cut, maximum cut, and step):\n",
    "cut_start = 3 # this is inclusive\n",
    "cut_stop = 50 # this is inclusive\n",
    "cut_step = 1\n",
    "\n",
    "# Enter the contamination limit (contamination = Nbad,kept/Nkept must be <= contam_lim% \n",
    "# for the final chi-square cut):\n",
    "contam_lim = 15.0\n",
    "\n",
    "# Enter the loss limit (loss = Ngood,cut/Ngood must be >= loss_lim%\n",
    "# for the final chi-square cut):\n",
    "loss_lim = 10.0\n",
    "\n",
    "# Enter the limit to prioritize (must be 'loss_lim' or 'contam_lim') in the event that\n",
    "# one or both limits are not met:\n",
    "lim_to_prioritize = 'loss_lim'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section describes in detail how we determine the final chi-square cut using the given contamination and loss limits.\n",
    "\n",
    "For each given limit (contamination and loss), we calculate a range of valid cuts whose contamination/loss percentage is less than that limit and then choose a single cut within that valid range. Then, we pass through a decision tree to determine which of the two suggested cuts to use using a variety of factors (including the user's selected `lim_to_prioritize`).\n",
    "\n",
    "When choosing the loss cut according to the loss percentage limit `loss_lim`:\n",
    "- <strong>If all loss percentages are below the limit</strong> `loss_lim`, all cuts falling on or between `cut_start` and `cut_stop` are valid.\n",
    "- <strong>If all loss percentages are above the limit</strong> `loss_lim`, a cut with the required loss percentage is not possible; therefore, any cuts with the smallest percentage of loss are valid.\n",
    "- <strong>Otherwise</strong>, the valid range of cuts includes any cuts with the loss percentage less than or equal to the limit `loss_lim`.\n",
    "- The chosen cut for this limit is the <strong>minimum cut</strong> within the stated valid range of cuts.\n",
    "\n",
    "When choosing the loss cut according to the contamination percentage limit `contam_lim`:\n",
    "- <strong>If all contamination percentages are below the limit</strong> `contam_lim`, all cuts falling on or between `cut_start` and `cut_stop` are valid.\n",
    "- <strong>If all contamination percentages are above the limit</strong> `contam_lim`, a cut with the required contamination percentage is not possible; therefore, any cuts with the smallest percentage of contamination are valid.\n",
    "- <strong>Otherwise</strong>, the valid range of cuts includes any cuts with the contamination percentage less than or equal to the limit `contam_lim`.\n",
    "- The chosen cut for this limit is the <strong>maximum cut</strong> within the stated valid range of cuts.\n",
    "\n",
    "After we have calculated two suggested cuts based on the loss and contamination percentage limits, we follow the decision tree in order to suggest a final cut:\n",
    "- If both loss and contamination cut percentages were chosen from a range that spanned from `cut_start` to `cut_stop`, we set the final cut to `cut_start`.\n",
    "- If one cut's percentage was chosen from a range that spanned from `cut_start` to `cut_stop` and the other cut's percentage was not, we set the final cut to the latter cut.\n",
    "- If both percentages were chosen from ranges that fell above their respective limits, we suggest reselecting either or both limits.\n",
    "- Otherwise, we take into account the user's prioritized limit `lim_to_prioritize`:\n",
    "    - If the loss cut is greater than the contamination cut, we set the final cut to whichever cut is associated with `lim_to_prioritize`.\n",
    "    - Otherwise, if `lim_to_prioritize` is set to `contam_lim`, we set the final cut to the loss cut, and if `lim_to_prioritize` is set to `loss_lim`, we set the final cut to the contamination cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the suggested best chi-square cut using contamination and loss\n",
    "\n",
    "def plot_lim_cuts(lim_cuts, contam_lim_cut, loss_lim_cut):\n",
    "    fig = plt.figure(figsize=(10,6), tight_layout=True)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.title('SN %s %s-band chi-square cut' % (lc_info['tnsname'],lc_info['filter']))\n",
    "\n",
    "    plt.axhline(linewidth=1,color='k')\n",
    "    plt.xlabel('Chi-square cut')\n",
    "    plt.ylabel('% of baseline measurements')\n",
    "\n",
    "    plt.axhline(loss_lim,linewidth=1,color='r',linestyle='--',label='Loss limit')\n",
    "    plt.plot(lim_cuts.t['PSF Chi-Square Cut'], lim_cuts.t['Ploss'],ms=5,color='r',marker='o',label='Loss')\n",
    "    plt.axvline(x=loss_lim_cut,color='r',label='Loss cut')\n",
    "    plt.axvspan(loss_lim_cut, cut_stop, alpha=0.2, color='r')\n",
    "\n",
    "    plt.axhline(contam_lim,linewidth=1,color='g',linestyle='--',label='Contamination limit')\n",
    "    plt.plot(lim_cuts.t['PSF Chi-Square Cut'], lim_cuts.t['Pcontamination'],ms=5,color='g',marker='o',label='Contamination')\n",
    "    plt.axvline(x=contam_lim_cut,color='g',label='Contamination cut')\n",
    "    plt.axvspan(cut_start, contam_lim_cut, alpha=0.2, color='g')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    #fig.savefig('000001.png',bbox_inches=\"tight\",dpi=200)\n",
    "\n",
    "def choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case):\n",
    "    # case 1 and 1: final_cut = 3\n",
    "    # case 1 and 2: take limit of case 2\n",
    "    # case 1 and 3: take limit of case 3\n",
    "    # case 2 and 2: print lims don't work\n",
    "    # case 2 and 3: choose_btwn_lim_cuts\n",
    "    # case 3 and 3: choose_btwn_lim_cuts\n",
    "\n",
    "    case1 = loss_case == 'below lim' or contam_case == 'below lim'\n",
    "    case2 = loss_case == 'above lim' or contam_case == 'above lim'\n",
    "    case3 = loss_case == 'crosses lim' or contam_case == 'crosses lim'\n",
    "\n",
    "    final_cut = None\n",
    "    if case1 and not case2 and not case3: # 1 and 1\n",
    "        print('Valid chi-square cut range from %0.2f to %0.2f! Setting to 3...' % (loss_lim_cut, contam_lim_cut))\n",
    "        final_cut = cut_start\n",
    "    elif case1: # 1\n",
    "        if case2: # and 2\n",
    "            if loss_case == 'above lim':\n",
    "                print('WARNING: contam_lim_cut <= %0.2f falls below limit %0.2f%%, but loss_lim_cut >= %0.2f falls above limit %0.2f%%! Setting to %0.2f...' % (contam_lim_cut, contam_lim, loss_lim_cut, loss_lim, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('WARNING: loss_lim_cut <= %0.2f falls below limit %0.2f%%, but contam_lim_cut >= %0.2f falls above limit %0.2f%%! Setting to %0.2f...' % (loss_lim_cut, loss_lim, contam_lim_cut, contam_lim, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "        else: # and 3\n",
    "            if loss_case == 'crosses lim':\n",
    "                print('Contam_lim_cut <= %0.2f falls below limit %0.2f%% and loss_lim_cut >= %0.2f crosses limit %0.2f%%, setting to %0.2f...' % (contam_lim_cut, contam_lim, loss_lim_cut, loss_lim, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('Loss_lim_cut <= %0.2f falls below limit %0.2f%% and contam_lim_cut >= %0.2f crosses limit %0.2f%%, setting to %0.2f...' % (loss_lim_cut, loss_lim, contam_lim_cut, contam_lim, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "    elif case2 and not case3: # 2 and 2\n",
    "        print('ERROR: chi-square loss_lim_cut >= %0.2f and contam_lim_cut <= %0.2f both fall above limits %0.2f%% and %0.2f%%! Try setting less strict limits. Setting final cut to nan.' % (loss_lim_cut, contam_lim_cut, loss_lim, contam_lim))\n",
    "        final_cut = np.nan\n",
    "    else: # 2 and 3 or 3 and 3\n",
    "        if loss_lim_cut > contam_lim_cut:\n",
    "            print('WARNING: chi-square loss_lim_cut >= %0.2f and contam_lim_cut <= %0.2f do not overlap! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %0.2f...' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "        else:\n",
    "            print('Valid chi-square cut range from %0.2f to %0.2f! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "    return final_cut\n",
    "\n",
    "def get_lim_cuts_data(cut, colname, indices=None):\n",
    "    if indices is None:\n",
    "        indices = lc_info['baseline_revised_i']\n",
    "\n",
    "    b_good_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stn_cut,uplim=stn_cut,indices=indices)\n",
    "    b_bad_i = AnotB(indices, b_good_i)\n",
    "    b_kept_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=cut,indices=indices)\n",
    "    b_cut_i = AnotB(indices, b_kept_i)\n",
    "\n",
    "    lc_info['%s_Ngood' % colname] = len(b_good_i)\n",
    "    lc_info['%s_Nbad' % colname] = len(b_bad_i)\n",
    "    lc_info['%s_Nkept' % colname] = len(b_kept_i)\n",
    "    lc_info['%s_Ncut' % colname] = len(b_cut_i)\n",
    "    lc_info['%s_Ngood,kept' % colname] = len(AandB(b_good_i,b_kept_i))\n",
    "    lc_info['%s_Ngood,cut' % colname] = len(AandB(b_good_i,b_cut_i))\n",
    "    lc_info['%s_Nbad,kept' % colname] = len(AandB(b_bad_i,b_kept_i))\n",
    "    lc_info['%s_Nbad,cut' % colname] = len(AandB(b_bad_i,b_cut_i))\n",
    "    lc_info['%s_Pgood,kept' % colname] = 100*len(AandB(b_good_i,b_kept_i))/len(indices)\n",
    "    lc_info['%s_Pgood,cut' % colname] = 100*len(AandB(b_good_i,b_cut_i))/len(indices)\n",
    "    lc_info['%s_Pbad,kept' % colname] = 100*len(AandB(b_bad_i,b_kept_i))/len(indices)\n",
    "    lc_info['%s_Pbad,cut' % colname] = 100*len(AandB(b_bad_i,b_cut_i))/len(indices)\n",
    "    lc_info['%s_Ngood,kept/Ngood' % colname] = 100*len(AandB(b_good_i,b_kept_i))/len(b_good_i)\n",
    "    lc_info['%s_Ploss' % colname] = 100*len(AandB(b_good_i,b_cut_i))/len(b_good_i)\n",
    "    lc_info['%s_Pcontamination' % colname] = 100*len(AandB(b_bad_i,b_kept_i))/len(b_kept_i)\n",
    "\n",
    "def get_lim_cuts(lim_cuts): \n",
    "    contam_lim_cut = None\n",
    "    loss_lim_cut = None\n",
    "    contam_case = None\n",
    "    loss_case = None\n",
    "\n",
    "    sortby_loss = lim_cuts.t.iloc[(lim_cuts.t['Ploss']).argsort()].reset_index()\n",
    "    min_loss = sortby_loss.loc[0,'Ploss']\n",
    "    max_loss = sortby_loss.loc[len(sortby_loss)-1,'Ploss']\n",
    "    # if all loss below lim, loss_lim_cut is min cut\n",
    "    if min_loss < loss_lim and max_loss < loss_lim:\n",
    "        loss_case = 'below lim'\n",
    "        loss_lim_cut = lim_cuts.t.loc[0,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all loss above lim, loss_lim_cut is min cut with min% loss\n",
    "        if min_loss > loss_lim and max_loss > loss_lim:\n",
    "            loss_case = 'above lim'\n",
    "            a = np.where(lim_cuts.t['Ploss'] == min_loss)[0]\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            loss_lim_cut = c.loc[0,'PSF Chi-Square Cut']\n",
    "        # else if loss crosses lim at some point, loss_lim_cut is min cut with max% loss <= loss_lim\n",
    "        else:\n",
    "            loss_case = 'crosses lim'\n",
    "            valid_cuts = sortby_loss[sortby_loss['Ploss'] <= loss_lim]\n",
    "            a = np.where(lim_cuts.t['Ploss'] == valid_cuts.loc[len(valid_cuts)-1,'Ploss'])[0]\n",
    "            # sort by cuts\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            # get midpoint of loss1 and loss2 (two points on either side of lim)\n",
    "            loss1_i = np.where(lim_cuts.t['PSF Chi-Square Cut'] == c.loc[0,'PSF Chi-Square Cut'])[0][0]\n",
    "            if lim_cuts.t.loc[loss1_i,'Ploss'] == loss_lim:\n",
    "                loss_lim_cut = lim_cuts.t.loc[loss1_i,'PSF Chi-Square Cut']\n",
    "            else:\n",
    "                loss2_i = loss1_i - 1\n",
    "                x = np.array([lim_cuts.t.loc[loss1_i,'PSF Chi-Square Cut'], lim_cuts.t.loc[loss2_i,'PSF Chi-Square Cut']])\n",
    "                contam_y = np.array([lim_cuts.t.loc[loss1_i,'Pcontamination'], lim_cuts.t.loc[loss2_i,'Pcontamination']])\n",
    "                loss_y = np.array([lim_cuts.t.loc[loss1_i,'Ploss'], lim_cuts.t.loc[loss2_i,'Ploss']])\n",
    "                contam_line = np.polyfit(x,contam_y,1)\n",
    "                loss_line = np.polyfit(x,loss_y,1)\n",
    "                loss_lim_cut = (loss_lim-loss_line[1])/loss_line[0]\n",
    "\n",
    "    sortby_contam = lim_cuts.t.iloc[(lim_cuts.t['Pcontamination']).argsort()].reset_index()\n",
    "    min_contam = sortby_contam.loc[0,'Pcontamination']\n",
    "    max_contam = sortby_contam.loc[len(sortby_contam)-1,'Pcontamination']\n",
    "    # if all contam below lim, contam_lim_cut is max cut\n",
    "    if min_contam < contam_lim and max_contam < contam_lim:\n",
    "        contam_case = 'below lim'\n",
    "        contam_lim_cut = lim_cuts.t.loc[len(lim_cuts.t)-1,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all contam above lim, contam_lim_cut is max cut with min% contam\n",
    "        if min_contam > contam_lim and max_contam > contam_lim:\n",
    "            contam_case = 'above lim'\n",
    "            a = np.where(lim_cuts.t['Pcontamination'] == min_contam)[0]\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            contam_lim_cut = c.loc[len(c)-1,'PSF Chi-Square Cut']\n",
    "        # else if contam crosses lim at some point, contam_lim_cut is max cut with max% contam <= contam_lim\n",
    "        else:\n",
    "            contam_case = 'crosses lim'\n",
    "            valid_cuts = sortby_contam[sortby_contam['Pcontamination'] <= contam_lim]\n",
    "            a = np.where(lim_cuts.t['Pcontamination'] == valid_cuts.loc[len(valid_cuts)-1,'Pcontamination'])[0]\n",
    "            # sort by cuts\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            # get midpoint of contam1 and contam2 (two points on either side of lim)\n",
    "            contam1_i = np.where(lim_cuts.t['PSF Chi-Square Cut'] == c.loc[len(c)-1,'PSF Chi-Square Cut'])[0][0]\n",
    "            if lim_cuts.t.loc[contam1_i,'Pcontamination'] == contam_lim:\n",
    "                contam_lim_cut = lim_cuts.t.loc[contam1_i,'PSF Chi-Square Cut']\n",
    "            else:\n",
    "                contam2_i = contam1_i + 1\n",
    "                x = np.array([lim_cuts.t.loc[contam1_i,'PSF Chi-Square Cut'], lim_cuts.t.loc[contam2_i,'PSF Chi-Square Cut']])\n",
    "                contam_y = np.array([lim_cuts.t.loc[contam1_i,'Pcontamination'], lim_cuts.t.loc[contam2_i,'Pcontamination']])\n",
    "                loss_y = np.array([lim_cuts.t.loc[contam1_i,'Ploss'], lim_cuts.t.loc[contam2_i,'Ploss']])\n",
    "                contam_line = np.polyfit(x,contam_y,1)\n",
    "                loss_line = np.polyfit(x,loss_y,1)\n",
    "                contam_lim_cut = (contam_lim-contam_line[1])/contam_line[0]\n",
    "\n",
    "    get_lim_cuts_data(loss_lim_cut, 'loss_lim_cut')\n",
    "    get_lim_cuts_data(contam_lim_cut, 'contam_lim_cut')\n",
    "\n",
    "    return contam_lim_cut, loss_lim_cut, contam_case, loss_case\n",
    "\n",
    "def get_lim_cuts_table(stn_cut, cut_start, cut_stop, cut_step, indices=None):\n",
    "    print('abs(uJy/duJy) cut at %0.2f \\nx2 cut from %0.2f to %0.2f inclusive, with step size %d' % (stn_cut,cut_start,cut_stop,cut_step))\n",
    "\n",
    "    if indices is None: \n",
    "        indices = lc_info['baseline_revised_i']\n",
    "\n",
    "    lim_cuts = pdastrostatsclass(columns=['PSF Chi-Square Cut', 'N', 'Ngood', 'Nbad', 'Nkept', 'Ncut', 'Ngood,kept', 'Ngood,cut', 'Nbad,kept', 'Nbad,cut',\n",
    "                                          'Pgood,kept', 'Pgood,cut', 'Pbad,kept', 'Pbad,cut', 'Ngood,kept/Ngood', 'Ploss', 'Pcontamination',\n",
    "                                          'Nbad,cut 3<stn<=5', 'Nbad,cut 5<stn<=10', 'Nbad,cut 10<stn', 'Nbad,kept 3<stn<=5', 'Nbad,kept 5<stn<=10', 'Nbad,kept 10<stn'])\n",
    "    \n",
    "    # static cut at x2 = 50\n",
    "    x2cut_50 = np.where(lc_info['lc'].t['chi/N'] < 50)[0]\n",
    "    print('Static chi square cut at 50: %0.2f%% cut for baseline' % (100*len(AnotB(indices,x2cut_50))/len(indices)))\n",
    "\n",
    "    # good baseline measurement indices\n",
    "    b_good_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stn_cut,uplim=stn_cut,indices=indices)\n",
    "    b_bad_i = AnotB(indices, b_good_i)\n",
    "    # for different x2 cuts decreasing from 50\n",
    "    for cut in range(cut_start,cut_stop+1,cut_step):\n",
    "        # kept baseline measurement indices\n",
    "        b_kept_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=cut,indices=indices)\n",
    "        b_cut_i = AnotB(indices, b_kept_i)\n",
    "\n",
    "        df = pd.DataFrame([[cut, len(indices), # N\n",
    "                            len(b_good_i), # Ngood\n",
    "                            len(b_bad_i), # Nbad\n",
    "                            len(b_kept_i), # Nkept\n",
    "                            len(b_cut_i), # Ncut\n",
    "                            len(AandB(b_good_i,b_kept_i)), # Ngood,kept\n",
    "                            len(AandB(b_good_i,b_cut_i)), # Ngood,cut\n",
    "                            len(AandB(b_bad_i,b_kept_i)), # Nbad,kept\n",
    "                            len(AandB(b_bad_i,b_cut_i)), # Nbad,cut\n",
    "                            100*len(AandB(b_good_i,b_kept_i))/len(indices), # Ngood,kept/Nbaseline\n",
    "                            100*len(AandB(b_good_i,b_cut_i))/len(indices), # Ngood,cut/Nbaseline \n",
    "                            100*len(AandB(b_bad_i,b_kept_i))/len(indices), # Nbad,kept/Nbaseline\n",
    "                            100*len(AandB(b_bad_i,b_cut_i))/len(indices), # Nbad,cut/Nbaseline\n",
    "                            100*len(AandB(b_good_i,b_kept_i))/len(b_good_i), # Ngood,kept/Ngood\n",
    "                            100*len(AandB(b_good_i,b_cut_i))/len(b_good_i), # Ngood,cut/Ngood = Loss\n",
    "                            100*len(AandB(b_bad_i,b_kept_i))/len(b_kept_i), # Nbad,kept/Nkept = Contamination\n",
    "                            len(AandB(AandB(b_bad_i,b_cut_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-3,uplim=5,exclude_lowlim=True))), # Nbad,cut 3<stn<=5\n",
    "                            len(AandB(AandB(b_bad_i,b_cut_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=5,uplim=10,exclude_lowlim=True))), # Nbad,cut 5<stn<=10\n",
    "                            len(AandB(AandB(b_bad_i,b_cut_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=10,exclude_lowlim=True))), # Nbad,cut 10<stn \n",
    "                            len(AandB(AandB(b_bad_i,b_kept_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-3,uplim=5,exclude_lowlim=True))), # Nbad,kept 3<stn<=5\n",
    "                            len(AandB(AandB(b_bad_i,b_kept_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=5,uplim=10,exclude_lowlim=True))), # Nbad,kept 5<stn<=10\n",
    "                            len(AandB(AandB(b_bad_i,b_kept_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=10,exclude_lowlim=True))), # Nbad,kept 10<stn \n",
    "                            ]], columns=['PSF Chi-Square Cut', 'N', 'Ngood', 'Nbad', 'Nkept', 'Ncut', 'Ngood,kept', 'Ngood,cut', 'Nbad,kept', 'Nbad,cut',\n",
    "                                         'Pgood,kept', 'Pgood,cut', 'Pbad,kept', 'Pbad,cut', 'Ngood,kept/Ngood', 'Ploss', 'Pcontamination',\n",
    "                                         'Nbad,cut 3<stn<=5', 'Nbad,cut 5<stn<=10', 'Nbad,cut 10<stn', 'Nbad,kept 3<stn<=5', 'Nbad,kept 5<stn<=10', 'Nbad,kept 10<stn'])\n",
    "        lim_cuts.t = pd.concat([lim_cuts.t,df],ignore_index=True)\n",
    "    return lim_cuts\n",
    "\n",
    "if lim_to_prioritize != 'loss_lim' and lim_to_prioritize != 'contam_lim':\n",
    "    print(\"ERROR: lim_to_prioritize must be 'loss_lim' or 'contam_lim'!\")\n",
    "    sys.exit()\n",
    "print('Contamination limit: %0.2f%%\\nLoss limit: %0.2f%%' % (contam_lim,loss_lim))\n",
    "\n",
    "lim_cuts = get_lim_cuts_table(stn_cut, cut_start, cut_stop, cut_step)\n",
    "contam_lim_cut, loss_lim_cut, contam_case, loss_case = get_lim_cuts(lim_cuts)\n",
    "lc_info['contam_case'] = contam_case\n",
    "lc_info['loss_case'] = loss_case\n",
    "lc_info['contam_lim_cut'] = contam_lim_cut\n",
    "lc_info['loss_lim_cut'] = loss_lim_cut\n",
    "\n",
    "print('\\nContamination cut according to given contam_limit, with %0.2f%% contamination and %0.2f%% loss: %0.2f' % (lc_info['contam_lim_cut_Pcontamination'], lc_info['contam_lim_cut_Ploss'], contam_lim_cut))\n",
    "if lc_info['contam_case'] == 'above lim':\n",
    "    print('WARNING: Contamination cut not possible with contamination <= contam_lim %0.1f!' % contam_lim)\n",
    "print('Loss cut according to given loss_limit, with %0.2f%% contamination and %0.2f%% loss: %0.2f' % (lc_info['loss_lim_cut_Pcontamination'], lc_info['loss_lim_cut_Ploss'], loss_lim_cut))\n",
    "if lc_info['loss_case'] == 'above lim':\n",
    "    print('WARNING: Loss cut not possible with loss <= loss_lim %0.2f!' % loss_lim)\n",
    "\n",
    "final_cut = choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case)\n",
    "lc_info['final_cut'] = final_cut\n",
    "        \n",
    "if np.isnan(final_cut):\n",
    "    print('\\nERROR: Final suggested chi-square cut could not be determined. We suggest rethinking your contamination and loss limits.')\n",
    "    lc_info['Pcontamination'] = np.nan\n",
    "    lc_info['Ploss'] = np.nan\n",
    "else:\n",
    "    if final_cut==contam_lim_cut:\n",
    "        lc_info['Pcontamination'] = lc_info['contam_lim_cut_Pcontamination']\n",
    "        lc_info['Ploss'] = lc_info['contam_lim_cut_Ploss']\n",
    "    else:\n",
    "        lc_info['Pcontamination'] = lc_info['loss_lim_cut_Pcontamination']\n",
    "        lc_info['Ploss'] = lc_info['loss_lim_cut_Ploss']\n",
    "    print('\\nFinal suggested chi-square cut is %0.2f, with %0.2f%% contamination and %0.2f%% loss.' % (final_cut, lc_info['Pcontamination'], lc_info['Ploss']))\n",
    "    if (lc_info['Pcontamination'] > contam_lim):\n",
    "        print('WARNING: Final cut\\'s contamination %0.2f%% exceeds contam_lim %0.2f%%!' % (lc_info['Pcontamination'],contam_lim))\n",
    "    if (lc_info['Ploss'] > loss_lim):\n",
    "        print('WARNING: Final cut\\'s loss exceeds %0.2f%% loss_lim %0.2f%%!' % (lc_info['Ploss'],loss_lim))\n",
    "\n",
    "plot_lim_cuts(lim_cuts, contam_lim_cut, loss_lim_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the final chi-square cut and update 'Mask' column\n",
    "\n",
    "answer = input('Accept final chi-square cut of %0.2f (y/n):' % float(lc_info['final_cut']))\n",
    "if answer != 'y':\n",
    "    final_cut_override = float(input('Overriding final chi-square cut; enter manual cut: '))\n",
    "\n",
    "    b_good_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stn_cut,uplim=stn_cut,indices=lc_info['baseline_i'])\n",
    "    b_bad_i = AnotB(lc_info['baseline_i'], b_good_i)\n",
    "    b_kept_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=final_cut_override,indices=lc_info['baseline_i'])\n",
    "    b_cut_i = AnotB(lc_info['baseline_i'], b_kept_i)\n",
    "    lc_info['Ploss'] = 100*len(AandB(b_good_i,b_cut_i))/len(b_good_i)\n",
    "    lc_info['Pcontamination'] = 100*len(AandB(b_bad_i,b_kept_i))/len(b_kept_i)\n",
    "    lc_info['final_cut'] = final_cut_override\n",
    "\n",
    "    print('Overridden: final cut is now %0.2f, with contamination %0.2f%% and loss %0.2f%%' % (lc_info['final_cut'],lc_info['Pcontamination'],lc_info['Ploss']))\n",
    "\n",
    "kept_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=lc_info['final_cut'])\n",
    "cut_i = AnotB(lc_info['lc'].getindices(), kept_i)\n",
    "update_mask_col(lc_info['lc'].t, flag_chisquare, cut_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ATLAS light curve with the chi-square cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, manually enter the plot's x and y limits:\n",
    "xlim_lower = 58900\n",
    "xlim_upper = 59800\n",
    "ylim_lower = -300\n",
    "ylim_upper = 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cut light curve\n",
    "plot_cut_lc('lc', flag_uncertainty|flag_chisquare, add2title='chi-square and uncertainty cuts', dflux_colname=dflux_colnames[0], xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the light curve with the new 'Mask' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the light curve with the new 'Mask' column?\n",
    "save = False\n",
    "\n",
    "# Overwrite the old light curve file?\n",
    "overwrite_old_lc = False\n",
    "\n",
    "# If not overriding old light curve file, enter new filename:\n",
    "filename_new = '/Users/sofiarest/Google Drive/My Drive/College/STScI Research Paper/atlaslc_chisquare/brightsne/2019vxm/NEW_2019vxm_i000.o.lc.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save light curve\n",
    "if save:\n",
    "    print('Saving light curve with updated mask column...')\n",
    "    if overwrite_old_lc:\n",
    "        print('Overwriting old light curve file at %s... ' % filename)\n",
    "        save_lc('lc',filename,overwrite=True) #lc_info['lc'].write(filename,overwrite=True)\n",
    "    else:\n",
    "        print('Writing new file at %s... ' % filename_new)\n",
    "        save_lc('lc',filename_new,overwrite=True) #lc_info['lc'].write(filename_new,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Light Curves: 3σ-Clipped Average Cut\n",
    "\n",
    "While the chi-square and uncertainty cuts are effective in cutting out a majority of the bad measurements, tricky cases may require a larger set of control light curves that can be used as a basis of comparison for inconsistent flux. In order to account for this inconsistent flux, we can obtain ~8 quality control forced photometry light curves in a 17\" circle pattern around the SN location OR around a nearby bright object that may be poorly subtracting. Then, we use statistics from these control light curves to cut bad measurements from the SN light curve.\n",
    "\n",
    "For a given epoch, we have 1 SN measurement for which we examine 8 control measurements within the same epoch. We know that if the control light curve measurements are NOT consistent with 0, this indicates something wrong with this epoch, so the SN measurement is unreliable. Therefore, we obtain statistics for the control light curves by calculating the 3σ-clipped average of the control flux. \n",
    "\n",
    "For the given epoch, we cut the SN measurement for which the returned control statistics fulfill any of the following criteria: \n",
    "- A returned chi-square > 2.5\n",
    "- A returned abs(flux/dflux) > 3.0\n",
    "- Number of clipped/\"bad\" measurements in the 3σ-clipped average > 2\n",
    "- Number of used/\"good\" measurements in the 3σ-clipped average < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the bound for an epoch's maximum chi-square \n",
    "# (if x2 > x2_max, flag SN measurement):\n",
    "x2_max = 3\n",
    "\n",
    "# Enter the bound for an epoch's maximum abs(flux/dflux) ratio \n",
    "# (if abs(flux/dflux) > stn_max, flag SN measurement):\n",
    "stn_max = 3.0\n",
    "\n",
    "# Enter the bound for an epoch's maximum number of clipped control measurements\n",
    "# (if Nclip > Nclip_max, flag SN measurement):\n",
    "Nclip_max = 2\n",
    "\n",
    "# Enter the bound for an epoch's minimum number of good control measurements\n",
    "# (if Ngood < Ngood_min, flag SN measurement):\n",
    "Ngood_min = 3\n",
    "\n",
    "# Optionally, manually enter the plot's x and y limits:\n",
    "xlim_lower = 58900\n",
    "xlim_upper = 59800\n",
    "ylim_lower = -300\n",
    "ylim_upper = 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_control_stats(Ncontrols):\n",
    "    print('\\nCalculating control light curve statistics...')\n",
    "\n",
    "    # construct arrays for control lc data\n",
    "    uJy = np.full((Ncontrols, len(lc_info['lc'].t['MJD'])), np.nan)\n",
    "    duJy = np.full((Ncontrols, len(lc_info['lc'].t['MJD'])), np.nan)\n",
    "    Mask = np.full((Ncontrols, len(lc_info['lc'].t['MJD'])), 0, dtype=np.int32)\n",
    "    \n",
    "    for control_index in range(1,Ncontrols+1):\n",
    "        if (len(controls[control_index].t) != len(lc_info['lc'].t['MJD'])) or (np.array_equal(lc_info['lc'].t['MJD'], controls[control_index].t['MJD']) is False):\n",
    "            raise RuntimeError(f'ERROR: SN lc not equal to control lc for control_index {control_index}! Rerun or debug verify_mjds().')\n",
    "        else:\n",
    "            uJy[control_index-1,:] = controls[control_index].t['uJy']\n",
    "            duJy[control_index-1,:] = controls[control_index].t[dflux_colnames[control_index]]\n",
    "            Mask[control_index-1,:] = controls[control_index].t['Mask']\n",
    "\n",
    "    c2_param2columnmapping = lc_info['lc'].intializecols4statparams(prefix='c2_',format4outvals='{:.2f}',skipparams=['converged','i'])\n",
    "\n",
    "    for index in range(uJy.shape[-1]):\n",
    "        pda4MJD = pdastrostatsclass()\n",
    "        pda4MJD.t['uJy'] = uJy[1:,index]\n",
    "        pda4MJD.t[dflux_colnames[0]] = duJy[1:,index]\n",
    "        pda4MJD.t['Mask'] = np.bitwise_and(Mask[1:,index], flag_chisquare|flag_uncertainty)\n",
    "        \n",
    "        pda4MJD.calcaverage_sigmacutloop('uJy',noisecol=dflux_colnames[0],maskcol='Mask',maskval=(flag_chisquare|flag_uncertainty),verbose=1,Nsigma=3.0,median_firstiteration=True)\n",
    "        lc_info['lc'].statresults2table(pda4MJD.statparams, c2_param2columnmapping, destindex=index)\n",
    "\n",
    "def controls_cut():\n",
    "    print('Flagging SN light curve based on control light curve statistics...')\n",
    "\n",
    "    lc_info['lc'].t['c2_abs_stn'] = lc_info['lc'].t['c2_mean']/lc_info['lc'].t['c2_mean_err']\n",
    "\n",
    "    # flag measurements according to given bounds\n",
    "    flag_x2_i = lc_info['lc'].ix_inrange(colnames=['c2_X2norm'], lowlim=x2_max, exclude_lowlim=True)\n",
    "    flag_stn_i = lc_info['lc'].ix_inrange(colnames=['c2_abs_stn'], lowlim=stn_max, exclude_lowlim=True)\n",
    "    flag_nclip_i = lc_info['lc'].ix_inrange(colnames=['c2_Nclip'], lowlim=Nclip_max, exclude_lowlim=True)\n",
    "    flag_ngood_i = lc_info['lc'].ix_inrange(colnames=['c2_Ngood'], uplim=Ngood_min, exclude_uplim=True)\n",
    "    lc_info['lc'].t.loc[flag_x2_i,'Mask'] |= flag_controls_x2\n",
    "    lc_info['lc'].t.loc[flag_stn_i,'Mask'] |= flag_controls_stn\n",
    "    lc_info['lc'].t.loc[flag_nclip_i,'Mask'] |= flag_controls_Nclip\n",
    "    lc_info['lc'].t.loc[flag_ngood_i,'Mask'] |= flag_controls_Ngood\n",
    "\n",
    "    # update mask column with control light curve cut on any measurements flagged according to given bounds\n",
    "    zero_Nclip_i = lc_info['lc'].ix_equal('c2_Nclip', 0)\n",
    "    unmasked_i = lc_info['lc'].ix_unmasked('Mask', maskval=flag_controls_x2|flag_controls_stn|flag_controls_Nclip|flag_controls_Ngood)\n",
    "    lc_info['lc'].t.loc[AnotB(unmasked_i,zero_Nclip_i),'Mask'] |= flag_controls_questionable\n",
    "    lc_info['lc'].t.loc[AnotB(lc_info['lc'].getindices(),unmasked_i),'Mask'] |= flag_controls_bad\n",
    "\n",
    "    # copy over SN's control cut flags to control light curve 'Mask' column\n",
    "    flags_arr = np.full(lc_info['lc'].t['Mask'].shape, (flag_badday|flag_controls_questionable|flag_controls_x2|flag_controls_stn|flag_controls_Nclip|flag_controls_Ngood))\n",
    "    flags_to_copy = np.bitwise_and(lc_info['lc'].t['Mask'], flags_arr)\n",
    "    for control_index in range(1,Ncontrols+1):\n",
    "        controls[control_index].t['Mask'] = controls[control_index].t['Mask'].astype(np.int32)\n",
    "        if len(controls[control_index].t) < 1:\n",
    "            continue\n",
    "        elif len(controls[control_index].t) == 1:\n",
    "            controls[control_index].t.loc[0,'Mask']= int(controls[control_index].t.loc[0,'Mask']) | flags_to_copy\n",
    "        else:\n",
    "            controls[control_index].t['Mask'] = np.bitwise_or(controls[control_index].t['Mask'], flags_to_copy)\n",
    "\n",
    "def print_flag_stats():\n",
    "    print('\\nLength of SN light curve: %d' % len(lc_info['lc'].t))\n",
    "    print('Percent of data above x2_max bound: %0.2f%%' % (100*len(lc_info['lc'].ix_masked('Mask',maskval=flag_controls_x2))/len(lc_info['lc'].t)))\n",
    "    print('Percent of data above stn_max bound: %0.2f%%' % (100*len(lc_info['lc'].ix_masked('Mask',maskval=flag_controls_stn))/len(lc_info['lc'].t)))\n",
    "    print('Percent of data above Nclip_max bound: %0.2f%%' % (100*len(lc_info['lc'].ix_masked('Mask',maskval=flag_controls_Nclip))/len(lc_info['lc'].t)))\n",
    "    print('Percent of data below Ngood_min bound: %0.2f%%' % (100*len(lc_info['lc'].ix_masked('Mask',maskval=flag_controls_Ngood))/len(lc_info['lc'].t)))\n",
    "    print('Total percent of data flagged as bad: %0.2f%%' % (100*len(lc_info['lc'].ix_masked('Mask',maskval=flag_controls_bad))/len(lc_info['lc'].t)))\n",
    "    print('Total percent of data flagged as questionable (not masked with control light curve flags but Nclip > 0): %0.2f%%' % (100*len(lc_info['lc'].ix_masked('Mask',maskval=flag_controls_questionable))/len(lc_info['lc'].t)))\n",
    "    \n",
    "plot_control_lcs(xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)\n",
    "get_control_stats(Ncontrols)\n",
    "controls_cut()\n",
    "print_flag_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ATLAS light curve with the chi-square, uncertainty, and control light curve cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, manually enter the plot's x and y limits:\n",
    "xlim_lower = 58900\n",
    "xlim_upper = 59800\n",
    "ylim_lower = -300\n",
    "ylim_upper = 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the light curve\n",
    "\n",
    "plot_cut_lc('lc', flag_controls_bad|flag_chisquare|flag_uncertainty, add2title='\\nchi-square, uncertainty, and control light curve cuts', xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)#flag_uncertainty|flag_chisquare|flag_controls_bad, xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the light curve with the new 'Mask' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the light curve with the new 'Mask' column?\n",
    "save = False\n",
    "\n",
    "# Overwrite the old light curve file?\n",
    "overwrite_old_lc = False\n",
    "\n",
    "# If not overwriting old light curve file, enter new filename:\n",
    "filename_new = '/Users/sofiarest/Google Drive/My Drive/College/STScI Research Paper/atlaslc_chisquare/brightsne/2019vxm/NEW_2019vxm_i000.o.lc.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save light curve\n",
    "if save:\n",
    "    print('Saving light curve with updated mask column...')\n",
    "    if overwrite_old_lc:\n",
    "        print('Overwriting old light curve file at %s... ' % filename)\n",
    "        save_lc('lc',filename,overwrite=True)\n",
    "    else:\n",
    "        print('Writing new file at %s... ' % filename_new)\n",
    "        save_lc('lc',filename_new,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging and Cutting Bad Days\n",
    "\n",
    "Our goal is to identify and cut out bad days by taking a 3σ-clipped average of each day. For each day, we calculate the 3σ-clipped average of any SN measurements falling within that day and use that average as our flux for that day. Because the ATLAS survey takes about 4 exposures every 2 days, we usually average together approximately 4 measurements per epoch. However, out of these 4 exposures, only measurements not cut in the previous methods are averaged in the 3σ-clipped average cut. (The exception to this statement would be the case that all 4 measurements are cut in previous methods; in this case, they are averaged anyway and flagged as a bad day.)\n",
    "\n",
    "Then we cut any measurements in the SN light curve for the given epoch for which statistics fulfill any of the following criteria: \n",
    "- A returned chi-square > 4.0\n",
    "- Number of measurements averaged < 2\n",
    "- Number of measurements clipped > 1\n",
    "\n",
    "For this part of the cleaning, we still need to improve the cutting at the peak of the SN (important epochs are sometimes cut, maybe due to fast rise, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the MJD bin size in days:\n",
    "mjdbinsize = 1\n",
    "\n",
    "# Should MJD bins with no measurements be translated as NaN (True) \n",
    "# or removed from the averaged light curve (False)?\n",
    "keep_empty_bins = False\n",
    "\n",
    "# After flux is averaged, average magnitudes are calculated using a flux-to-magnitude conversion.\n",
    "# Magnitudes are limits if the dmagnitude is NaN. Enter these magnitudes' sigma limit:\n",
    "flux2mag_sigma_limit = 3\n",
    "\n",
    "# Enter the bound for a bin's maximum number of clipped measurements\n",
    "# (if Nclip > Nclip_max, flag day):\n",
    "Nclip_max = 1\n",
    "\n",
    "# Enter the bound for a bin's minimum number of good measurements\n",
    "# (if Ngood < Ngood_min, flag day):\n",
    "Ngood_min = 2\n",
    "\n",
    "# Enter the bound for a bin's maximum chi-square (if x2 > x2_max, flag day):\n",
    "x2_max = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the light curve\n",
    "\n",
    "def average_lc(Nclip_max, Ngood_min, x2_max, flag_badday, flag_ixclip, flag_smallnum, mjdbinsize=1, flux2mag_sigma_limit=3.0, keep_empty_bins=True):\n",
    "    mjd = int(np.amin(lc_info['lc'].t['MJD']))\n",
    "    mjd_max = int(np.amax(lc_info['lc'].t['MJD']))+1\n",
    "\n",
    "    good_i = lc_info['lc'].ix_unmasked('Mask', maskval=flag_chisquare|flag_uncertainty)\n",
    "\n",
    "    while mjd <= mjd_max:\n",
    "        range_i = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=mjd, uplim=mjd+mjdbinsize, exclude_uplim=True)\n",
    "        range_good_i = AandB(range_i,good_i)\n",
    "\n",
    "        # add new row to avglc if keep_empty_bins or any measurements present\n",
    "        if keep_empty_bins or len(range_i) >= 1:\n",
    "            new_row = {'MJDbin':mjd+0.5*mjdbinsize, 'Nclip':0, 'Ngood':0, 'Nexcluded':len(range_i)-len(range_good_i), 'Mask':0}\n",
    "            avglc_index = lc_info['avglc'].newrow(new_row)\n",
    "        \n",
    "        # if no measurements present, flag or skip over day\n",
    "        if len(range_i) < 1:\n",
    "            if keep_empty_bins:\n",
    "                update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "            mjd += mjdbinsize\n",
    "            continue\n",
    "        \n",
    "        # if no good measurements, average values anyway and flag\n",
    "        if len(range_good_i) < 1:\n",
    "            # average flux\n",
    "            lc_info['lc'].calcaverage_sigmacutloop('uJy', noisecol='duJy_new', indices=range_i, Nsigma=3.0, median_firstiteration=True)\n",
    "            fluxstatparams = deepcopy(lc_info['lc'].statparams)\n",
    "\n",
    "            # average mjd\n",
    "            # SHOULD NOISECOL HERE BE DUJY OR NONE??\n",
    "            lc_info['lc'].calcaverage_sigmacutloop('MJD', noisecol='duJy_new', indices=fluxstatparams['ix_good'], Nsigma=0, median_firstiteration=False)\n",
    "            avg_mjd = lc_info['lc'].statparams['mean']\n",
    "\n",
    "            # add row and flag\n",
    "            lc_info['avglc'].add2row(avglc_index, {'MJD':avg_mjd, \n",
    "                                                   'uJy':fluxstatparams['mean'], \n",
    "                                                   'duJy':fluxstatparams['mean_err'], \n",
    "                                                   'stdev':fluxstatparams['stdev'],\n",
    "                                                   'x2':fluxstatparams['X2norm'],\n",
    "                                                   'Nclip':fluxstatparams['Nclip'],\n",
    "                                                   'Ngood':fluxstatparams['Ngood'],\n",
    "                                                   'Mask':0})\n",
    "            update_mask_col(lc_info['lc'].t, flag_badday, range_i)\n",
    "            update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "\n",
    "            mjd += mjdbinsize\n",
    "            continue\n",
    "        \n",
    "        # average good measurements\n",
    "        lc_info['lc'].calcaverage_sigmacutloop('uJy', noisecol='duJy_new', indices=range_good_i, Nsigma=3.0, median_firstiteration=True)\n",
    "        fluxstatparams = deepcopy(lc_info['lc'].statparams)\n",
    "\n",
    "        if fluxstatparams['mean'] is None or len(fluxstatparams['ix_good']) < 1:\n",
    "            update_mask_col(lc_info['lc'].t, flag_badday, range_i)\n",
    "            update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "            mjd += mjdbinsize\n",
    "            continue\n",
    "\n",
    "        # average mjd\n",
    "        # SHOULD NOISECOL HERE BE DUJY OR NONE??\n",
    "        lc_info['lc'].calcaverage_sigmacutloop('MJD', noisecol='duJy_new', indices=fluxstatparams['ix_good'], Nsigma=0, median_firstiteration=False)\n",
    "        avg_mjd = lc_info['lc'].statparams['mean']\n",
    "\n",
    "        # add row\n",
    "        lc_info['avglc'].add2row(avglc_index, {'MJD':avg_mjd, \n",
    "                                                   'uJy':fluxstatparams['mean'], \n",
    "                                                   'duJy':fluxstatparams['mean_err'], \n",
    "                                                   'stdev':fluxstatparams['stdev'],\n",
    "                                                   'x2':fluxstatparams['X2norm'],\n",
    "                                                   'Nclip':fluxstatparams['Nclip'],\n",
    "                                                   'Ngood':fluxstatparams['Ngood'],\n",
    "                                                   'Mask':0})\n",
    "        \n",
    "        # flag clipped measurements in lc\n",
    "        if len(fluxstatparams['ix_clip']) > 0:\n",
    "            update_mask_col(lc_info['lc'].t, flag_ixclip, fluxstatparams['ix_clip'])\n",
    "        \n",
    "        # if small number within this bin, flag measurements\n",
    "        if len(range_good_i) < 3:\n",
    "            update_mask_col(lc_info['lc'].t, flag_smallnum, range_good_i) # CHANGE TO RANGE_I??\n",
    "            update_mask_col(lc_info['avglc'].t, flag_smallnum, [avglc_index])\n",
    "        # else check sigmacut bounds and flag\n",
    "        else:\n",
    "            is_bad = False\n",
    "            if fluxstatparams['Ngood'] < Ngood_min:\n",
    "                is_bad = True\n",
    "            if fluxstatparams['Nclip'] > Nclip_max:\n",
    "                is_bad = True\n",
    "            if not(fluxstatparams['X2norm'] is None) and fluxstatparams['X2norm'] > x2_max:\n",
    "                is_bad = True\n",
    "            if is_bad:\n",
    "                update_mask_col(lc_info['lc'].t, flag_badday, range_i)\n",
    "                update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "\n",
    "        mjd += mjdbinsize\n",
    "    \n",
    "    # convert flux to magnitude and dflux to dmagnitude\n",
    "    lc_info['avglc'].flux2mag('uJy','duJy','m','dm', zpt=23.9, upperlim_Nsigma=flux2mag_sigma_limit)\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "    drop_extra_columns('avglc')\n",
    "\n",
    "    for col in ['Nclip','Ngood','Nexcluded','Mask']: \n",
    "        lc_info['avglc'].t[col] = lc_info['avglc'].t[col].astype(np.int32)\n",
    "\n",
    "if len(lc_info['lc'].t) < 1:\n",
    "    print('ERROR: No data in lc so cannot average; exiting... ')\n",
    "    sys.exit()\n",
    "\n",
    "print('Averaging light curve with the following criteria: Nclip_max = %d, Ngood_min = %d, x2_max = %0.2f... ' % (Nclip_max, Ngood_min, x2_max))\n",
    "lc_info['avglc'] = pdastrostatsclass(columns=['MJD','MJDbin','uJy','duJy','stdev','x2','Nclip','Ngood','Nexcluded','Mask'],hexcols=['Mask'])\n",
    "average_lc(Nclip_max, Ngood_min, x2_max, flag_badday, flag_ixclip, flag_smallnum, keep_empty_bins=keep_empty_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the averaged light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, manually enter the plot's x and y limits:\n",
    "xlim_lower = 58900\n",
    "xlim_upper = 59800\n",
    "ylim_lower = -300\n",
    "ylim_upper = 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot averaged light curve\n",
    "\n",
    "plot_cut_lc('avglc', flag_chisquare|flag_uncertainty|flag_badday, xlim_lower=xlim_lower, xlim_upper=xlim_upper, ylim_lower=ylim_lower, ylim_upper=ylim_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the averaged light curve with the new 'Mask' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the light curve with the new 'Mask' column?\n",
    "save = True\n",
    "\n",
    "# Overwrite the old light curve file?\n",
    "overwrite_old_lc = False\n",
    "\n",
    "# If not overwriting old light curve file, enter new filename:\n",
    "filename_new = '/Users/sofiarest/Google Drive/My Drive/College/STScI Research Paper/atlaslc_chisquare/brightsne/2019vxm/NEW_2019vxm_i000.o.1.00days.lc.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save light curve\n",
    "if save:\n",
    "    print('Saving light curve with updated mask column...')\n",
    "    if overwrite_old_lc:\n",
    "        print('Overwriting old light curve file at %s... ' % filename)\n",
    "        save_lc('avglc',filename,overwrite=True) #lc_info['lc'].write(filename,overwrite=True)\n",
    "    else:\n",
    "        print('Writing new file at %s... ' % filename_new)\n",
    "        save_lc('avglc',filename_new,overwrite=True) #lc_info['lc'].write(filename_new,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
