{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-Square Cut and Averaging\n",
    "\n",
    "## Part 1: Chi-Square Cut\n",
    "\n",
    "This iPython notebook will help you ascertain the best chi-square cut for a certain supernova's ATLAS light curve, then average together the light curve after specifying the MJD bin size. After running a cell, the descriptions located above that cell will help you interpret the plots and make decisions about the supernova.\n",
    "\n",
    "This notebook takes into account ATLAS's periodic replacement of the difference image reference templates, which may cause step discontinuities in flux. Two template changes have been recorded at MJDs 58417 and 58882. More information can be found here: https://fallingstar-data.com/forcedphot/faq/.\n",
    "\n",
    "In order for this notebook to work correctly, the ATLAS light curve must already be downloaded and saved. It must also only include measurements for a single filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules, set preliminary variables, etc.\n",
    "\n",
    "import sys, re\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import sigmacut\n",
    "\n",
    "\n",
    "# storing, accessing, and manipulating the light curve\n",
    "import pandas as pd\n",
    "from pdastro import pdastrostatsclass, AandB, AnotB, AorB, not_AandB\n",
    "\n",
    "# getting discovery date from TNS\n",
    "import requests, json\n",
    "from collections import OrderedDict\n",
    "from astropy.time import Time\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as matlib\n",
    "import warnings\n",
    "warnings.simplefilter('error', RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# plotting styles\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=13)\n",
    "plt.rc('ytick', labelsize=13)\n",
    "plt.rc('legend', fontsize=13)\n",
    "plt.rc('font', size=13)\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "#plt.rcParams['font.serif'] = 'Times'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# ATLAS template changes\n",
    "global tchange1\n",
    "global tchange2\n",
    "tchange1 = 58417\n",
    "tchange2 = 58882\n",
    "\n",
    "# dictionary for storing important information about the light curve\n",
    "global lc_info\n",
    "lc_info = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Load the ATLAS light curve, account for template changes, etc.\n",
    "\n",
    "Before analysis on the chi-squares of the light curve can be done, we correct for any potential flux in the template. We do this by calculating the median of any baseline flux with a chi-square less than or equal to 5 and then subtracting that median from the entire light curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the target SN name:\n",
    "tnsname = '2019vxm'\n",
    "\n",
    "# Enter the SN light curve file name:\n",
    "filename = '/Users/sofiarest/Google Drive/My Drive/College/STScI Research Paper/atlaslc_chisquare/brightsne/2019vxm/2019vxm_i000.o.lc.txt'\n",
    "\n",
    "# Enter the filter for this light curve (must be 'o' or 'c'):\n",
    "filter = 'o'\n",
    "\n",
    "# Optionally, enter the SN's discovery date (if None is entered, it will be \n",
    "# fetched automatically from TNS using api_key):\n",
    "discdate = 58818.04\n",
    "api_key = None\n",
    "\n",
    "# Optionally, enter the x limits for the flux (µJy) distribution histograms:\n",
    "xlim_lower = -200\n",
    "xlim_upper = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get discovery date if needed, load in light curve, account for template changes, \n",
    "# and add uJy/duJy column\n",
    "\n",
    "def get_tns_data(tnsname, api_key):\n",
    "\ttry:\n",
    "\t\tget_obj = [(\"objname\",tnsname), (\"objid\",\"\"), (\"photometry\",\"1\"), (\"spectra\",\"1\")]\n",
    "\t\tget_url = 'https://www.wis-tns.org/api/get/object'\n",
    "\t\tjson_file = OrderedDict(get_obj)\n",
    "\t\tget_data = {'api_key':api_key,'data':json.dumps(json_file)}\n",
    "\t\tresponse = requests.post(get_url, data=get_data, headers={'User-Agent':'tns_marker{\"tns_id\":104739,\"type\": \"bot\", \"name\":\"Name and Redshift Retriever\"}'})\n",
    "\t\tjson_data = json.loads(response.text,object_pairs_hook=OrderedDict)\n",
    "\t\treturn json_data\n",
    "\texcept Exception as e:\n",
    "\t\treturn 'Error: \\n'+str(e)\n",
    "\n",
    "def get_discdate(tnsname, api_key):\n",
    "\tjson_data = get_tns_data(tnsname, api_key)\n",
    "\tdiscoverydate = json_data['data']['reply']['discoverydate']\n",
    "\tdate = list(discoverydate.partition(' '))[0]\n",
    "\ttime = list(discoverydate.partition(' '))[2]\n",
    "\tdisc_date_format = date+'T'+time\n",
    "\tdateobjects = Time(disc_date_format, format='isot', scale='utc')\n",
    "\tdisc_date = dateobjects.mjd\n",
    "\treturn disc_date\n",
    "\n",
    "def get_regions():\n",
    "\tregions = {}\n",
    "\tregions['t0'] = lc_info['lc'].ix_inrange(colnames=['MJD'],uplim=tchange1)\n",
    "\tregions['t1'] = lc_info['lc'].ix_inrange(colnames=['MJD'],lowlim=tchange1,uplim=tchange2)\n",
    "\tregions['t2'] = lc_info['lc'].ix_inrange(colnames=['MJD'],lowlim=tchange2)\n",
    "\tregions['b_t0'] = lc_info['lc'].ix_inrange(colnames=['MJD'],uplim=tchange1,indices=lc_info['baseline_i'])\n",
    "\tregions['b_t1'] = lc_info['lc'].ix_inrange(colnames=['MJD'],lowlim=tchange1,uplim=tchange2,indices=lc_info['baseline_i'])\n",
    "\tregions['b_t2'] = lc_info['lc'].ix_inrange(colnames=['MJD'],lowlim=tchange2,indices=lc_info['baseline_i'])\n",
    "\treturn regions\n",
    "\n",
    "def count_valid_regions(regions):\n",
    "\tcount = 0\n",
    "\tfor region_index in range(0,3):\n",
    "\t\tif len(regions['b_t%d'%region_index]) > 0:\n",
    "\t\t\tcount += 1\n",
    "\treturn count\n",
    "\n",
    "def plot_fdf(ax, region_i,b_goodx2_i,b_badx2_i,median=None,xlim_lower=None,xlim_upper=None):\n",
    "\tax.set_title('Baseline region (MJD): %d-%d' % (lc_info['lc'].t.loc[region_i[0],'MJD'], lc_info['lc'].t.loc[region_i[-1],'MJD']), size=15)\n",
    "\tax.set_xlabel('Baseline flux (µJy)')\n",
    "\tplt.axvline(x=median,color='magenta')\n",
    "\tplt.gca().spines['right'].set_visible(False)\n",
    "\tplt.gca().spines['top'].set_visible(False)\n",
    "\tif len(lc_info['baseline_i'])>0: \n",
    "\t\tif xlim_lower is None: \n",
    "\t\t\txlim_lower = min(lc_info['lc'].t.loc[lc_info['baseline_i'], 'uJy'])\n",
    "\t\tif xlim_upper is None:\n",
    "\t\t\txlim_upper = max(lc_info['lc'].t.loc[lc_info['baseline_i'], 'uJy'])\n",
    "\t\tax.hist(lc_info['lc'].t.loc[b_goodx2_i, 'uJy'], bins=30, color='orange', alpha=0.5, range=(xlim_lower,xlim_upper), density=True)\n",
    "\t\tax.hist(lc_info['lc'].t.loc[b_badx2_i, 'uJy'], bins=30, color='blue', alpha=0.5, range=(xlim_lower,xlim_upper), density=True)\n",
    "\telse:\n",
    "\t\tax.hist(lc_info['lc'].t.loc[b_goodx2_i, 'uJy'], bins=30, color='orange', alpha=0.5, density=True)\n",
    "\t\tax.hist(lc_info['lc'].t.loc[b_badx2_i, 'uJy'], bins=30, color='blue', alpha=0.5, density=True)\n",
    "\n",
    "def correct4template(xlim_lower=None, xlim_upper=None):\n",
    "\tb_goodx2_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=5,indices=lc_info['baseline_i'])\n",
    "\tb_badx2_i = AnotB(lc_info['baseline_i'],b_goodx2_i)\n",
    "\n",
    "\tfig = plt.figure(figsize=(10, 5), tight_layout=True)\n",
    "\tplt.suptitle('SN %s %s-band flux before correcting for template' % (lc_info['tnsname'], lc_info['filter']), fontsize=17, y=1)\n",
    "\n",
    "\tregions = get_regions()\n",
    "\tfor region_index in range(0,3):\n",
    "\t\tregion_i = regions['b_t%d'%region_index]\n",
    "\t\tif len(region_i) > 0:\n",
    "\t\t\tprint('Adjusting for template change in region b_t%d from %0.2f-%0.2f ' % (region_index, lc_info['lc'].t.loc[region_i[0],'MJD'], lc_info['lc'].t.loc[region_i[-1],'MJD']))\n",
    "\t\t\tprint('# Baseline median before: ', np.median(lc_info['lc'].t.loc[region_i,'uJy']))\n",
    "\n",
    "\t\t\tif len(AandB(region_i,b_goodx2_i)) > 0:\n",
    "\t\t\t\tmedian = np.median(lc_info['lc'].t.loc[AandB(region_i,b_goodx2_i),'uJy'])\n",
    "\t\t\telse:\n",
    "\t\t\t\tmedian = np.median(lc_info['lc'].t.loc[region_i,'uJy'])\n",
    "\t\t\t\n",
    "\t\t\tax = fig.add_subplot(1, count_valid_regions(regions), region_index+1)\n",
    "\t\t\tplot_fdf(ax, region_i,b_goodx2_i,b_badx2_i,median=median,xlim_lower=xlim_lower,xlim_upper=xlim_upper)\n",
    "\t\t\t\n",
    "\t\t\tprint(f'# Subtracting median {median:0.1f} uJy of baseline flux with chi-square ≤ 5 from light curve flux due to potential flux in the template...')\n",
    "\t\t\tlc_info['lc'].t.loc[regions['t%d'%region_index],'uJy'] -= median\n",
    "\n",
    "\t\t\tprint('# Baseline median now: ', np.median(lc_info['lc'].t.loc[region_i,'uJy']))\n",
    "\t\telse:\n",
    "\t\t\tprint('No baseline region for region b_t%d, skipping... ' % region_index)\n",
    "\n",
    "\torange = mpatches.Patch(color='orange', alpha=0.5, label='Data with chi-square≤%d' % 5)\n",
    "\tblue = mpatches.Patch(color='blue', alpha=0.5, label='Data with chi-square>%d' % 5)\n",
    "\tmagenta = mpatches.Patch(color='magenta', alpha=0.5, label='Median of baseline flux with chi-square≤%d' % 5)\n",
    "\tfig.legend(handles=[orange, blue, magenta], loc='upper center', bbox_to_anchor=(0.5, 0), frameon=True)\n",
    "\n",
    "def drop_extra_columns(lc_type):\n",
    "\tdropcols=[]\n",
    "\tif 'Noffsetlc' in lc_info[lc_type].t.columns: dropcols.append('Noffsetlc')\n",
    "\tif '__tmp_SN' in lc_info[lc_type].t.columns: dropcols.append('__tmp_SN')\n",
    "\tfor col in lc_info[lc_type].t.columns:\n",
    "\t\tif re.search('^c\\d_',col): \n",
    "\t\t\tdropcols.append(col)\n",
    "\tif len(dropcols)>0: \n",
    "\t\tprint('Dropping extra columns: ',dropcols)\n",
    "\t\tlc_info[lc_type].t.drop(columns=dropcols,inplace=True)\n",
    "\n",
    "def load_lc(filename, xlim_lower=None, xlim_upper=None):\n",
    "\tlc_info['lc'] = pdastrostatsclass()\n",
    "\ttry:\n",
    "\t\tprint('Loading SN %s light curve at %s...' % (lc_info['tnsname'], filename))\n",
    "\t\tlc_info['lc'].load_spacesep(filename,delim_whitespace=True)\n",
    "\texcept Exception as e:\n",
    "\t\tprint('Could not load light curve for SN %s at %s: %s' % (lc_info['tnsname'], filename, str(e)))\n",
    "\t\tsys.exit()\n",
    "\t\n",
    "\tlc_info['baseline_i'] = lc_info['lc'].ix_inrange(colnames=['MJD'],uplim=lc_info['discdate'],exclude_uplim=True)\n",
    "\tif len(lc_info['baseline_i'])<=0:\n",
    "\t\tprint('Baseline length is 0--cannot find best chi-square cut! Exiting...')\n",
    "\t\tsys.exit()\n",
    "\tlc_info['afterdiscdate_i'] = AnotB(lc_info['lc'].getindices(), lc_info['baseline_i']) \n",
    "\t\n",
    "\tcorrect4template(xlim_lower=xlim_upper,xlim_upper=xlim_lower)\n",
    "\tdrop_extra_columns('lc') # comment out this line if dropping columns is giving you unexpected trouble\n",
    "\n",
    "\t# add flux/dflux column\n",
    "\tprint('Adding uJy/duJy column to light curve...')\n",
    "\tlc_info['lc'].t['uJy/duJy'] = lc_info['lc'].t['uJy']/lc_info['lc'].t['duJy']\n",
    "\tlc_info['lc'].t = lc_info['lc'].t.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "lc_info['tnsname'] = tnsname\n",
    "\n",
    "if filter != 'o' and filter != 'c': \n",
    "\tprint('Filter must be \"o\" or \"c\"!')\n",
    "\tsys.exit()\n",
    "lc_info['filter'] = filter\n",
    "\n",
    "if discdate is None:\n",
    "\tprint('Obtaining SN %s discovery date from TNS...' % lc_info['tnsname'])\n",
    "\tdiscdate = get_discdate(lc_info['tnsname'], api_key)\n",
    "\tprint('Discovery date: ',discdate)\n",
    "lc_info['discdate'] = discdate\n",
    "\n",
    "load_lc(filename, xlim_lower=xlim_upper, xlim_upper=xlim_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Plot the ATLAS light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the scaling parameter for the SN plot's upper y limit \n",
    "# (ylim_upper = scale * 95th percentile flux):\n",
    "scale = 2\n",
    "\n",
    "# Optionally, manually enter the SN plot's y limits to override automatic scaling:\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the light curve\n",
    "\n",
    "def get_xth_percentile_flux(percentile, indices):\n",
    "    if len(indices)==0: \n",
    "        return None\n",
    "    else:\n",
    "        return np.percentile(lc_info['lc'].t.loc[indices, 'uJy'], percentile)\n",
    "\n",
    "def plot_lc(ylim_lower=None, ylim_upper=None):\n",
    "    fig = plt.figure(figsize=(10,6), tight_layout=True)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.axhline(linewidth=1,color='k')\n",
    "    plt.ylabel('Flux (µJy)')\n",
    "    plt.xlabel('MJD')\n",
    "    plt.title('SN %s %s-band flux' % (lc_info['tnsname'],lc_info['filter']))\n",
    "    plt.axvline(x=tchange1,color='magenta', label='ATLAS template change')\n",
    "    plt.axvline(x=tchange2,color='magenta')\n",
    "\n",
    "    # set y limits\n",
    "    if ylim_lower is None: ylim_lower = -200\n",
    "    if ylim_upper is None: ylim_upper = scale*get_xth_percentile_flux(95, lc_info['afterdiscdate_i'])\n",
    "    plt.ylim(ylim_lower,ylim_upper)\n",
    "\n",
    "    plt.scatter(lc_info['lc'].t.loc[lc_info['baseline_i'],'MJD'], lc_info['lc'].t.loc[lc_info['baseline_i'],'uJy'], \n",
    "               s=45,alpha=0.5,color='b',marker='o',label='Baseline')\n",
    "\n",
    "    plt.scatter(lc_info['lc'].t.loc[lc_info['afterdiscdate_i'],'MJD'], lc_info['lc'].t.loc[lc_info['afterdiscdate_i'],'uJy'],\n",
    "               s=45,alpha=0.5,color='c',marker='o',label='During or after SN')\n",
    "    \n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=3)\n",
    "\n",
    "plot_lc(ylim_lower=ylim_lower, ylim_upper=ylim_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Plot the flux/dflux and chi-square distributions\n",
    "\n",
    "The following two histograms display the flux/dflux and chi-square distributions of the target SN. Both histograms show probability density so as to ease comparison between the groups plotted within each histogram.\n",
    "\n",
    "- The first histogram focuses on the baseline flux/dflux (µJy/dµJy) measurements, where we can expect the flux to equal 0. In orange, we plot flux/dflux (µJy/dµJy) measurements with a chi-square value less than or equal to `x2bound`, which is currently set to 5 below; in blue, we plot flux/dflux (µJy/dµJy) measurements with a chi-square value greater than `x2bound`. \n",
    "- The second histogram focuses on the baseline chi-square measurements. In green, we plot chi-square measurements with an abs(µJy/dµJy) value less than or equal to `stnbound`, which is currently set to 3 below; in red, we plot chi-square measurements with an abs(µJy/dµJy) value greater than `stnbound`. \n",
    "\n",
    "Ideally, all measurements with a chi-square value less than or equal to `x2bound` should have an abs(µJy/dµJy) value less than or equal to `stnbound`, and measurements with a chi-square value greater than `x2bound` should have an abs(µJy/dµJy) value greater than `stnbound`. Our goal is to separate good measurements from bad measurements using a chi-square cut; in order for our cut to be effective, these histograms should hopefully showcase this relation between the target SN's flux/dflux and chi-square measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the bound that should separate a good chi-square measurement from a bad one:\n",
    "x2bound = 5.0\n",
    "\n",
    "# Enter the bound that should separate a good abs(flux/dflux) measurement from a bad one:\n",
    "stnbound = 3.0\n",
    "\n",
    "# Optionally, manually enter the histograms' x limits here:\n",
    "# flux/dflux histogram x limits:\n",
    "fdf_xlim_lower = -10\n",
    "fdf_xlim_upper = 10 #300\n",
    "# chi-square histogram x limits:\n",
    "x2_xlim_lower = None\n",
    "x2_xlim_upper = 20 #2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot flux/dflux and chi-square distribution histograms\n",
    "\n",
    "def plot_hists(x2bound, stnbound, fdf_xlim_lower=None, fdf_xlim_upper=None, x2_xlim_lower=None, x2_xlim_upper=None):\n",
    "    b_goodstn_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stnbound,uplim=stnbound,indices=lc_info['baseline_i'])\n",
    "    b_badstn_i = AnotB(lc_info['baseline_i'],b_goodstn_i)\n",
    "    b_goodx2_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=x2bound,indices=lc_info['baseline_i'])\n",
    "    b_badx2_i = AnotB(lc_info['baseline_i'],b_goodx2_i)\n",
    "\n",
    "    fig, (stn, x2) = plt.subplots(1, 2, figsize=(10, 6.5), tight_layout=True)\n",
    "    plt.suptitle('SN %s %s-band (Baseline Only)' % (lc_info['tnsname'], lc_info['filter']), fontsize=17, y=1)\n",
    "\n",
    "    stn.set_title('µJy/dµJy distribution')\n",
    "    stn.set_xlabel('µJy/dµJy')\n",
    "    stn.spines.right.set_visible(False)\n",
    "    stn.spines.top.set_visible(False)\n",
    "    orange = mpatches.Patch(color='orange', label='Data with chi-square<%0.2f' % x2bound)\n",
    "    blue = mpatches.Patch(color='blue', label='Data with chi-square≥%0.2f' % x2bound)\n",
    "    stn.legend(handles=[orange, blue], loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=1)\n",
    "    if len(lc_info['baseline_i'])>0: \n",
    "        if fdf_xlim_lower is None: \n",
    "            fdf_xlim_lower = min(lc_info['lc'].t.loc[lc_info['baseline_i'], 'uJy/duJy'])\n",
    "        if fdf_xlim_upper is None: \n",
    "            fdf_xlim_upper = max(lc_info['lc'].t.loc[lc_info['baseline_i'], 'uJy/duJy'])\n",
    "        stn.hist(lc_info['lc'].t.loc[b_goodx2_i, 'uJy/duJy'], bins=30, color='orange', alpha=0.5, range=(fdf_xlim_lower,fdf_xlim_upper), density=True)\n",
    "        stn.hist(lc_info['lc'].t.loc[b_badx2_i, 'uJy/duJy'], bins=30, color='blue', alpha=0.5, range=(fdf_xlim_lower,fdf_xlim_upper), density=True)\n",
    "    else:\n",
    "        stn.hist(lc_info['lc'].t.loc[b_goodx2_i, 'uJy/duJy'], bins=30, color='orange', alpha=0.5, density=True)\n",
    "        stn.hist(lc_info['lc'].t.loc[b_badx2_i, 'uJy/duJy'], bins=30, color='blue', alpha=0.5, density=True)\n",
    "\n",
    "    x2.set_title('Chi-square distribution')\n",
    "    x2.set_xlabel('Chi-square')\n",
    "    x2.spines.right.set_visible(False)\n",
    "    x2.spines.top.set_visible(False)\n",
    "    red = mpatches.Patch(color='green', label='Data with µJy/dµJy<%0.2f' % stnbound)\n",
    "    green = mpatches.Patch(color='red', label='Data with µJy/dµJy≥%0.2f' % stnbound)\n",
    "    x2.legend(handles=[red, green], loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=1)\n",
    "    if len(lc_info['baseline_i'])>0:\n",
    "        if x2_xlim_lower is None: \n",
    "            x2_xlim_lower = min(lc_info['lc'].t.loc[lc_info['baseline_i'], 'chi/N'])\n",
    "        if x2_xlim_upper is None: \n",
    "            x2_xlim_upper = max(lc_info['lc'].t.loc[lc_info['baseline_i'], 'chi/N'])\n",
    "        x2.hist(lc_info['lc'].t.loc[b_goodstn_i, 'chi/N'], bins=30, color='green', alpha=0.5, range=(x2_xlim_lower,x2_xlim_upper), density=True)\n",
    "        x2.hist(lc_info['lc'].t.loc[b_badstn_i, 'chi/N'], bins=30, color='red', alpha=0.5, range=(x2_xlim_lower,x2_xlim_upper), density=True)\n",
    "    else:\n",
    "        x2.hist(lc_info['lc'].t.loc[b_goodstn_i, 'chi/N'], bins=30, color='green', alpha=0.5, density=True)\n",
    "        x2.hist(lc_info['lc'].t.loc[b_badstn_i, 'chi/N'], bins=30, color='red', alpha=0.5, density=True)\n",
    "\n",
    "plot_hists(x2bound, stnbound, fdf_xlim_lower=fdf_xlim_lower, fdf_xlim_upper=fdf_xlim_upper, x2_xlim_lower=x2_xlim_lower, x2_xlim_upper=x2_xlim_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Calculate best chi-square cut based on contamination and loss\n",
    "\n",
    "The following cells use two factors, <strong>contamination</strong> and <strong>loss</strong>, to attempt to calculate an optimal PSF chi-square cut for the target SN, with flux/dflux as the deciding factor of what constitutes a good measurement vs. a bad measurement. We aim to separate good measurements from bad using the calculated chi-square cut by removing as much contamination as possible with the smallest loss possible. Since we can assume that the expected value of the baseline flux is 0, we look only at the baseline measurements before the SN occurs in order to determine the best chi-square cut for the SN itself.\n",
    "\n",
    "First, we decide what will determine a good measurement vs. a bad measurement using a factor outside of the chi-square values. Our chosen factor is the <strong>absolute value of flux (µJy) divided by dflux (dµJy)</strong>. The recommended boundary is a value of 3, such that any measurements with a value of abs(µJy/dµJy) less than or equal to 3 are regarded as \"good\" measurements, and any measurements with a value of abs(µJy/dµJy) greater than 3 are regarded as \"bad\" measurements. You can set this boundary to a different number by changing the value of `stn_cut` below.\n",
    "\n",
    "Next, we set the upper and lower bounds of our final chi-square cut. We start at a low value of 3 (which can be changed by setting the value of `cut_start` below) and end at 50 (this value is inclusive and can be changed by setting the value of `cut_stop` below) with a step size of 1 (`cut_step` below). <strong>For chi-square cuts falling on or between `cut_start` and `cut_stop` in increments of `cut_step`, we can begin to calculate contamination and loss percentages.</strong>\n",
    "\n",
    "We define contamination to be the number of bad kept measurements over the total number of kept measurements for that chi-square cut (<strong>contamination = Nbad,kept/Nkept</strong>). For our final chi-square cut, we can also set a limit on what maximum percent contamination we want to have--the recommended value is <strong>10%</strong> but can be changed by setting the value of `contam_lim` below.\n",
    "\n",
    "We define loss to be the number of good cut measurements over the total number of good measurements for that chi-square cut (<strong>loss = Ngood,cut/Ngood</strong>). For our final chi-square cut, we can also set a limit on what maximum percent loss we want to have--the recommended value is <strong>10%</strong> but can be changed by setting the value of `loss_lim` below.\n",
    "\n",
    "Finally, we define which limit (`contam_lim` or `loss_lim`) to prioritize in the event that an optimal chi-square cut fitting both limits is not found. The default prioritized limit is `loss_lim` but can be changed by setting the value of `lim_to_prioritize` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the abs(uJy/duJy) boundary that will determine a \"good\" measurement vs. \"bad\" measurement:\n",
    "stn_cut = 3\n",
    "\n",
    "# Enter the bounds for the final chi-square cut (minimum cut, maximum cut, and step):\n",
    "cut_start = 3 # this is inclusive\n",
    "cut_stop = 50 # this is inclusive\n",
    "cut_step = 1\n",
    "\n",
    "# Enter the contamination limit (contamination = Nbad,kept/Nkept must be <= contam_lim% \n",
    "# for the final chi-square cut):\n",
    "contam_lim = 15.0\n",
    "\n",
    "# Enter the loss limit (loss = Ngood,cut/Ngood must be >= loss_lim%\n",
    "# for the final chi-square cut):\n",
    "loss_lim = 10.0\n",
    "\n",
    "# Enter the limit to prioritize (must be 'loss_lim' or 'contam_lim') in the event that\n",
    "# one or both limits are not met:\n",
    "lim_to_prioritize = 'loss_lim'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section describes in detail how we determine the final chi-square cut using the given contamination and loss limits.\n",
    "\n",
    "For each given limit (contamination and loss), we calculate a range of valid cuts whose contamination/loss percentage is less than that limit and then choose a single cut within that valid range. Then, we pass through a decision tree to determine which of the two suggested cuts to use using a variety of factors (including the user's selected `lim_to_prioritize`).\n",
    "\n",
    "When choosing the loss cut according to the loss percentage limit `loss_lim`:\n",
    "- <strong>If all loss percentages are below the limit</strong> `loss_lim`, all cuts falling on or between `cut_start` and `cut_stop` are valid.\n",
    "- <strong>If all loss percentages are above the limit</strong> `loss_lim`, a cut with the required loss percentage is not possible; therefore, any cuts with the smallest percentage of loss are valid.\n",
    "- <strong>Otherwise</strong>, the valid range of cuts includes any cuts with the loss percentage less than or equal to the limit `loss_lim`.\n",
    "- The chosen cut for this limit is the <strong>minimum cut</strong> within the stated valid range of cuts.\n",
    "\n",
    "When choosing the loss cut according to the contamination percentage limit `contam_lim`:\n",
    "- <strong>If all contamination percentages are below the limit</strong> `contam_lim`, all cuts falling on or between `cut_start` and `cut_stop` are valid.\n",
    "- <strong>If all contamination percentages are above the limit</strong> `contam_lim`, a cut with the required contamination percentage is not possible; therefore, any cuts with the smallest percentage of contamination are valid.\n",
    "- <strong>Otherwise</strong>, the valid range of cuts includes any cuts with the contamination percentage less than or equal to the limit `contam_lim`.\n",
    "- The chosen cut for this limit is the <strong>maximum cut</strong> within the stated valid range of cuts.\n",
    "\n",
    "After we have calculated two suggested cuts based on the loss and contamination percentage limits, we follow the decision tree in order to suggest a final cut:\n",
    "- If both loss and contamination cut percentages were chosen from a range that spanned from `cut_start` to `cut_stop`, we set the final cut to `cut_start`.\n",
    "- If one cut's percentage was chosen from a range that spanned from `cut_start` to `cut_stop` and the other cut's percentage was not, we set the final cut to the latter cut.\n",
    "- If both percentages were chosen from ranges that fell above their respective limits, we suggest reselecting either or both limits.\n",
    "- Otherwise, we take into account the user's prioritized limit `lim_to_prioritize`:\n",
    "    - If the loss cut is greater than the contamination cut, we set the final cut to whichever cut is associated with `lim_to_prioritize`.\n",
    "    - Otherwise, if `lim_to_prioritize` is set to `contam_lim`, we set the final cut to the loss cut, and if `lim_to_prioritize` is set to `loss_lim`, we set the final cut to the contamination cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the suggested best chi-square cut using contamination and loss\n",
    "\n",
    "def plot_lim_cuts(lim_cuts, contam_lim_cut, loss_lim_cut):\n",
    "    fig = plt.figure(figsize=(10,6), tight_layout=True)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.title('SN %s %s-band chi-square cut' % (lc_info['tnsname'],lc_info['filter']))\n",
    "\n",
    "    plt.axhline(linewidth=1,color='k')\n",
    "    plt.xlabel('Chi-square cut')\n",
    "    plt.ylabel('% of baseline measurements')\n",
    "\n",
    "    plt.axhline(loss_lim,linewidth=1,color='r',linestyle='--',label='Loss limit')\n",
    "    plt.plot(lim_cuts.t['PSF Chi-Square Cut'], lim_cuts.t['Ploss'],ms=5,color='r',marker='o',label='Loss')\n",
    "    plt.axvline(x=loss_lim_cut,color='r',label='Loss cut')\n",
    "    plt.axvspan(loss_lim_cut, cut_stop, alpha=0.2, color='r')\n",
    "\n",
    "    plt.axhline(contam_lim,linewidth=1,color='g',linestyle='--',label='Contamination limit')\n",
    "    plt.plot(lim_cuts.t['PSF Chi-Square Cut'], lim_cuts.t['Pcontamination'],ms=5,color='g',marker='o',label='Contamination')\n",
    "    plt.axvline(x=contam_lim_cut,color='g',label='Contamination cut')\n",
    "    plt.axvspan(cut_start, contam_lim_cut, alpha=0.2, color='g')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    fig.tight_layout()\n",
    "    #fig.savefig('000001.png',bbox_inches=\"tight\",dpi=200)\n",
    "\n",
    "def choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case):\n",
    "    # case 1 and 1: final_cut = 3\n",
    "    # case 1 and 2: take limit of case 2\n",
    "    # case 1 and 3: take limit of case 3\n",
    "    # case 2 and 2: print lims don't work\n",
    "    # case 2 and 3: choose_btwn_lim_cuts\n",
    "    # case 3 and 3: choose_btwn_lim_cuts\n",
    "\n",
    "    case1 = loss_case == 'below lim' or contam_case == 'below lim'\n",
    "    case2 = loss_case == 'above lim' or contam_case == 'above lim'\n",
    "    case3 = loss_case == 'crosses lim' or contam_case == 'crosses lim'\n",
    "\n",
    "    final_cut = None\n",
    "    if case1 and not case2 and not case3: # 1 and 1\n",
    "        print('Valid chi-square cut range from %0.2f to %0.2f! Setting to 3...' % (loss_lim_cut, contam_lim_cut))\n",
    "        final_cut = cut_start\n",
    "    elif case1: # 1\n",
    "        if case2: # and 2\n",
    "            if loss_case == 'above lim':\n",
    "                print('WARNING: contam_lim_cut <= %0.2f falls below limit %0.2f%%, but loss_lim_cut >= %0.2f falls above limit %0.2f%%! Setting to %0.2f...' % (contam_lim_cut, contam_lim, loss_lim_cut, loss_lim, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('WARNING: loss_lim_cut <= %0.2f falls below limit %0.2f%%, but contam_lim_cut >= %0.2f falls above limit %0.2f%%! Setting to %0.2f...' % (loss_lim_cut, loss_lim, contam_lim_cut, contam_lim, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "        else: # and 3\n",
    "            if loss_case == 'crosses lim':\n",
    "                print('Contam_lim_cut <= %0.2f falls below limit %0.2f%% and loss_lim_cut >= %0.2f crosses limit %0.2f%%, setting to %0.2f...' % (contam_lim_cut, contam_lim, loss_lim_cut, loss_lim, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('Loss_lim_cut <= %0.2f falls below limit %0.2f%% and contam_lim_cut >= %0.2f crosses limit %0.2f%%, setting to %0.2f...' % (loss_lim_cut, loss_lim, contam_lim_cut, contam_lim, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "    elif case2 and not case3: # 2 and 2\n",
    "        print('ERROR: chi-square loss_lim_cut >= %0.2f and contam_lim_cut <= %0.2f both fall above limits %0.2f%% and %0.2f%%! Try setting less strict limits. Setting final cut to nan.' % (loss_lim_cut, contam_lim_cut, loss_lim, contam_lim))\n",
    "        final_cut = np.nan\n",
    "    else: # 2 and 3 or 3 and 3\n",
    "        if loss_lim_cut > contam_lim_cut:\n",
    "            print('WARNING: chi-square loss_lim_cut >= %0.2f and contam_lim_cut <= %0.2f do not overlap! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %0.2f...' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "        else:\n",
    "            print('Valid chi-square cut range from %0.2f to %0.2f! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "    return final_cut\n",
    "\n",
    "def get_lim_cuts_data(cut, colname, indices=None):\n",
    "    if indices is None:\n",
    "        indices = lc_info['baseline_i']\n",
    "\n",
    "    b_good_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stn_cut,uplim=stn_cut,indices=indices)\n",
    "    b_bad_i = AnotB(indices, b_good_i)\n",
    "    b_kept_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=cut,indices=indices)\n",
    "    b_cut_i = AnotB(indices, b_kept_i)\n",
    "\n",
    "    lc_info['%s_Ngood' % colname] = len(b_good_i)\n",
    "    lc_info['%s_Nbad' % colname] = len(b_bad_i)\n",
    "    lc_info['%s_Nkept' % colname] = len(b_kept_i)\n",
    "    lc_info['%s_Ncut' % colname] = len(b_cut_i)\n",
    "    lc_info['%s_Ngood,kept' % colname] = len(AandB(b_good_i,b_kept_i))\n",
    "    lc_info['%s_Ngood,cut' % colname] = len(AandB(b_good_i,b_cut_i))\n",
    "    lc_info['%s_Nbad,kept' % colname] = len(AandB(b_bad_i,b_kept_i))\n",
    "    lc_info['%s_Nbad,cut' % colname] = len(AandB(b_bad_i,b_cut_i))\n",
    "    lc_info['%s_Pgood,kept' % colname] = 100*len(AandB(b_good_i,b_kept_i))/len(indices)\n",
    "    lc_info['%s_Pgood,cut' % colname] = 100*len(AandB(b_good_i,b_cut_i))/len(indices)\n",
    "    lc_info['%s_Pbad,kept' % colname] = 100*len(AandB(b_bad_i,b_kept_i))/len(indices)\n",
    "    lc_info['%s_Pbad,cut' % colname] = 100*len(AandB(b_bad_i,b_cut_i))/len(indices)\n",
    "    lc_info['%s_Ngood,kept/Ngood' % colname] = 100*len(AandB(b_good_i,b_kept_i))/len(b_good_i)\n",
    "    lc_info['%s_Ploss' % colname] = 100*len(AandB(b_good_i,b_cut_i))/len(b_good_i)\n",
    "    lc_info['%s_Pcontamination' % colname] = 100*len(AandB(b_bad_i,b_kept_i))/len(b_kept_i)\n",
    "\n",
    "def get_lim_cuts(lim_cuts): \n",
    "    contam_lim_cut = None\n",
    "    loss_lim_cut = None\n",
    "    contam_case = None\n",
    "    loss_case = None\n",
    "\n",
    "    sortby_loss = lim_cuts.t.iloc[(lim_cuts.t['Ploss']).argsort()].reset_index()\n",
    "    min_loss = sortby_loss.loc[0,'Ploss']\n",
    "    max_loss = sortby_loss.loc[len(sortby_loss)-1,'Ploss']\n",
    "    # if all loss below lim, loss_lim_cut is min cut\n",
    "    if min_loss < loss_lim and max_loss < loss_lim:\n",
    "        loss_case = 'below lim'\n",
    "        loss_lim_cut = lim_cuts.t.loc[0,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all loss above lim, loss_lim_cut is min cut with min% loss\n",
    "        if min_loss > loss_lim and max_loss > loss_lim:\n",
    "            loss_case = 'above lim'\n",
    "            a = np.where(lim_cuts.t['Ploss'] == min_loss)[0]\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            loss_lim_cut = c.loc[0,'PSF Chi-Square Cut']\n",
    "        # else if loss crosses lim at some point, loss_lim_cut is min cut with max% loss <= loss_lim\n",
    "        else:\n",
    "            loss_case = 'crosses lim'\n",
    "            valid_cuts = sortby_loss[sortby_loss['Ploss'] <= loss_lim]\n",
    "            a = np.where(lim_cuts.t['Ploss'] == valid_cuts.loc[len(valid_cuts)-1,'Ploss'])[0]\n",
    "            # sort by cuts\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            # get midpoint of loss1 and loss2 (two points on either side of lim)\n",
    "            loss1_i = np.where(lim_cuts.t['PSF Chi-Square Cut'] == c.loc[0,'PSF Chi-Square Cut'])[0][0]\n",
    "            if lim_cuts.t.loc[loss1_i,'Ploss'] == loss_lim:\n",
    "                loss_lim_cut = lim_cuts.t.loc[loss1_i,'PSF Chi-Square Cut']\n",
    "            else:\n",
    "                loss2_i = loss1_i - 1\n",
    "                x = np.array([lim_cuts.t.loc[loss1_i,'PSF Chi-Square Cut'], lim_cuts.t.loc[loss2_i,'PSF Chi-Square Cut']])\n",
    "                contam_y = np.array([lim_cuts.t.loc[loss1_i,'Pcontamination'], lim_cuts.t.loc[loss2_i,'Pcontamination']])\n",
    "                loss_y = np.array([lim_cuts.t.loc[loss1_i,'Ploss'], lim_cuts.t.loc[loss2_i,'Ploss']])\n",
    "                contam_line = np.polyfit(x,contam_y,1)\n",
    "                loss_line = np.polyfit(x,loss_y,1)\n",
    "                loss_lim_cut = (loss_lim-loss_line[1])/loss_line[0]\n",
    "\n",
    "    sortby_contam = lim_cuts.t.iloc[(lim_cuts.t['Pcontamination']).argsort()].reset_index()\n",
    "    min_contam = sortby_contam.loc[0,'Pcontamination']\n",
    "    max_contam = sortby_contam.loc[len(sortby_contam)-1,'Pcontamination']\n",
    "    # if all contam below lim, contam_lim_cut is max cut\n",
    "    if min_contam < contam_lim and max_contam < contam_lim:\n",
    "        contam_case = 'below lim'\n",
    "        contam_lim_cut = lim_cuts.t.loc[len(lim_cuts.t)-1,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all contam above lim, contam_lim_cut is max cut with min% contam\n",
    "        if min_contam > contam_lim and max_contam > contam_lim:\n",
    "            contam_case = 'above lim'\n",
    "            a = np.where(lim_cuts.t['Pcontamination'] == min_contam)[0]\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            contam_lim_cut = c.loc[len(c)-1,'PSF Chi-Square Cut']\n",
    "        # else if contam crosses lim at some point, contam_lim_cut is max cut with max% contam <= contam_lim\n",
    "        else:\n",
    "            contam_case = 'crosses lim'\n",
    "            valid_cuts = sortby_contam[sortby_contam['Pcontamination'] <= contam_lim]\n",
    "            a = np.where(lim_cuts.t['Pcontamination'] == valid_cuts.loc[len(valid_cuts)-1,'Pcontamination'])[0]\n",
    "            # sort by cuts\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            # get midpoint of contam1 and contam2 (two points on either side of lim)\n",
    "            contam1_i = np.where(lim_cuts.t['PSF Chi-Square Cut'] == c.loc[len(c)-1,'PSF Chi-Square Cut'])[0][0]\n",
    "            if lim_cuts.t.loc[contam1_i,'Pcontamination'] == contam_lim:\n",
    "                contam_lim_cut = lim_cuts.t.loc[contam1_i,'PSF Chi-Square Cut']\n",
    "            else:\n",
    "                contam2_i = contam1_i + 1\n",
    "                x = np.array([lim_cuts.t.loc[contam1_i,'PSF Chi-Square Cut'], lim_cuts.t.loc[contam2_i,'PSF Chi-Square Cut']])\n",
    "                contam_y = np.array([lim_cuts.t.loc[contam1_i,'Pcontamination'], lim_cuts.t.loc[contam2_i,'Pcontamination']])\n",
    "                loss_y = np.array([lim_cuts.t.loc[contam1_i,'Ploss'], lim_cuts.t.loc[contam2_i,'Ploss']])\n",
    "                contam_line = np.polyfit(x,contam_y,1)\n",
    "                loss_line = np.polyfit(x,loss_y,1)\n",
    "                contam_lim_cut = (contam_lim-contam_line[1])/contam_line[0]\n",
    "\n",
    "    get_lim_cuts_data(loss_lim_cut, 'loss_lim_cut')\n",
    "    get_lim_cuts_data(contam_lim_cut, 'contam_lim_cut')\n",
    "\n",
    "    return contam_lim_cut, loss_lim_cut, contam_case, loss_case\n",
    "\n",
    "def get_lim_cuts_table(stn_cut, cut_start, cut_stop, cut_step, indices=None):\n",
    "    print('abs(uJy/duJy) cut at %0.2f \\nx2 cut from %0.2f to %0.2f inclusive, with step size %d' % (stn_cut,cut_start,cut_stop,cut_step))\n",
    "\n",
    "    if indices is None: \n",
    "        indices = lc_info['baseline_i']\n",
    "\n",
    "    lim_cuts = pdastrostatsclass(columns=['PSF Chi-Square Cut', 'N', 'Ngood', 'Nbad', 'Nkept', 'Ncut', 'Ngood,kept', 'Ngood,cut', 'Nbad,kept', 'Nbad,cut',\n",
    "                                          'Pgood,kept', 'Pgood,cut', 'Pbad,kept', 'Pbad,cut', 'Ngood,kept/Ngood', 'Ploss', 'Pcontamination',\n",
    "                                          'Nbad,cut 3<stn<=5', 'Nbad,cut 5<stn<=10', 'Nbad,cut 10<stn', 'Nbad,kept 3<stn<=5', 'Nbad,kept 5<stn<=10', 'Nbad,kept 10<stn'])\n",
    "    \n",
    "    # static cut at x2 = 50\n",
    "    x2cut_50 = np.where(lc_info['lc'].t['chi/N'] < 50)[0]\n",
    "    print('Static chi square cut at 50: %0.2f%% cut for baseline' % (100*len(AnotB(indices,x2cut_50))/len(indices)))\n",
    "\n",
    "    # good baseline measurement indices\n",
    "    b_good_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stn_cut,uplim=stn_cut,indices=indices)\n",
    "    b_bad_i = AnotB(indices, b_good_i)\n",
    "    # for different x2 cuts decreasing from 50\n",
    "    for cut in range(cut_start,cut_stop+1,cut_step):\n",
    "        # kept baseline measurement indices\n",
    "        b_kept_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=cut,indices=indices)\n",
    "        b_cut_i = AnotB(indices, b_kept_i)\n",
    "\n",
    "        df = pd.DataFrame([[cut, len(indices), # N\n",
    "                            len(b_good_i), # Ngood\n",
    "                            len(b_bad_i), # Nbad\n",
    "                            len(b_kept_i), # Nkept\n",
    "                            len(b_cut_i), # Ncut\n",
    "                            len(AandB(b_good_i,b_kept_i)), # Ngood,kept\n",
    "                            len(AandB(b_good_i,b_cut_i)), # Ngood,cut\n",
    "                            len(AandB(b_bad_i,b_kept_i)), # Nbad,kept\n",
    "                            len(AandB(b_bad_i,b_cut_i)), # Nbad,cut\n",
    "                            100*len(AandB(b_good_i,b_kept_i))/len(indices), # Ngood,kept/Nbaseline\n",
    "                            100*len(AandB(b_good_i,b_cut_i))/len(indices), # Ngood,cut/Nbaseline \n",
    "                            100*len(AandB(b_bad_i,b_kept_i))/len(indices), # Nbad,kept/Nbaseline\n",
    "                            100*len(AandB(b_bad_i,b_cut_i))/len(indices), # Nbad,cut/Nbaseline\n",
    "                            100*len(AandB(b_good_i,b_kept_i))/len(b_good_i), # Ngood,kept/Ngood\n",
    "                            100*len(AandB(b_good_i,b_cut_i))/len(b_good_i), # Ngood,cut/Ngood = Loss\n",
    "                            100*len(AandB(b_bad_i,b_kept_i))/len(b_kept_i), # Nbad,kept/Nkept = Contamination\n",
    "                            len(AandB(AandB(b_bad_i,b_cut_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-3,uplim=5,exclude_lowlim=True))), # Nbad,cut 3<stn<=5\n",
    "                            len(AandB(AandB(b_bad_i,b_cut_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=5,uplim=10,exclude_lowlim=True))), # Nbad,cut 5<stn<=10\n",
    "                            len(AandB(AandB(b_bad_i,b_cut_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=10,exclude_lowlim=True))), # Nbad,cut 10<stn \n",
    "                            len(AandB(AandB(b_bad_i,b_kept_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-3,uplim=5,exclude_lowlim=True))), # Nbad,kept 3<stn<=5\n",
    "                            len(AandB(AandB(b_bad_i,b_kept_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=5,uplim=10,exclude_lowlim=True))), # Nbad,kept 5<stn<=10\n",
    "                            len(AandB(AandB(b_bad_i,b_kept_i), lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=10,exclude_lowlim=True))), # Nbad,kept 10<stn \n",
    "                            ]], columns=['PSF Chi-Square Cut', 'N', 'Ngood', 'Nbad', 'Nkept', 'Ncut', 'Ngood,kept', 'Ngood,cut', 'Nbad,kept', 'Nbad,cut',\n",
    "                                         'Pgood,kept', 'Pgood,cut', 'Pbad,kept', 'Pbad,cut', 'Ngood,kept/Ngood', 'Ploss', 'Pcontamination',\n",
    "                                         'Nbad,cut 3<stn<=5', 'Nbad,cut 5<stn<=10', 'Nbad,cut 10<stn', 'Nbad,kept 3<stn<=5', 'Nbad,kept 5<stn<=10', 'Nbad,kept 10<stn'])\n",
    "        lim_cuts.t = pd.concat([lim_cuts.t,df],ignore_index=True)\n",
    "    return lim_cuts\n",
    "\n",
    "if lim_to_prioritize != 'loss_lim' and lim_to_prioritize != 'contam_lim':\n",
    "    print(\"ERROR: lim_to_prioritize must be 'loss_lim' or 'contam_lim'!\")\n",
    "    sys.exit()\n",
    "print('Contamination limit: %0.2f%%\\nLoss limit: %0.2f%%' % (contam_lim,loss_lim))\n",
    "\n",
    "lim_cuts = get_lim_cuts_table(stn_cut, cut_start, cut_stop, cut_step)\n",
    "#print(lim_cuts.t.to_string())\n",
    "contam_lim_cut, loss_lim_cut, contam_case, loss_case = get_lim_cuts(lim_cuts)\n",
    "lc_info['contam_case'] = contam_case\n",
    "lc_info['loss_case'] = loss_case\n",
    "lc_info['contam_lim_cut'] = contam_lim_cut\n",
    "lc_info['loss_lim_cut'] = loss_lim_cut\n",
    "\n",
    "print('\\nContamination cut according to given contam_limit, with %0.2f%% contamination and %0.2f%% loss: %0.2f' % (lc_info['contam_lim_cut_Pcontamination'], lc_info['contam_lim_cut_Ploss'], contam_lim_cut))\n",
    "if lc_info['contam_case'] == 'above lim':\n",
    "    print('WARNING: Contamination cut not possible with contamination <= contam_lim %0.1f!' % contam_lim)\n",
    "print('Loss cut according to given loss_limit, with %0.2f%% contamination and %0.2f%% loss: %0.2f' % (lc_info['loss_lim_cut_Pcontamination'], lc_info['loss_lim_cut_Ploss'], loss_lim_cut))\n",
    "if lc_info['loss_case'] == 'above lim':\n",
    "    print('WARNING: Loss cut not possible with loss <= loss_lim %0.2f!' % loss_lim)\n",
    "\n",
    "final_cut = choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case)\n",
    "lc_info['final_cut'] = final_cut\n",
    "        \n",
    "if np.isnan(final_cut):\n",
    "    print('\\nERROR: Final suggested chi-square cut could not be determined. We suggest rethinking your contamination and loss limits.')\n",
    "    lc_info['Pcontamination'] = np.nan\n",
    "    lc_info['Ploss'] = np.nan\n",
    "else:\n",
    "    if final_cut==contam_lim_cut:\n",
    "        lc_info['Pcontamination'] = lc_info['contam_lim_cut_Pcontamination']\n",
    "        lc_info['Ploss'] = lc_info['contam_lim_cut_Ploss']\n",
    "    else:\n",
    "        lc_info['Pcontamination'] = lc_info['loss_lim_cut_Pcontamination']\n",
    "        lc_info['Ploss'] = lc_info['loss_lim_cut_Ploss']\n",
    "    print('\\nFinal suggested chi-square cut is %0.2f, with %0.2f%% contamination and %0.2f%% loss.' % (final_cut, lc_info['Pcontamination'], lc_info['Ploss']))\n",
    "    if (lc_info['Pcontamination'] > contam_lim):\n",
    "        print('WARNING: Final cut\\'s contamination %0.2f%% exceeds contam_lim %0.2f%%!' % (lc_info['Pcontamination'],contam_lim))\n",
    "    if (lc_info['Ploss'] > loss_lim):\n",
    "        print('WARNING: Final cut\\'s loss exceeds %0.2f%% loss_lim %0.2f%%!' % (lc_info['Ploss'],loss_lim))\n",
    "\n",
    "plot_lim_cuts(lim_cuts, contam_lim_cut, loss_lim_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the final chi-square cut; if not, ask for user override\n",
    "\n",
    "answer = input('Accept final chi-square cut of %0.2f (y/n):' % float(lc_info['final_cut']))\n",
    "if answer != 'y':\n",
    "    final_cut_override = float(input('Overriding final chi-square cut; enter manual cut: '))\n",
    "\n",
    "    b_good_i = lc_info['lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stn_cut,uplim=stn_cut,indices=lc_info['baseline_i'])\n",
    "    b_bad_i = AnotB(lc_info['baseline_i'], b_good_i)\n",
    "    b_kept_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=final_cut_override,indices=lc_info['baseline_i'])\n",
    "    b_cut_i = AnotB(lc_info['baseline_i'], b_kept_i)\n",
    "    lc_info['Ploss'] = 100*len(AandB(b_good_i,b_cut_i))/len(b_good_i)\n",
    "    lc_info['Pcontamination'] = 100*len(AandB(b_bad_i,b_kept_i))/len(b_kept_i)\n",
    "    lc_info['final_cut'] = final_cut_override\n",
    "\n",
    "    print('Overridden: final cut is now %0.2f, with contamination %0.2f%% and loss %0.2f%%' % (lc_info['final_cut'],lc_info['Pcontamination'],lc_info['Ploss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Restart the 'Mask' column and update with the chi-square cut flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart 'Mask' column and update with chi-square cut flag\n",
    "\n",
    "def update_mask_col(lc, flag, indices):\n",
    "    if len(indices) > 1:\n",
    "        flag_arr = np.full(lc.loc[indices,'Mask'].shape, flag)\n",
    "        lc.loc[indices,'Mask'] = np.bitwise_or(lc.loc[indices,'Mask'], flag_arr)\n",
    "    elif len(indices) == 1:\n",
    "        lc.loc[indices[0],'Mask'] = int(lc.loc[indices[0],'Mask']) | flag\n",
    "    else:\n",
    "        print('WARNING: must pass at least 1 index to update_mask_col()! No indices masked...')\n",
    "\n",
    "# remove old mask column\n",
    "if 'Mask' in lc_info['lc'].t.columns: \n",
    "    lc_info['lc'].t.drop(columns=['Mask'],inplace=True)\n",
    "\n",
    "# create new mask column and update it with final chi-square cut\n",
    "lc_info['lc'].t['Mask'] = 0\n",
    "kept_i = lc_info['lc'].ix_inrange(colnames=['chi/N'],uplim=lc_info['final_cut'])\n",
    "cut_i = AnotB(lc_info['lc'].getindices(), kept_i)\n",
    "update_mask_col(lc_info['lc'].t, 0x1, cut_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Plot the ATLAS light curve with the chi-square cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cut light curve\n",
    "\n",
    "def plot_cut_lc(lc_type, mask, ylim_lower=None, ylim_upper=None):\n",
    "    good_ix = lc_info[lc_type].ix_unmasked('Mask',maskval=0|mask)\n",
    "    bad_ix = AnotB(lc_info[lc_type].getindices(),good_ix)\n",
    "\n",
    "    fig, (cut, clean) = plt.subplots(1, 2, figsize=(16, 6.5), tight_layout=True)\n",
    "    plt.suptitle('SN %s, %s-band, mask value %d' % (lc_info['tnsname'], lc_info['filter'], mask), fontsize=19, y=1)\n",
    "\n",
    "    if lc_info['filter'] == 'o':\n",
    "        color = 'orange'\n",
    "    else:\n",
    "        color = 'cyan'\n",
    "        \n",
    "    if ylim_lower is None: \n",
    "        ylim_lower = -200\n",
    "    if ylim_upper is None: \n",
    "        if len(AandB(lc_info['afterdiscdate_i'],good_ix)) < 1:\n",
    "            ylim_upper = scale*get_xth_percentile_flux(95, lc_info['afterdiscdate_i'])\n",
    "        else:\n",
    "            ylim_upper = scale*get_xth_percentile_flux(95, AandB(lc_info['afterdiscdate_i'],good_ix))\n",
    "\n",
    "    cut.spines.right.set_visible(False)\n",
    "    cut.spines.top.set_visible(False)\n",
    "    cut.errorbar(lc_info[lc_type].t.loc[good_ix,'MJD'], lc_info[lc_type].t.loc[good_ix,'uJy'], yerr=lc_info[lc_type].t.loc[good_ix,'duJy'], fmt='o',ecolor='black',elinewidth=1,c=color,label='Kept measurements')\n",
    "    cut.errorbar(lc_info[lc_type].t.loc[bad_ix,'MJD'], lc_info[lc_type].t.loc[bad_ix,'uJy'], yerr=lc_info[lc_type].t.loc[bad_ix,'duJy'], fmt='o',mfc='white',ecolor='black',elinewidth=1,c=color,label='Cut measurements')\n",
    "    cut.set_title('All measurements')\n",
    "    cut.axhline(linewidth=1,color='k')\n",
    "    cut.set_xlabel('MJD')\n",
    "    cut.set_ylabel('Flux (uJy)')\n",
    "    cut.set_ylim(ylim_lower, ylim_upper)\n",
    "\n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.5, 0),ncol=2)\n",
    "\n",
    "    clean.spines.right.set_visible(False)\n",
    "    clean.spines.top.set_visible(False)\n",
    "    clean.errorbar(lc_info[lc_type].t.loc[good_ix,'MJD'], lc_info[lc_type].t.loc[good_ix,'uJy'], yerr=lc_info[lc_type].t.loc[good_ix,'duJy'], fmt='o',ecolor='black',elinewidth=1,c=color,label='kept measurements')\n",
    "    clean.set_title('Kept measurements only')\n",
    "    clean.axhline(linewidth=1,color='k')\n",
    "    clean.set_xlabel('MJD')\n",
    "    clean.set_ylabel('Flux (uJy)')\n",
    "    clean.set_ylim(ylim_lower, ylim_upper)\n",
    "\n",
    "plot_cut_lc('lc', 0x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FIX/EDIT DESCRIPTION HERE]\n",
    "\n",
    "Can we identify and cut out bad days by taking a 3σ-clipped average of each day?\n",
    "\n",
    "For each day, we calculate the 3σ-clipped average of any SN measurements falling within that day. Because the ATLAS survey takes ~4 exposures every 2 days, we average together ~4 measurements per epoch. However, out of these 4 exposures, only measurements not cut in the previous methods are averaged in the 3σ cut.\n",
    "\n",
    "Then we cut any measurements in the SN light curve for the given epoch for which statistics fulfill any of the following criteria: \n",
    "- A returned chi-square > 4.0\n",
    "- Number of measurements averaged < 2\n",
    "- Number of measurements clipped > 1\n",
    "\n",
    "Still need to improve cutting at peak (some important epochs were cut, maybe due to fast rise, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the MJD bin size:\n",
    "mjdbinsize = 1\n",
    "\n",
    "# Should MJD bins with no measurements be translated as NaN (True) \n",
    "# or removed from the averaged light curve (False)?\n",
    "keep_empty_bins = False\n",
    "\n",
    "# Enter the magnitudes' sigma limit (magnitudes are limits if dmagnitude is NaN):\n",
    "flux2mag_sigma_limit = 3\n",
    "\n",
    "# Enter the bound for a bin's maximum number of clipped measurements\n",
    "# (if Nclip > Nclip_max, flag day):\n",
    "Nclip_max = 1\n",
    "\n",
    "# Enter the bound for a bin's minimum number of good measurements\n",
    "# (if Ngood < Ngood_min, flag day):\n",
    "Ngood_min = 2\n",
    "\n",
    "# Enter the bound for a bin's maximum chi-square (if x2 > x2_max, flag day):\n",
    "x2_max = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the light curve\n",
    "\n",
    "def average_lc(Nclip_max, Ngood_min, x2_max, flag_badday, flag_ixclip, flag_smallnum, mjdbinsize=1, flux2mag_sigma_limit=3.0, keep_empty_bins=True):\n",
    "    mjd = int(np.amin(lc_info['lc'].t['MJD']))\n",
    "    mjd_max = int(np.amax(lc_info['lc'].t['MJD']))+1\n",
    "\n",
    "    good_i = lc_info['lc'].ix_unmasked('Mask', maskval=0x1)\n",
    "\n",
    "    while mjd <= mjd_max:\n",
    "        range_i = lc_info['lc'].ix_inrange(colnames=['MJD'], lowlim=mjd, uplim=mjd+mjdbinsize, exclude_uplim=True)\n",
    "        range_good_i = AandB(range_i,good_i)\n",
    "\n",
    "        # add new row to avglc if keep_empty_bins or any measurements present\n",
    "        if keep_empty_bins or len(range_i) >= 1:\n",
    "            new_row = {'MJDbin':mjd+0.5*mjdbinsize, 'Nclip':0, 'Ngood':0, 'Nexcluded':len(range_i)-len(range_good_i), 'Mask':0}\n",
    "            avglc_index = lc_info['avglc'].newrow(new_row)\n",
    "        \n",
    "        # if no measurements present, flag or skip over day\n",
    "        if len(range_i) < 1:\n",
    "            if keep_empty_bins:\n",
    "                update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "            mjd += mjdbinsize\n",
    "            continue\n",
    "        \n",
    "        # if no good measurements, average values anyway and flag\n",
    "        if len(range_good_i) < 1:\n",
    "            # average flux\n",
    "            lc_info['lc'].calcaverage_sigmacutloop('uJy', noisecol='duJy', indices=range_i, Nsigma=3.0, median_firstiteration=True)\n",
    "            fluxstatparams = deepcopy(lc_info['lc'].statparams)\n",
    "\n",
    "            # average mjd\n",
    "            # SHOULD NOISECOL HERE BE DUJY OR NONE??\n",
    "            lc_info['lc'].calcaverage_sigmacutloop('MJD', noisecol='duJy', indices=fluxstatparams['ix_good'], Nsigma=0, median_firstiteration=False)\n",
    "            avg_mjd = lc_info['lc'].statparams['mean']\n",
    "\n",
    "            # add row and flag\n",
    "            lc_info['avglc'].add2row(avglc_index, {'MJD':avg_mjd, \n",
    "                                                   'uJy':fluxstatparams['mean'], \n",
    "                                                   'duJy':fluxstatparams['mean_err'], \n",
    "                                                   'stdev':fluxstatparams['stdev'],\n",
    "                                                   'x2':fluxstatparams['X2norm'],\n",
    "                                                   'Nclip':fluxstatparams['Nclip'],\n",
    "                                                   'Ngood':fluxstatparams['Ngood'],\n",
    "                                                   'Mask':0})\n",
    "            update_mask_col(lc_info['lc'].t, flag_badday, range_i)\n",
    "            update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "\n",
    "            mjd += mjdbinsize\n",
    "            continue\n",
    "        \n",
    "        # average good measurements\n",
    "        lc_info['lc'].calcaverage_sigmacutloop('uJy', noisecol='duJy', indices=range_good_i, Nsigma=3.0, median_firstiteration=True)\n",
    "        fluxstatparams = deepcopy(lc_info['lc'].statparams)\n",
    "\n",
    "        if fluxstatparams['mean'] is None or len(fluxstatparams['ix_good']) < 1:\n",
    "            update_mask_col(lc_info['lc'].t, flag_badday, range_i)\n",
    "            update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "            mjd += mjdbinsize\n",
    "            continue\n",
    "\n",
    "        # average mjd\n",
    "        # SHOULD NOISECOL HERE BE DUJY OR NONE??\n",
    "        lc_info['lc'].calcaverage_sigmacutloop('MJD', noisecol='duJy', indices=fluxstatparams['ix_good'], Nsigma=0, median_firstiteration=False)\n",
    "        avg_mjd = lc_info['lc'].statparams['mean']\n",
    "\n",
    "        # add row\n",
    "        lc_info['avglc'].add2row(avglc_index, {'MJD':avg_mjd, \n",
    "                                                   'uJy':fluxstatparams['mean'], \n",
    "                                                   'duJy':fluxstatparams['mean_err'], \n",
    "                                                   'stdev':fluxstatparams['stdev'],\n",
    "                                                   'x2':fluxstatparams['X2norm'],\n",
    "                                                   'Nclip':fluxstatparams['Nclip'],\n",
    "                                                   'Ngood':fluxstatparams['Ngood'],\n",
    "                                                   'Mask':0})\n",
    "        \n",
    "        # flag clipped measurements in lc\n",
    "        if len(fluxstatparams['ix_clip']) > 0:\n",
    "            update_mask_col(lc_info['lc'].t, flag_ixclip, fluxstatparams['ix_clip'])\n",
    "        \n",
    "        # if small number within this bin, flag measurements\n",
    "        if len(range_good_i) < 3:\n",
    "            update_mask_col(lc_info['lc'].t, flag_smallnum, range_good_i) # CHANGE TO RANGE_I??\n",
    "            update_mask_col(lc_info['avglc'].t, flag_smallnum, [avglc_index])\n",
    "        # else check sigmacut bounds and flag\n",
    "        else:\n",
    "            is_bad = False\n",
    "            if fluxstatparams['Ngood'] < Ngood_min:\n",
    "                is_bad = True\n",
    "            if fluxstatparams['Nclip'] > Nclip_max:\n",
    "                is_bad = True\n",
    "            if not(fluxstatparams['X2norm'] is None) and fluxstatparams['X2norm'] > x2_max:\n",
    "                is_bad = True\n",
    "            if is_bad:\n",
    "                update_mask_col(lc_info['lc'].t, flag_badday, range_i)\n",
    "                update_mask_col(lc_info['avglc'].t, flag_badday, [avglc_index])\n",
    "\n",
    "        mjd += mjdbinsize\n",
    "    \n",
    "    # convert flux to magnitude and dflux to dmagnitude\n",
    "    lc_info['avglc'].flux2mag('uJy','duJy','m','dm', zpt=23.9, upperlim_Nsigma=flux2mag_sigma_limit)\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "    drop_extra_columns('avglc')\n",
    "\n",
    "    for col in ['Nclip','Ngood','Nexcluded','Mask']: \n",
    "        lc_info['avglc'].t[col] = lc_info['avglc'].t[col].astype(np.int32)\n",
    "\n",
    "if len(lc_info['lc'].t) < 1:\n",
    "    print('ERROR: No data in lc so cannot average; exiting... ')\n",
    "    sys.exit()\n",
    "\n",
    "print('Averaging light curve with the following criteria: Nclip_max = %d, Ngood_min = %d, x2_max = %0.2f... ' % (Nclip_max, Ngood_min, x2_max))\n",
    "lc_info['avglc'] = pdastrostatsclass(columns=['MJD','MJDbin','uJy','duJy','stdev','x2','Nclip','Ngood','Nexcluded','Mask'],hexcols=['Mask'])\n",
    "average_lc(Nclip_max, Ngood_min, x2_max, 0x800000, 0x1000, 0x2000, keep_empty_bins=keep_empty_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Plot the averaged light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot averaged light curve\n",
    "\n",
    "plot_cut_lc('avglc', 0x1|0x800000|0x1000|0x2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: Save the averaged light curve with the new 'Mask' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the light curve with the new 'Mask' column?\n",
    "save_lc = True\n",
    "\n",
    "# Override the old light curve file?\n",
    "override_old_lc = False\n",
    "\n",
    "# If not overriding old light curve file, enter new filename:\n",
    "filename_new = '/Users/sofiarest/Google Drive/My Drive/College/STScI Research Paper/atlaslc_chisquare/brightsne/2019vxm/NEW_2019vxm_i000.o.1.00days.lc.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save light curve\n",
    "if save_lc:\n",
    "    print('Saving light curve with updated mask column...')\n",
    "    if override_old_lc:\n",
    "        print('Overriding old light curve file at %s... ' % filename)\n",
    "        lc_info['avglc'].write(filename,overwrite=True)\n",
    "    else:\n",
    "        print('Writing new file at %s... ' % filename_new)\n",
    "        lc_info['avglc'].write(filename_new,overwrite=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
