{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Averaging ATLAS Light Curves\n",
    "### Includes chi-square cut, uncertainty cut, control light curve cut, and averaging\n",
    "\n",
    "This iPython notebook will help you apply each cut with a greater degree of control than an automatic cleaning. You will be walked through the following:\n",
    "1. Static uncertainty cut\n",
    "2. Estimating true uncertainties\n",
    "3. Dynamic chi-square cut\n",
    "4. Control light curve cut\n",
    "5. Averaging the light curve and cutting bad bins\n",
    "6. Optionally correct for ATLAS template changes\n",
    "7. Save files\n",
    "\n",
    "After running a cell, the descriptions located above that cell will help you interpret the plots and make decisions about the supernova.\n",
    "\n",
    "In order for this notebook to work correctly, the ATLAS light curves must already be downloaded and saved. Each light curve must also only include measurements for a single filter.\n",
    "\n",
    "## Step 1: Load the ATLAS light curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOADING THE SN LIGHT CURVE #####\n",
    "\n",
    "# Enter the target SN name:\n",
    "tnsname = '2018gkr'\n",
    "\n",
    "# Enter the path to the data directory that contains the SN directory:\n",
    "source_dir = f'/Users/sofiarest/Desktop/Supernovae/data/temp'\n",
    "\n",
    "# Enter the path to a directory to optionally save any plots:\n",
    "output_dir = f'{source_dir}/{tnsname}/plots'\n",
    "\n",
    "# Enter the filter for this light curve (must be 'o' or 'c'):\n",
    "filt = 'o'\n",
    "\n",
    "# Optionally, enter the SN's discovery date (if None is entered, it will be \n",
    "# fetched automatically from TNS using the API key, TNS ID, and bot name):\n",
    "discovery_date = 58207.146991\n",
    "api_key = None\n",
    "tns_id = None\n",
    "bot_name = None\n",
    "\n",
    "##### LOADING CONTROL LIGHT CURVES #####\n",
    "\n",
    "# Set to True if you are planning on applying the control light curve cut \n",
    "# and have already downloaded the control light curves:\n",
    "load_controls = True\n",
    "\n",
    "# Enter the number of control light curves to load:\n",
    "n_controls = 8\n",
    "\n",
    "# Enter the source directory of the control light curve files:\n",
    "controls_dir = f'{source_dir}/{tnsname}/controls'\n",
    "\n",
    "##### DEFAULT SETTINGS FOR PLOTTING #####\n",
    "\n",
    "# If True, try to calculate the best y limits automatically for each plot;\n",
    "# if False, leave y limits to matplotlib \n",
    "auto_xylimits = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules, set preliminary variables, etc.\n",
    "\n",
    "from pdastro import pdastrostatsclass, AandB, AnotB, AorB, not_AandB\n",
    "from atlas_lc import atlas_lc\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import warnings\n",
    "warnings.simplefilter('error', RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# plotting styles\n",
    "plt.rc('axes', titlesize = 17)\n",
    "plt.rc('xtick', labelsize = 12)\n",
    "plt.rc('ytick', labelsize = 12)\n",
    "plt.rc('legend', fontsize = 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=['red', 'orange', 'green', 'blue', 'purple', 'magenta'])\n",
    "matplotlib.rcParams['xtick.major.size'] = 6\n",
    "matplotlib.rcParams['xtick.major.width'] = 1\n",
    "matplotlib.rcParams['xtick.minor.size'] = 3\n",
    "matplotlib.rcParams['xtick.minor.width'] = 1\n",
    "matplotlib.rcParams['ytick.major.size'] = 6\n",
    "matplotlib.rcParams['ytick.major.width'] = 1\n",
    "matplotlib.rcParams['ytick.minor.size'] = 3\n",
    "matplotlib.rcParams['ytick.minor.width'] = 1\n",
    "matplotlib.rcParams['axes.linewidth'] = 1\n",
    "marker_size = 30\n",
    "marker_edgewidth = 1.5\n",
    "sn_flux = 'orange' if filt == 'o' else 'cyan'\n",
    "sn_flagged_flux = 'red' \n",
    "ctrl_flux = 'steelblue' #'darkgreen' #'limegreen' \n",
    "#select_ctrl_flux = 'darkgreen'\n",
    "\n",
    "# ATLAS template change dates (MJD)\n",
    "#tchange0 = 57500\n",
    "tchange1 = 58417\n",
    "tchange2 = 58882\n",
    "\n",
    "# 'Mask' column flags\n",
    "flags = {'chisquare':0x1, \n",
    "\t\t \n",
    "\t\t 'uncertainty':0x2,\n",
    "\t\t \n",
    "\t\t 'controls_bad':0x400000,\n",
    "\t\t 'controls_questionable':0x80000,\n",
    "\t\t 'controls_x2':0x100,\n",
    "\t\t 'controls_stn':0x200,\n",
    "\t\t 'controls_Nclip':0x400,\n",
    "\t\t 'controls_Ngood':0x800,\n",
    "\t\t \n",
    "\t\t 'avg_badday':0x800000,\n",
    "\t\t 'avg_ixclip':0x1000,\n",
    "\t\t 'avg_smallnum':0x2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get discovery date if needed, load in light curve, account for template changes\n",
    "\n",
    "if filt != 'o' and filt != 'c': \n",
    "\traise RuntimeError('Filter must be \"o\" or \"c\"!')\n",
    "\n",
    "# new text file that will contain record of each cut, etc.\n",
    "f = open(f'{source_dir}/{tnsname}/{tnsname}_output.md', 'w')\n",
    "f.write(f'# SN {tnsname} Light Curve Cleaning and Averaging\\n\\nFilter: {filt}-band\\nDiscovery date: {discovery_date}\\nNumber of control light curves: {n_controls}')\n",
    "\n",
    "# SN and control light curves\n",
    "lc = atlas_lc(tnsname, discdate=discovery_date)\n",
    "if lc.discdate is None:\n",
    "\tlc._get_tns_data(tnsname, api_key, tns_id, bot_name)\n",
    "lc._load(source_dir, filt, n_controls)\n",
    "lc.prep_for_cleaning()\n",
    "print(lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot SN and control light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot control light curves underneath SN light curve?\n",
    "plot_controls = True\n",
    "\n",
    "# Optionally, manually enter the x and y limits:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot SN and control light curves\n",
    "\n",
    "def do_manual_xylimits(limits):\n",
    "    for limit in limits:\n",
    "        if not limit is None:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def set_xylimits(lc, limits, control_index=0, indices=None):\n",
    "    if auto_xylimits:\n",
    "        if limits[0] is None:\n",
    "            limits[0] = lc.lcs[control_index].t['MJD'].min() * 0.999\n",
    "        if limits[1] is None:\n",
    "            limits[1] = lc.lcs[control_index].t['MJD'].max() * 1.001\n",
    "        \n",
    "        if indices is None:\n",
    "            indices = lc.get_ix()\n",
    "        # exclude measurements with duJy > 160\n",
    "        good_ix = lc.lcs[control_index].ix_inrange(colnames='duJy', uplim=160, indices=indices)\n",
    "        # get 5% of abs(max flux - min flux)\n",
    "        flux_min = lc.lcs[control_index].t.loc[good_ix, 'uJy'].min()\n",
    "        flux_max = lc.lcs[control_index].t.loc[good_ix, 'uJy'].max()\n",
    "        diff = abs(flux_max - flux_min)\n",
    "        offset = 0.05 * diff\n",
    "\n",
    "        if limits[2] is None: \n",
    "            limits[2] = flux_min - offset\n",
    "        if limits[3] is None:\n",
    "            limits[3] = flux_max + offset\n",
    "\n",
    "        return limits\n",
    "    \n",
    "    if do_manual_xylimits(limits):\n",
    "        return limits\n",
    "    \n",
    "    return None\n",
    "\n",
    "def save_plot(save_filename=None):\n",
    "    if not save_filename is None:\n",
    "        filename = f'{output_dir}/{save_filename}.png'\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        print(f'Saving plot: {filename}')\n",
    "        plt.savefig(filename, dpi=200)\n",
    "\n",
    "def plot_all_lcs(lc, add2title=None, plot_controls=False, plot_templates=False, limits=None, save_filename=None):\n",
    "    fig, ax1 = plt.subplots(1, constrained_layout=True)\n",
    "    fig.set_figwidth(7)\n",
    "    fig.set_figheight(4)\n",
    "\n",
    "    title = f'SN {tnsname} & control light curves {filt}-band flux'\n",
    "    if not(add2title is None):\n",
    "        title += add2title\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    ax1.minorticks_on()\n",
    "    ax1.tick_params(direction='in', which='both')\n",
    "    ax1.set_ylabel(r'Flux ($\\mu$Jy)')\n",
    "    ax1.set_xlabel('MJD')\n",
    "    ax1.axhline(linewidth=1, color='k')\n",
    "\n",
    "    # set x and y limits\n",
    "    limits = set_xylimits(lc, limits)\n",
    "    if not limits is None:\n",
    "        ax1.set_xlim(limits[0],limits[1])\n",
    "        ax1.set_ylim(limits[2],limits[3])\n",
    "\n",
    "    if plot_templates:\n",
    "        ax1.axvline(x=tchange1, color='k', linestyle='dotted', label='ATLAS template change', zorder=100)\n",
    "        ax1.axvline(x=tchange2, color='k', linestyle='dotted', zorder=100)\n",
    "        #ax1.axvline(x=tchange2, color='k', linestyle='dotted', zorder=100)\n",
    "\n",
    "    preSN_ix = lc.get_pre_SN_ix()\n",
    "    postSN_ix = lc.get_post_SN_ix()\n",
    "\n",
    "    if load_controls and plot_controls:\n",
    "        for control_index in range(1, lc.num_controls+1):\n",
    "            plt.errorbar(lc.lcs[control_index].t['MJD'], lc.lcs[control_index].t['uJy'], yerr=lc.lcs[control_index].t[lc.dflux_colnames[control_index]], fmt='none', ecolor=ctrl_flux, elinewidth=1.5, capsize=1.2, c=ctrl_flux, alpha=0.5, zorder=0)\n",
    "            if control_index == 1:\n",
    "                plt.scatter(lc.lcs[control_index].t['MJD'], lc.lcs[control_index].t['uJy'], s=marker_size, color=ctrl_flux, marker='o', alpha=0.5, zorder=0, label=f'{lc.num_controls} control light curves')\n",
    "            else:\n",
    "                plt.scatter(lc.lcs[control_index].t['MJD'], lc.lcs[control_index].t['uJy'], s=marker_size, color=ctrl_flux, marker='o', alpha=0.5, zorder=0)\n",
    "\n",
    "\n",
    "    plt.errorbar(lc.lcs[0].t.loc[preSN_ix,'MJD'], lc.lcs[0].t.loc[preSN_ix,'uJy'], yerr=lc.lcs[0].t.loc[preSN_ix,lc.dflux_colnames[0]], fmt='none', ecolor=sn_flux, elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5, zorder=10)\n",
    "    plt.scatter(lc.lcs[0].t.loc[preSN_ix,'MJD'], lc.lcs[0].t.loc[preSN_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color=sn_flux, marker='o', alpha=0.5, zorder=10, label='Pre-SN light curve')\n",
    "\n",
    "    plt.errorbar(lc.lcs[0].t.loc[postSN_ix,'MJD'], lc.lcs[0].t.loc[postSN_ix,'uJy'], yerr=lc.lcs[0].t.loc[postSN_ix,lc.dflux_colnames[0]], fmt='none', ecolor='red', elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5, zorder=10)\n",
    "    plt.scatter(lc.lcs[0].t.loc[postSN_ix,'MJD'], lc.lcs[0].t.loc[postSN_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color='red', marker='o', alpha=0.5, zorder=10, label='Post-SN light curve')\n",
    "\n",
    "    ax1.legend(loc='upper right', facecolor='white', framealpha=1.0).set_zorder(100)\n",
    "\n",
    "    save_plot(save_filename=save_filename)\n",
    "\n",
    "limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "plot_all_lcs(lc, plot_controls=plot_controls, plot_templates=True, limits=limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Static uncertainty cut\n",
    "\n",
    "The following uncertainty cut implements a static cut that applies the same way to each light curve. The purpose of this cut is to identify and clean out the most egregious outliers with large uncertainties and small chi-square values that would not be cut out in the dynamic chi-square cut. The default value of this cut (160) was determined after calculating the typical uncertainty of bright stars just below the saturation limit. \n",
    "\n",
    "WARNING: **if the SN is particularly bright, you may want to increase the value and rerun the cut**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may change the following static uncertainty cut value to your liking;\n",
    "# however, the default value is set to 160.\n",
    "uncertainty_cut = 160\n",
    "\n",
    "# Plot the light curve before and after the applied uncertainty cut?:\n",
    "plot = True\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the uncertainty cut plot:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update 'Mask' column with uncertainty cut\n",
    "print(f'Applying uncertainty cut of {uncertainty_cut:0.2f}...')\n",
    "\n",
    "def plot_cut_lc(lc, title, flag, limits=None, save_filename=None):\n",
    "    fig, (ax2, ax1) = plt.subplots(2, constrained_layout=True)\n",
    "    fig.set_figwidth(7)\n",
    "    fig.set_figheight(5)\n",
    "\n",
    "    fig.suptitle(f'{title} (flag {hex(flag)})')\n",
    "\n",
    "    ax1.minorticks_on()\n",
    "    ax1.tick_params(direction='in', which='both')\n",
    "    ax2.get_xaxis().set_ticks([])\n",
    "    ax1.set_ylabel(r'Flux ($\\mu$Jy)')\n",
    "    ax1.axhline(linewidth=1, color='k')\n",
    "\n",
    "    ax2.minorticks_on()\n",
    "    ax2.tick_params(direction='in', which='both')\n",
    "    ax2.set_ylabel(r'Flux ($\\mu$Jy)')\n",
    "    ax1.set_xlabel('MJD')\n",
    "    ax2.axhline(linewidth=1, color='k')\n",
    "\n",
    "    # set x and y limits\n",
    "    limits = set_xylimits(lc, limits)\n",
    "    if not limits is None:\n",
    "        ax1.set_xlim(limits[0],limits[1])\n",
    "        ax1.set_ylim(limits[2],limits[3])\n",
    "        ax2.set_xlim(limits[0],limits[1])\n",
    "        ax2.set_ylim(limits[2],limits[3])\n",
    "\n",
    "    good_ix = lc.lcs[0].ix_unmasked('Mask', maskval=flag)\n",
    "    bad_ix = lc.lcs[0].ix_masked('Mask', maskval=flag)\n",
    "\n",
    "    ax1.errorbar(lc.lcs[0].t.loc[good_ix,'MJD'], lc.lcs[0].t.loc[good_ix,'uJy'], yerr=lc.lcs[0].t.loc[good_ix,lc.dflux_colnames[0]], fmt='none', ecolor=sn_flux, elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5)\n",
    "    ax1.scatter(lc.lcs[0].t.loc[good_ix,'MJD'], lc.lcs[0].t.loc[good_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color=sn_flux, marker='o', alpha=0.5, label='Kept measurements')\n",
    "\n",
    "    ax2.errorbar(lc.lcs[0].t.loc[good_ix,'MJD'], lc.lcs[0].t.loc[good_ix,'uJy'], yerr=lc.lcs[0].t.loc[good_ix,lc.dflux_colnames[0]], fmt='none', ecolor=sn_flux, elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5, zorder=5)\n",
    "    ax2.scatter(lc.lcs[0].t.loc[good_ix,'MJD'], lc.lcs[0].t.loc[good_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color=sn_flux, marker='o', alpha=0.5, label='Kept measurements', zorder=5)\n",
    "\n",
    "    ax2.errorbar(lc.lcs[0].t.loc[bad_ix,'MJD'], lc.lcs[0].t.loc[bad_ix,'uJy'], yerr=lc.lcs[0].t.loc[bad_ix,lc.dflux_colnames[0]], fmt='none', ecolor=sn_flagged_flux, elinewidth=1, capsize=1.2, c=sn_flagged_flux, alpha=0.5, zorder=10)\n",
    "    ax2.scatter(lc.lcs[0].t.loc[bad_ix,'MJD'], lc.lcs[0].t.loc[bad_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color=sn_flagged_flux, facecolors='none', edgecolors=sn_flagged_flux, marker='o', alpha=0.5, label='Cut measurements', zorder=10)\n",
    "\n",
    "    ax1.legend(loc='upper right', facecolor='white', framealpha=1.0).set_zorder(100)\n",
    "    ax2.legend(loc='upper right', facecolor='white', framealpha=1.0).set_zorder(100)\n",
    "\n",
    "    save_plot(save_filename=save_filename)\n",
    "\n",
    "def print_statistics(num_cut, percent_cut):\n",
    "    print(f'\\nNumber of cut measurements: {num_cut:d}\\nPercent of total measurements cut: {percent_cut:0.2f}%')\n",
    "    if percent_cut > 10:\n",
    "        print(f'WARNING: percent of total measurements cut is greater than 10%. Plotting...')\n",
    "\n",
    "def apply_uncertainty_cut(lc, uncertainty_cut, control_index=0):\n",
    "    ix = lc.get_ix(control_index=control_index)\n",
    "    kept_ix = lc.lcs[control_index].ix_inrange(colnames=['duJy'],uplim=uncertainty_cut)\n",
    "    cut_ix = AnotB(ix, kept_ix)\n",
    "    lc.update_mask_col(flags['uncertainty'], cut_ix, control_index=control_index)\n",
    "\n",
    "    if control_index == 0:\n",
    "        num_cut = len(cut_ix)\n",
    "        percent_cut = 100 * num_cut/len(ix)\n",
    "        print_statistics(num_cut, percent_cut)\n",
    "        f.write(f'\\n\\n## Uncertainty cut\\nNumber of cut measurements: {num_cut:d}\\nPercent of total measurements cut: {percent_cut:0.2f}%\\nHex value in \"Mask\" column: 0x2')\n",
    "        \n",
    "        if plot or percent_cut > 10:\n",
    "            limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "            plot_cut_lc(lc, 'Uncertainty cut', flags['uncertainty'], limits=limits, save_filename='uncertainty_cut')\n",
    "\n",
    "for control_index in range(lc.num_controls+1):\n",
    "    apply_uncertainty_cut(lc, uncertainty_cut, control_index=control_index)\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Estimating true uncertainties\n",
    "\n",
    "This section attempts to account for an extra noise source in the data by estimating the true typical uncertainty, deriving the additional systematic uncertainty, and lastly applying this extra noise to a new uncertainty column. This new uncertainty column will be used in the cuts following this section.\n",
    "\n",
    "Here is the procedure we use:\n",
    "1. Keep the previously applied uncertainty cut and apply a preliminary chi-square cut at 20 (default value). Filter out any measurements flagged by these two cuts.\n",
    "2. Calculate the true typical uncertainties $\\text{sigma\\_true\\_typical}$ for each control light curve by taking a 3σ cut of the unflagged flux and getting the standard deviation.\n",
    "3. ~~If $\\text{sigma\\_true\\_typical}$ is 10%+ greater than the median uncertainty of the unflagged control light curve flux, $\\text{median}(∂µJy)$, proceed with estimating the extra noise to add. Otherwise, skip this procedure.~~\n",
    "4. Calculate the extra noise source for each control light curve using the following formula, where the median uncertainty, $\\text{median}(∂µJy)$, is taken from the unflagged baseline flux:\n",
    "    - $\\text{sigma\\_extra}^2 = \\text{sigma\\_true\\_typical}^2 - \\text{sigma\\_poisson}^2$\n",
    "    - $\\text{sigma\\_extra} = \\sqrt{\\text{sigma\\_true\\_typical}^2 - \\text{median}(∂µJy)^2}$\n",
    "5. Calculate the final extra noise source by taking the median of all $\\text{sigma\\_extra}$.\n",
    "5. Apply the extra noise source to the existing uncertainty using the following formula:\n",
    "    - $\\text{new }∂µJy = \\sqrt{(\\text{old }∂µJy)^2 + \\text{sigma\\_extra}^2}$\n",
    "6. For cuts following this procedure, use the new uncertainty column with the extra noise added instead of the old uncertainty column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter a preliminary chi-square cut (keep at a high number):\n",
    "prelim_x2_cut = 20\n",
    "\n",
    "# Plot the light curve before and after estimating true uncertainties?:\n",
    "plot = False\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the plot:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_dflux(lc, indices=None, control_index=0):\n",
    "    if indices is None:\n",
    "        indices = lc.get_ix(control_index=control_index)\n",
    "    return np.median(lc.lcs[control_index].t.loc[indices, 'duJy'])\n",
    "\n",
    "def get_stdev(lc, indices=None, control_index=0):\n",
    "    lc.lcs[control_index].calcaverage_sigmacutloop('uJy', indices=indices, Nsigma=3.0, median_firstiteration=True)\n",
    "    return lc.lcs[control_index].statparams['stdev']\n",
    "\n",
    "def get_sigma_extra(median_dflux, stdev):\n",
    "    return max(0, np.sqrt(stdev**2 - median_dflux**2))\n",
    "    # make sure bigger than 0 \n",
    "\n",
    "def get_stats(lc):\n",
    "    stats = pd.DataFrame(columns=['control_index', 'median_dflux', 'stdev', 'sigma_extra'])\n",
    "    stats['control_index'] = list(range(1, lc.num_controls+1))\n",
    "    stats.set_index('control_index', inplace=True)\n",
    "\n",
    "    for control_index in range(1, lc.num_controls+1):\n",
    "        dflux_clean_ix = lc.lcs[control_index].ix_unmasked('Mask', maskval=flags['uncertainty'])\n",
    "        x2_clean_ix = lc.lcs[control_index].ix_inrange(colnames=['chi/N'], uplim=prelim_x2_cut, exclude_uplim=True)\n",
    "        clean_ix = AandB(dflux_clean_ix, x2_clean_ix)\n",
    "\n",
    "        median_dflux = get_median_dflux(lc, indices=clean_ix, control_index=control_index)\n",
    "        stdev = get_stdev(lc, indices=clean_ix, control_index=control_index)\n",
    "        sigma_extra = get_sigma_extra(median_dflux, stdev)\n",
    "\n",
    "        stats.loc[control_index, 'median_dflux'] = median_dflux\n",
    "        stats.loc[control_index, 'stdev'] = stdev\n",
    "        stats.loc[control_index, 'sigma_extra'] = sigma_extra\n",
    "\n",
    "    print(stats)\n",
    "    return stats\n",
    "\n",
    "def get_final_sigma_extra(stats):\n",
    "    return np.median(stats['sigma_extra'])\n",
    "\n",
    "def hist_stats(stats):\n",
    "    fig, ax1 = plt.subplots(1, constrained_layout=True)\n",
    "    fig.set_figwidth(4)\n",
    "    fig.set_figheight(2)\n",
    "\n",
    "    max_y = max(max(stats['median_dflux']), max(stats['stdev']), max(stats['sigma_extra']))\n",
    "    bins = np.linspace(0, max_y+3, 10)\n",
    "    median_sigma_extra = np.median(stats['sigma_extra'])\n",
    "    #labels = ['median dflux', 'std dev', 'sigma extra']\n",
    "    #ax1.hist([stats['median_dflux'], stats['stdev'], stats['sigma_extra']], bins=bins, label=labels)\n",
    "   \n",
    "    ax1.hist(stats['median_dflux'], bins=bins, alpha=0.5, label='median dflux')\n",
    "    \n",
    "    ax1.hist(stats['stdev'], bins=bins, alpha=0.5, label='std dev')\n",
    "\n",
    "    ax1.hist(stats['sigma_extra'], bins=bins, alpha=0.5, label='sigma extra')\n",
    "    ax1.axvline(median_sigma_extra, color='green', label='median sigma extra')\n",
    "\n",
    "    ax1.legend(facecolor='white', framealpha=1, loc='upper left',  bbox_to_anchor=(1, 1))\n",
    "\n",
    "def add_noise(lc, sigma_extra):\n",
    "    lc.dflux_colnames = ['duJy_new'] * (lc.num_controls+1)\n",
    "    for control_index in range(0, lc.num_controls+1):\n",
    "        lc.lcs[control_index].t['duJy_new'] = np.sqrt(lc.lcs[control_index].t['duJy']*lc.lcs[control_index].t['duJy'] + sigma_extra**2)\n",
    "        lc.recalculate_fdf(control_index=control_index)\n",
    "    return lc\n",
    "\n",
    "def get_results(lc, stats, final_sigma_extra, limits=None):\n",
    "    print(f'Final sigma extra: {final_sigma_extra:0.2f}')\n",
    "\n",
    "    sigma_typical_old = np.median(stats['median_dflux'])\n",
    "    sigma_typical_new = np.sqrt(final_sigma_extra**2 + sigma_typical_old**2)\n",
    "    percent_greater = 100 * ((sigma_typical_new - sigma_typical_old)/sigma_typical_old)\n",
    "    \n",
    "    print(f'We increase the typical uncertainties from {sigma_typical_old:0.2f} to {sigma_typical_new:0.2f} by adding an additional systematic uncertainty of {final_sigma_extra:0.2f} in quadrature. Proceed?')\n",
    "    print(f'New typical uncertainty is {percent_greater:0.2f}% greater than old typical uncertainty.')\n",
    "    if percent_greater >= 10:\n",
    "        answer = input('True uncertainties estimation recommended. Proceed? (y/n)')\n",
    "        if answer == 'y':\n",
    "            print('Calculating new uncertainties in \\'duJy_new\\' column for each light curve...') \n",
    "            lc = add_noise(lc, sigma_extra=final_sigma_extra)\n",
    "            print('Success')\n",
    "            print('Quick sanity check:')\n",
    "            print(lc.lcs[0].t[['MJD', 'uJy', 'duJy', 'duJy_new']].head())\n",
    "            if plot:\n",
    "                plot_true_uncertainties(lc, limits=limits, save_filename='true_uncerts')\n",
    "        else:\n",
    "            print('Skipping procedure.')\n",
    "    else:\n",
    "        print(f'True uncertainties estimation not needed.')\n",
    "    return lc\n",
    "\n",
    "def plot_true_uncertainties(lc, limits=None, save_filename=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, constrained_layout=True)\n",
    "    fig.set_figwidth(7)\n",
    "    fig.set_figheight(5)\n",
    "\n",
    "    ax1.set_title(f'SN {tnsname} {filt}-band flux\\nbefore true uncertainties estimation')\n",
    "    ax1.minorticks_on()\n",
    "    ax1.tick_params(direction='in', which='both')\n",
    "    ax1.get_xaxis().set_ticks([])\n",
    "    ax1.set_ylabel(r'Flux ($\\mu$Jy)')\n",
    "    ax1.axhline(linewidth=1, color='k')\n",
    "\n",
    "    ax2.set_title(f'after true uncertainties estimation')\n",
    "    ax2.minorticks_on()\n",
    "    ax2.tick_params(direction='in', which='both')\n",
    "    ax2.set_ylabel(r'Flux ($\\mu$Jy)')\n",
    "    ax2.set_xlabel('MJD')\n",
    "    ax2.axhline(linewidth=1, color='k')\n",
    "\n",
    "    # set x and y limits\n",
    "    limits = set_xylimits(lc, limits)\n",
    "    if not limits is None:\n",
    "        ax1.set_xlim(limits[0],limits[1])\n",
    "        ax1.set_ylim(limits[2],limits[3])\n",
    "        ax2.set_xlim(limits[0],limits[1])\n",
    "        ax2.set_ylim(limits[2],limits[3])\n",
    "\n",
    "    ax1.errorbar(lc.lcs[0].t['MJD'], lc.lcs[0].t['uJy'], yerr=lc.lcs[0].t['duJy'], fmt='none', ecolor=sn_flux, elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5)\n",
    "    ax1.scatter(lc.lcs[0].t['MJD'], lc.lcs[0].t['uJy'], s=marker_size, lw=marker_edgewidth, color=sn_flux, marker='o', alpha=0.5)\n",
    "\n",
    "    ax2.errorbar(lc.lcs[0].t['MJD'], lc.lcs[0].t['uJy'], yerr=lc.lcs[0].t['duJy_new'], fmt='none', ecolor=sn_flux, elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5)\n",
    "    ax2.scatter(lc.lcs[0].t['MJD'], lc.lcs[0].t['uJy'], s=marker_size, lw=marker_edgewidth, color=sn_flux, marker='o', alpha=0.5)\n",
    "\n",
    "    save_plot(save_filename=save_filename)\n",
    "\n",
    "stats = get_stats(lc)\n",
    "hist_stats(stats)\n",
    "final_sigma_extra = get_final_sigma_extra(stats)\n",
    "lc = get_results(lc, stats, final_sigma_extra, limits=limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Dynamic chi-square cut\n",
    "\n",
    "### 5a) Plot the flux/dflux and chi-square distributions\n",
    "\n",
    "The following two histograms display the flux/dflux and chi-square distributions of the SN and control light curves. Both histograms show probability density so as to ease comparison between the groups plotted within each histogram.\n",
    "\n",
    "- The first histogram focuses on the baseline flux/dflux (µJy/dµJy) measurements, where we can expect the flux to equal 0. In orange, we plot flux/dflux (µJy/dµJy) measurements with a chi-square value less than or equal to `x2bound`, which is currently set to 5 below; in blue, we plot flux/dflux (µJy/dµJy) measurements with a chi-square value greater than `x2bound`. \n",
    "- The second histogram focuses on the baseline chi-square measurements. In green, we plot chi-square measurements with an abs(µJy/dµJy) value less than or equal to `stnbound`, which is currently set to 3 below; in red, we plot chi-square measurements with an abs(µJy/dµJy) value greater than `stnbound`. \n",
    "\n",
    "Ideally, all measurements with a chi-square value less than or equal to `x2bound` should have an abs(µJy/dµJy) value less than or equal to `stnbound`, and measurements with a chi-square value greater than `x2bound` should have an abs(µJy/dµJy) value greater than `stnbound`. Our goal is to separate good measurements from bad measurements using a chi-square cut; in order for our cut to be effective, these histograms should hopefully showcase this relation between the target SN's flux/dflux and chi-square measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the bound that should separate a good chi-square measurement from a bad one:\n",
    "x2bound = 5.0\n",
    "\n",
    "# Enter the bound that should separate a good abs(flux/dflux) measurement from a bad one:\n",
    "stnbound = 3.0\n",
    "\n",
    "# Optionally, set the density (if True, each bin will display the bin's raw count divided by total count and bin width)\n",
    "# flux/dflux histogram density:\n",
    "fdf_density = True\n",
    "# chi-square histogram density:\n",
    "#x2_density = True\n",
    "\n",
    "# Optionally, manually enter the histograms' x limits here:\n",
    "# flux/dflux histogram x limits:\n",
    "fdf_xlims = (-30, 30)\n",
    "# chi-square histogram x limits:\n",
    "#x2_xlims = (-10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdf_hist(lc, all_controls, limits=None, density=True): \n",
    "    preSN_ix = lc.get_pre_SN_ix()\n",
    "    preSN_good_ix = lc.lcs[0].ix_inrange(colnames=['chi/N'], uplim=x2bound, indices=preSN_ix)\n",
    "    preSN_bad_ix = AnotB(preSN_ix, preSN_good_ix)\n",
    "\n",
    "    controls_ix = all_controls.t.index.values\n",
    "    controls_good_ix = all_controls.ix_inrange(colnames=['chi/N'], uplim=x2bound)\n",
    "    controls_bad_ix = AnotB(controls_ix, controls_good_ix)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, constrained_layout=True)\n",
    "    fig.set_figwidth(3)\n",
    "    fig.set_figheight(3)\n",
    "\n",
    "    ax1.set_title('for pre-SN light curve', fontsize=12)\n",
    "    ax2.set_title('for control light curves', fontsize=12)\n",
    "    ax2.set_xlabel('µJy/dµJy')\n",
    "    \n",
    "    bins = None\n",
    "    if not limits is None:\n",
    "        bins = np.linspace(limits[0]-10, limits[1]+10, 20)\n",
    "        ax1.get_xaxis().set_ticks([])\n",
    "        ax1.set_xlim(limits[0], limits[1])\n",
    "        ax2.set_xlim(limits[0], limits[1])\n",
    "\n",
    "    ax1.hist(lc.lcs[0].t.loc[preSN_good_ix,'uJy/duJy'], bins=bins, color='green', alpha=0.5, label=f'Data with chi-square<{x2bound}', density=density)\n",
    "    ax1.hist(lc.lcs[0].t.loc[preSN_bad_ix,'uJy/duJy'], bins=bins, color='red', alpha=0.5, label=f'Data with chi-square≥{x2bound}', density=density)\n",
    "    \n",
    "    ax2.hist(all_controls.t.loc[controls_good_ix,'uJy/duJy'], bins=bins, color='green', alpha=0.5, density=density)\n",
    "    ax2.hist(all_controls.t.loc[controls_bad_ix,'uJy/duJy'], bins=bins, color='red', alpha=0.5, density=density)\n",
    "\n",
    "    fig.legend(facecolor='white', framealpha=1, loc='upper left',  bbox_to_anchor=(1, 1))\n",
    "\n",
    "def x2_hist(lc, all_controls, limits=None, density=True):\n",
    "    preSN_ix = lc.get_pre_SN_ix()\n",
    "    preSN_good_ix = lc.lcs[0].ix_inrange(colnames=['uJy/duJy'], lowlim=-stnbound, uplim=stnbound, indices=preSN_ix)\n",
    "    preSN_bad_ix = AnotB(preSN_ix, preSN_good_ix)\n",
    "\n",
    "    controls_ix = all_controls.t.index.values\n",
    "    controls_good_ix = all_controls.ix_inrange(colnames=['uJy/duJy'], lowlim=-stnbound, uplim=stnbound)\n",
    "    controls_bad_ix = AnotB(controls_ix, controls_good_ix)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, constrained_layout=True)\n",
    "    fig.set_figwidth(3)\n",
    "    fig.set_figheight(3)\n",
    "\n",
    "    ax1.set_title('for pre-SN light curve', fontsize=12)\n",
    "    ax2.set_title('for control light curves', fontsize=12)\n",
    "    ax2.set_xlabel('chi-square')\n",
    "    \n",
    "    bins = None\n",
    "    if not limits is None:\n",
    "        bins = np.linspace(limits[0]-10, limits[1]+10, 20)\n",
    "        ax1.get_xaxis().set_ticks([])\n",
    "        ax1.set_xlim(limits[0], limits[1])\n",
    "        ax2.set_xlim(limits[0], limits[1])\n",
    "\n",
    "    ax1.hist(lc.lcs[0].t.loc[preSN_good_ix,'chi/N'], bins=bins, color='green', alpha=0.5, label=f'Data with µJy/dµJy<{stnbound}', density=density)\n",
    "    ax1.hist(lc.lcs[0].t.loc[preSN_bad_ix,'chi/N'], bins=bins, color='red', alpha=0.5, label=f'Data with µJy/dµJy≥{stnbound}', density=density)\n",
    "    \n",
    "    ax2.hist(all_controls.t.loc[controls_good_ix,'chi/N'], bins=bins, color='green', alpha=0.5, density=density)\n",
    "    ax2.hist(all_controls.t.loc[controls_bad_ix,'chi/N'], bins=bins, color='red', alpha=0.5, density=density)\n",
    "\n",
    "    fig.legend(facecolor='white', framealpha=1, loc='upper left',  bbox_to_anchor=(1, 1))\n",
    "\n",
    "controls = [lc.lcs[control_index].t for control_index in lc.lcs if control_index > 0]\n",
    "all_controls = pdastrostatsclass()\n",
    "all_controls.t = pd.concat(controls, ignore_index=True)\n",
    "\n",
    "fdf_hist(lc, all_controls, limits=fdf_xlims, density=fdf_density)\n",
    "#x2_hist(lc, all_controls, limits=x2_xlims, density=x2_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b) Calculate best chi-square cut based on contamination and loss\n",
    "\n",
    "The following cells use two factors, <strong>contamination</strong> and <strong>loss</strong>, to attempt to calculate an optimal PSF chi-square cut for the target SN, with flux/dflux as the deciding factor of what constitutes a good measurement vs. a bad measurement. We aim to separate good measurements from bad using the calculated chi-square cut by removing as much contamination as possible with the smallest loss possible. Since we can assume that the expected value of the baseline flux is 0, we look only at the baseline measurements before the SN occurs in order to determine the best chi-square cut for the SN itself.\n",
    "\n",
    "First, we decide what will determine a good measurement vs. a bad measurement using a factor outside of the chi-square values. Our chosen factor is the <strong>absolute value of flux (µJy) divided by dflux (dµJy)</strong>. The recommended boundary is a value of 3, such that any measurements with a value of abs(µJy/dµJy) less than or equal to 3 are regarded as \"good\" measurements, and any measurements with a value of abs(µJy/dµJy) greater than 3 are regarded as \"bad\" measurements. You can set this boundary to a different number by changing the value of `stn_cut` below.\n",
    "\n",
    "Next, we set the upper and lower bounds of our final chi-square cut. We start at a low value of 3 (which can be changed by setting the value of `cut_start` below) and end at 50 (this value is inclusive and can be changed by setting the value of `cut_stop` below) with a step size of 1 (`cut_step` below). <strong>For chi-square cuts falling on or between `cut_start` and `cut_stop` in increments of `cut_step`, we can begin to calculate contamination and loss percentages.</strong>\n",
    "\n",
    "We define contamination to be the number of bad kept measurements over the total number of kept measurements for that chi-square cut (<strong>contamination = Nbad,kept/Nkept</strong>). For our final chi-square cut, we can also set a limit on what maximum percent contamination we want to have--the recommended value is <strong>15%</strong> but can be changed by setting the value of `contam_lim` below.\n",
    "\n",
    "We define loss to be the number of good cut measurements over the total number of good measurements for that chi-square cut (<strong>loss = Ngood,cut/Ngood</strong>). For our final chi-square cut, we can also set a limit on what maximum percent loss we want to have--the recommended value is <strong>10%</strong> but can be changed by setting the value of `loss_lim` below.\n",
    "\n",
    "Finally, we define which limit (`contam_lim` or `loss_lim`) to prioritize in the event that an optimal chi-square cut fitting both limits is not found. The default prioritized limit is `loss_lim` but can be changed by setting the value of `lim_to_prioritize` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the abs(uJy/duJy) boundary that will determine a \"good\" measurement vs. \"bad\" measurement:\n",
    "stn_cut = 3\n",
    "\n",
    "# Enter the bounds for the final chi-square cut (minimum cut, maximum cut, and step):\n",
    "cut_start = 3 # this is inclusive\n",
    "cut_stop = 50 # this is inclusive\n",
    "cut_step = 1\n",
    "\n",
    "# Enter the contamination limit (contamination = Nbad,kept/Nkept must be <= contam_lim% \n",
    "# for the final chi-square cut):\n",
    "contam_lim = 15.0\n",
    "\n",
    "# Enter the loss limit (loss = Ngood,cut/Ngood must be >= loss_lim%\n",
    "# for the final chi-square cut):\n",
    "loss_lim = 10.0\n",
    "\n",
    "# Enter the limit to prioritize (must be 'loss_lim' or 'contam_lim') in the event that\n",
    "# one or both limits are not met:\n",
    "lim_to_prioritize = 'loss_lim'\n",
    "\n",
    "# If set to True, we use the pre-SN light curve to find the best cut.\n",
    "# Else, we use the control light curves.\n",
    "use_preSN_lc = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section describes in detail how we determine the final chi-square cut using the given contamination and loss limits (feel free to skip).\n",
    "\n",
    "For each given limit (contamination and loss), we calculate a range of valid cuts whose contamination/loss percentage is less than that limit and then choose a single cut within that valid range. Then, we pass through a decision tree to determine which of the two suggested cuts to use using a variety of factors (including the user's selected `lim_to_prioritize`).\n",
    "\n",
    "When choosing the loss cut according to the loss percentage limit `loss_lim`:\n",
    "- <strong>If all loss percentages are below the limit</strong> `loss_lim`, all cuts falling on or between `cut_start` and `cut_stop` are valid.\n",
    "- <strong>If all loss percentages are above the limit</strong> `loss_lim`, a cut with the required loss percentage is not possible; therefore, any cuts with the smallest percentage of loss are valid.\n",
    "- <strong>Otherwise</strong>, the valid range of cuts includes any cuts with the loss percentage less than or equal to the limit `loss_lim`.\n",
    "- The chosen cut for this limit is the <strong>minimum cut</strong> within the stated valid range of cuts.\n",
    "\n",
    "When choosing the loss cut according to the contamination percentage limit `contam_lim`:\n",
    "- <strong>If all contamination percentages are below the limit</strong> `contam_lim`, all cuts falling on or between `cut_start` and `cut_stop` are valid.\n",
    "- <strong>If all contamination percentages are above the limit</strong> `contam_lim`, a cut with the required contamination percentage is not possible; therefore, any cuts with the smallest percentage of contamination are valid.\n",
    "- <strong>Otherwise</strong>, the valid range of cuts includes any cuts with the contamination percentage less than or equal to the limit `contam_lim`.\n",
    "- The chosen cut for this limit is the <strong>maximum cut</strong> within the stated valid range of cuts.\n",
    "\n",
    "After we have calculated two suggested cuts based on the loss and contamination percentage limits, we follow the decision tree in order to suggest a final cut:\n",
    "- If both loss and contamination cut percentages were chosen from a range that spanned from `cut_start` to `cut_stop`, we set the final cut to `cut_start`.\n",
    "- If one cut's percentage was chosen from a range that spanned from `cut_start` to `cut_stop` and the other cut's percentage was not, we set the final cut to the latter cut.\n",
    "- If both percentages were chosen from ranges that fell above their respective limits, we suggest reselecting either or both limits.\n",
    "- Otherwise, we take into account the user's prioritized limit `lim_to_prioritize`:\n",
    "    - If the loss cut is greater than the contamination cut, we set the final cut to whichever cut is associated with `lim_to_prioritize`.\n",
    "    - Otherwise, if `lim_to_prioritize` is set to `contam_lim`, we set the final cut to the loss cut, and if `lim_to_prioritize` is set to `loss_lim`, we set the final cut to the contamination cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lim_cuts(lim_cuts, contam_lim_cut, loss_lim_cut):\n",
    "    loss_color = 'darkmagenta'\n",
    "    contam_color = 'teal'\n",
    "\n",
    "    fig, ax1 = plt.subplots(1, constrained_layout=True)\n",
    "    fig.set_figwidth(5.5)\n",
    "    fig.set_figheight(3)\n",
    "\n",
    "    #ax1.set_ylim(0,1) # DELETE ME\n",
    "    ax1.set_title(f'SN {tnsname} {filt}-band chi-square cut')\n",
    "\n",
    "    #ax1.set_xlim(0, max())\n",
    "\n",
    "    ax1.minorticks_on()\n",
    "    ax1.tick_params(direction='in', which='both')\n",
    "    if use_preSN_lc:\n",
    "        ax1.set_ylabel(f'% pre-SN light curve measurements')\n",
    "    else:\n",
    "        ax1.set_ylabel(f'% control light curve measurements')\n",
    "    ax1.set_xlabel('Chi-square cut')\n",
    "    ax1.axhline(linewidth=1, color='k')\n",
    "\n",
    "    ax1.axhline(loss_lim, linewidth=1, color=loss_color, linestyle='dotted')#, label='Loss limit')\n",
    "    ax1.text(3.5, loss_lim+0.1, 'Loss limit', color=loss_color)\n",
    "    ax1.plot(lim_cuts['PSF Chi-Square Cut'], lim_cuts['Ploss'], ms=3.5, color=loss_color, marker='o', label='Loss')\n",
    "    ax1.axvline(x=loss_lim_cut, color=loss_color, linestyle='--', label='Loss cut')\n",
    "    ax1.axvspan(loss_lim_cut, cut_stop, alpha=0.2, color=loss_color)\n",
    "\n",
    "    ax1.axhline(contam_lim, linewidth=1, color=contam_color, linestyle='dotted')#, label='Contamination limit')\n",
    "    ax1.text(3.5, contam_lim+0.1, 'Contamination limit', color=contam_color)\n",
    "    ax1.plot(lim_cuts['PSF Chi-Square Cut'], lim_cuts['Pcontamination'], ms=3.5, color=contam_color, marker='o', label='Contamination')\n",
    "    ax1.axvline(x=contam_lim_cut, color=contam_color, linestyle='--', label='Contamination cut')\n",
    "    ax1.axvspan(cut_start, contam_lim_cut, alpha=0.2, color=contam_color)\n",
    "\n",
    "    if ax1.get_ylim()[1] < loss_lim+2 or ax1.get_ylim()[1] < contam_lim+2:\n",
    "        ax1.set_ylim(0, max(loss_lim+2, contam_lim+2))\n",
    "\n",
    "    ax1.legend(facecolor='white', framealpha=1, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "def choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case):\n",
    "    # case 1 and 1: final_cut = 3\n",
    "    # case 1 and 2: take limit of case 2\n",
    "    # case 1 and 3: take limit of case 3\n",
    "    # case 2 and 2: print lims don't work\n",
    "    # case 2 and 3: choose_btwn_lim_cuts\n",
    "    # case 3 and 3: choose_btwn_lim_cuts\n",
    "\n",
    "    case1 = loss_case == 'below lim' or contam_case == 'below lim'\n",
    "    case2 = loss_case == 'above lim' or contam_case == 'above lim'\n",
    "    case3 = loss_case == 'crosses lim' or contam_case == 'crosses lim'\n",
    "\n",
    "    final_cut = None\n",
    "    if case1 and not case2 and not case3: # 1 and 1\n",
    "        print('Valid chi-square cut range from %0.2f to %0.2f! Setting to 3...' % (loss_lim_cut, contam_lim_cut))\n",
    "        final_cut = cut_start\n",
    "    elif case1: # 1\n",
    "        if case2: # and 2\n",
    "            if loss_case == 'above lim':\n",
    "                print('WARNING: contam_lim_cut <= %0.2f falls below limit %0.2f%%, but loss_lim_cut >= %0.2f falls above limit %0.2f%%! Setting to %0.2f...' % (contam_lim_cut, contam_lim, loss_lim_cut, loss_lim, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('WARNING: loss_lim_cut <= %0.2f falls below limit %0.2f%%, but contam_lim_cut >= %0.2f falls above limit %0.2f%%! Setting to %0.2f...' % (loss_lim_cut, loss_lim, contam_lim_cut, contam_lim, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "        else: # and 3\n",
    "            if loss_case == 'crosses lim':\n",
    "                print('Contam_lim_cut <= %0.2f falls below limit %0.2f%% and loss_lim_cut >= %0.2f crosses limit %0.2f%%, setting to %0.2f...' % (contam_lim_cut, contam_lim, loss_lim_cut, loss_lim, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('Loss_lim_cut <= %0.2f falls below limit %0.2f%% and contam_lim_cut >= %0.2f crosses limit %0.2f%%, setting to %0.2f...' % (loss_lim_cut, loss_lim, contam_lim_cut, contam_lim, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "    elif case2 and not case3: # 2 and 2\n",
    "        print('ERROR: chi-square loss_lim_cut >= %0.2f and contam_lim_cut <= %0.2f both fall above limits %0.2f%% and %0.2f%%! Try setting less strict limits. Setting final cut to nan.' % (loss_lim_cut, contam_lim_cut, loss_lim, contam_lim))\n",
    "        final_cut = np.nan\n",
    "    else: # 2 and 3 or 3 and 3\n",
    "        if loss_lim_cut > contam_lim_cut:\n",
    "            print('WARNING: chi-square loss_lim_cut >= %0.2f and contam_lim_cut <= %0.2f do not overlap! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %0.2f...' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "        else:\n",
    "            print('Valid chi-square cut range from %0.2f to %0.2f! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "    return final_cut\n",
    "\n",
    "def get_lim_cuts(lim_cuts): \n",
    "    contam_lim_cut = None\n",
    "    loss_lim_cut = None\n",
    "    contam_case = None\n",
    "    loss_case = None\n",
    "\n",
    "    sortby_loss = lim_cuts.iloc[(lim_cuts['Ploss']).argsort()].reset_index()\n",
    "    min_loss = sortby_loss.loc[0,'Ploss']\n",
    "    max_loss = sortby_loss.loc[len(sortby_loss)-1,'Ploss']\n",
    "    # if all loss below lim, loss_lim_cut is min cut\n",
    "    if min_loss < loss_lim and max_loss < loss_lim:\n",
    "        loss_case = 'below lim'\n",
    "        loss_lim_cut = lim_cuts.loc[0,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all loss above lim, loss_lim_cut is min cut with min% loss\n",
    "        if min_loss > loss_lim and max_loss > loss_lim:\n",
    "            loss_case = 'above lim'\n",
    "            a = np.where(lim_cuts['Ploss'] == min_loss)[0]\n",
    "            b = lim_cuts.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            loss_lim_cut = c.loc[0,'PSF Chi-Square Cut']\n",
    "        # else if loss crosses lim at some point, loss_lim_cut is min cut with max% loss <= loss_lim\n",
    "        else:\n",
    "            loss_case = 'crosses lim'\n",
    "            valid_cuts = sortby_loss[sortby_loss['Ploss'] <= loss_lim]\n",
    "            a = np.where(lim_cuts['Ploss'] == valid_cuts.loc[len(valid_cuts)-1,'Ploss'])[0]\n",
    "            # sort by cuts\n",
    "            b = lim_cuts.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            # get midpoint of loss1 and loss2 (two points on either side of lim)\n",
    "            loss1_i = np.where(lim_cuts['PSF Chi-Square Cut'] == c.loc[0,'PSF Chi-Square Cut'])[0][0]\n",
    "            if lim_cuts.loc[loss1_i,'Ploss'] == loss_lim:\n",
    "                loss_lim_cut = lim_cuts.loc[loss1_i,'PSF Chi-Square Cut']\n",
    "            else:\n",
    "                loss2_i = loss1_i - 1\n",
    "                x = np.array([lim_cuts.loc[loss1_i,'PSF Chi-Square Cut'], lim_cuts.loc[loss2_i,'PSF Chi-Square Cut']])\n",
    "                contam_y = np.array([lim_cuts.loc[loss1_i,'Pcontamination'], lim_cuts.loc[loss2_i,'Pcontamination']])\n",
    "                loss_y = np.array([lim_cuts.loc[loss1_i,'Ploss'], lim_cuts.loc[loss2_i,'Ploss']])\n",
    "                contam_line = np.polyfit(x,contam_y,1)\n",
    "                loss_line = np.polyfit(x,loss_y,1)\n",
    "                loss_lim_cut = (loss_lim-loss_line[1])/loss_line[0]\n",
    "\n",
    "    sortby_contam = lim_cuts.iloc[(lim_cuts['Pcontamination']).argsort()].reset_index()\n",
    "    min_contam = sortby_contam.loc[0,'Pcontamination']\n",
    "    max_contam = sortby_contam.loc[len(sortby_contam)-1,'Pcontamination']\n",
    "    # if all contam below lim, contam_lim_cut is max cut\n",
    "    if min_contam < contam_lim and max_contam < contam_lim:\n",
    "        contam_case = 'below lim'\n",
    "        contam_lim_cut = lim_cuts.loc[len(lim_cuts)-1,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all contam above lim, contam_lim_cut is max cut with min% contam\n",
    "        if min_contam > contam_lim and max_contam > contam_lim:\n",
    "            contam_case = 'above lim'\n",
    "            a = np.where(lim_cuts['Pcontamination'] == min_contam)[0]\n",
    "            b = lim_cuts.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            contam_lim_cut = c.loc[len(c)-1,'PSF Chi-Square Cut']\n",
    "        # else if contam crosses lim at some point, contam_lim_cut is max cut with max% contam <= contam_lim\n",
    "        else:\n",
    "            contam_case = 'crosses lim'\n",
    "            valid_cuts = sortby_contam[sortby_contam['Pcontamination'] <= contam_lim]\n",
    "            a = np.where(lim_cuts['Pcontamination'] == valid_cuts.loc[len(valid_cuts)-1,'Pcontamination'])[0]\n",
    "            # sort by cuts\n",
    "            b = lim_cuts.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            # get midpoint of contam1 and contam2 (two points on either side of lim)\n",
    "            contam1_i = np.where(lim_cuts['PSF Chi-Square Cut'] == c.loc[len(c)-1,'PSF Chi-Square Cut'])[0][0]\n",
    "            if lim_cuts.loc[contam1_i,'Pcontamination'] == contam_lim:\n",
    "                contam_lim_cut = lim_cuts.loc[contam1_i,'PSF Chi-Square Cut']\n",
    "            else:\n",
    "                contam2_i = contam1_i + 1\n",
    "                x = np.array([lim_cuts.loc[contam1_i,'PSF Chi-Square Cut'], lim_cuts.loc[contam2_i,'PSF Chi-Square Cut']])\n",
    "                contam_y = np.array([lim_cuts.loc[contam1_i,'Pcontamination'], lim_cuts.loc[contam2_i,'Pcontamination']])\n",
    "                loss_y = np.array([lim_cuts.loc[contam1_i,'Ploss'], lim_cuts.loc[contam2_i,'Ploss']])\n",
    "                contam_line = np.polyfit(x,contam_y,1)\n",
    "                loss_line = np.polyfit(x,loss_y,1)\n",
    "                contam_lim_cut = (contam_lim-contam_line[1])/contam_line[0]\n",
    "\n",
    "    return contam_lim_cut, loss_lim_cut, contam_case, loss_case\n",
    "\n",
    "def get_keptcut_indices(lc, ix, cut):\n",
    "    kept_ix = lc.ix_inrange(colnames=['chi/N'], uplim=cut, indices=ix)\n",
    "    cut_ix = AnotB(ix, kept_ix)\n",
    "    return kept_ix, cut_ix\n",
    "\n",
    "def get_goodbad_indices(lc, ix):\n",
    "    good_ix = lc.ix_inrange(colnames=['uJy/duJy'], lowlim=-stn_cut, uplim=stn_cut, indices=ix)\n",
    "    bad_ix = AnotB(ix, good_ix)\n",
    "    return good_ix, bad_ix\n",
    "\n",
    "def get_lim_cuts_data(lc, cut, ix, good_ix, bad_ix):\n",
    "    kept_ix, cut_ix = get_keptcut_indices(lc, ix, cut)\n",
    "    out = {}\n",
    "    out['PSF Chi-Square Cut'] = cut\n",
    "    out['N'] = len(ix)\n",
    "    out['Ngood'] = len(good_ix)\n",
    "    out['Nbad'] = len(bad_ix)\n",
    "    out['Nkept'] = len(kept_ix)\n",
    "    out['Ncut'] = len(cut_ix)\n",
    "    out['Ngood,kept'] = len(AandB(good_ix,kept_ix))\n",
    "    out['Ngood,cut'] = len(AandB(good_ix,cut_ix))\n",
    "    out['Nbad,kept'] = len(AandB(bad_ix,kept_ix))\n",
    "    out['Nbad,cut'] = len(AandB(bad_ix,cut_ix))\n",
    "    out['Pgood,kept'] = 100*len(AandB(good_ix,kept_ix))/len(ix)\n",
    "    out['Pgood,cut'] = 100*len(AandB(good_ix,cut_ix))/len(ix)\n",
    "    out['Pbad,kept'] = 100*len(AandB(bad_ix,kept_ix))/len(ix)\n",
    "    out['Pbad,cut'] = 100*len(AandB(bad_ix,cut_ix))/len(ix)\n",
    "    out['Ngood,kept/Ngood'] = 100*len(AandB(good_ix,kept_ix))/len(good_ix)\n",
    "    out['Ploss'] = 100*len(AandB(good_ix,cut_ix))/len(good_ix)\n",
    "    out['Pcontamination'] = 100*len(AandB(bad_ix,kept_ix))/len( kept_ix)\n",
    "    return out\n",
    "\n",
    "def get_lim_cuts_table(lc, ix, good_ix, bad_ix, stn_cut, cut_start, cut_stop, cut_step, is_SNlc=False):\n",
    "    print('abs(uJy/duJy) cut at %0.2f \\nx2 cut from %0.2f to %0.2f inclusive, with step size %d' % (stn_cut,cut_start,cut_stop,cut_step))\n",
    "\n",
    "    lim_cuts = pd.DataFrame(columns=['PSF Chi-Square Cut', 'N', 'Ngood', 'Nbad', 'Nkept', 'Ncut', 'Ngood,kept', 'Ngood,cut', 'Nbad,kept', 'Nbad,cut',\n",
    "                                     'Pgood,kept', 'Pgood,cut', 'Pbad,kept', 'Pbad,cut', 'Ngood,kept/Ngood', 'Ploss', 'Pcontamination'])#,\n",
    "                                     #'Nbad,cut 3<stn<=5', 'Nbad,cut 5<stn<=10', 'Nbad,cut 10<stn', 'Nbad,kept 3<stn<=5', 'Nbad,kept 5<stn<=10', 'Nbad,kept 10<stn'])\n",
    "\n",
    "    # static cut at x2 = 50\n",
    "    x2cut_50 = np.where(lc.t['chi/N'] >= 50)[0]\n",
    "    print('Static chi square cut at 50: %0.2f%% cut for baseline' % (100*len(AandB(ix,x2cut_50))/len(ix)))\n",
    "\n",
    "    # for different x2 cuts decreasing from 50\n",
    "    for cut in range(cut_start,cut_stop+1,cut_step):\n",
    "        kept_ix, cut_ix = get_keptcut_indices(lc, ix, cut)\n",
    "        if 100*(len(kept_ix)/len(ix)) < 10:\n",
    "            # less than 10% of measurements kept, so no chi-square cuts beyond this point are valid\n",
    "            print(f'# At cut {cut}, less than 10% of measurements are kept ({100*(len(kept_ix)/len(ix)):0.2f}% kept)--skipping...')\n",
    "            continue\n",
    "        \n",
    "        data = get_lim_cuts_data(lc, cut, ix, good_ix, bad_ix)\n",
    "        lim_cuts = pd.concat([lim_cuts, pd.DataFrame([data])],ignore_index=True)\n",
    "\n",
    "    return lim_cuts\n",
    "\n",
    "if lim_to_prioritize != 'loss_lim' and lim_to_prioritize != 'contam_lim':\n",
    "    print(\"ERROR: lim_to_prioritize must be 'loss_lim' or 'contam_lim'!\")\n",
    "    sys.exit()\n",
    "print('Contamination limit: %0.2f%%\\nLoss limit: %0.2f%%' % (contam_lim,loss_lim))\n",
    "\n",
    "if use_preSN_lc:\n",
    "    lc_temp = lc.lcs[0]\n",
    "    ix = lc_temp.ix_inrange('MJD', uplim=discovery_date)\n",
    "else:\n",
    "    lc_temp = all_controls\n",
    "    ix = all_controls.t.index.values\n",
    "good_ix, bad_ix = get_goodbad_indices(lc_temp, ix)\n",
    "\n",
    "lim_cuts = get_lim_cuts_table(lc_temp, \n",
    "                              ix, \n",
    "                              good_ix, \n",
    "                              bad_ix, \n",
    "                              stn_cut, \n",
    "                              cut_start, \n",
    "                              cut_stop, \n",
    "                              cut_step, \n",
    "                              is_SNlc=use_preSN_lc)\n",
    "#print(lim_cuts.to_string())\n",
    "if lim_cuts.empty:\n",
    "    raise RuntimeError('No cuts kept more than 10%% of measurements--chi-square cut not applicable for this SN!')\n",
    "\n",
    "contam_lim_cut, loss_lim_cut, contam_case, loss_case = get_lim_cuts(lim_cuts)\n",
    "#kept_ix, cut_ix = get_keptcut_indices(lc_temp, )\n",
    "contam_data = get_lim_cuts_data(lc_temp, contam_lim_cut, ix, good_ix, bad_ix)\n",
    "loss_data = get_lim_cuts_data(lc_temp, loss_lim_cut, ix, good_ix, bad_ix)\n",
    "\n",
    "print('\\nContamination cut according to given contam_limit, with %0.2f%% contamination and %0.2f%% loss: %0.2f' % (contam_data['Pcontamination'], contam_data['Ploss'], contam_lim_cut))\n",
    "if contam_case == 'above lim':\n",
    "    print('WARNING: Contamination cut not possible with contamination <= contam_lim %0.1f!' % contam_lim)\n",
    "print('Loss cut according to given loss_limit, with %0.2f%% contamination and %0.2f%% loss: %0.2f' % (loss_data['Pcontamination'], loss_data['Ploss'], loss_lim_cut))\n",
    "if loss_case == 'above lim':\n",
    "    print('WARNING: Loss cut not possible with loss <= loss_lim %0.2f!' % loss_lim)\n",
    "\n",
    "final_cut = choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case)\n",
    "\n",
    "if np.isnan(final_cut):\n",
    "    print('\\nERROR: Final suggested chi-square cut could not be determined. We suggest rethinking your contamination and loss limits.')\n",
    "    Pcontamination = np.nan\n",
    "    Ploss = np.nan\n",
    "else:\n",
    "    if final_cut == contam_lim_cut:\n",
    "        Pcontamination = contam_data['Pcontamination']\n",
    "        Ploss = contam_data['Ploss']\n",
    "    else: # final_cut == loss_lim_cut\n",
    "        Pcontamination = loss_data['Pcontamination']\n",
    "        Ploss = loss_data['Ploss']\n",
    "    print('\\nFinal suggested chi-square cut is %0.2f, with %0.2f%% contamination and %0.2f%% loss.' % (final_cut, Pcontamination, Ploss))\n",
    "    if (Pcontamination > contam_lim):\n",
    "        print('WARNING: Final cut\\'s contamination %0.2f%% exceeds contam_lim %0.2f%%!' % (Pcontamination,contam_lim))\n",
    "    if (Ploss > loss_lim):\n",
    "        print('WARNING: Final cut\\'s loss exceeds %0.2f%% loss_lim %0.2f%%!' % (Ploss,loss_lim))\n",
    "\n",
    "fdf_hist(lc, all_controls, limits=fdf_xlims, density=fdf_density)\n",
    "plot_lim_cuts(lim_cuts, contam_lim_cut, loss_lim_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c) Confirm or override the final cut, apply final cut, and optionally plot\n",
    "\n",
    "For this last section, we apply the final chi-square cut to the SN and control light curves. We warn that for very bright SNe, the chi-square values may increase even for good measurements due to imperfection in PSF fitting--therefore, we recommend that the user double-check the chi-square values (or this section's plot) to verify that the cut is working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the light curve before and after the applied chi-square cut?:\n",
    "plot = True\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the chi-square cut plot:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chisquare_cut(lc, chisquare_cut, control_index=0):\n",
    "    ix = lc.get_ix(control_index=control_index)\n",
    "    kept_ix = lc.lcs[control_index].ix_inrange(colnames=['chi/N'],uplim=chisquare_cut)\n",
    "    cut_ix = AnotB(ix, kept_ix)\n",
    "    lc.update_mask_col(flags['chisquare'], cut_ix, control_index=control_index)\n",
    "\n",
    "    if control_index == 0:\n",
    "        num_cut = len(cut_ix)\n",
    "        percent_cut = 100 * num_cut/len(ix)\n",
    "        print_statistics(num_cut, percent_cut)\n",
    "        f.write(f'\\n\\n## Chi-square cut\\nNumber of cut measurements: {num_cut:d}\\nPercent of total measurements cut: {percent_cut:0.2f}%\\nHex value in \"Mask\" column: 0x2')\n",
    "\n",
    "        if plot or percent_cut > 10:\n",
    "            limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "            plot_cut_lc(lc, 'Chi-square cut', flags['chisquare'], limits=limits, save_filename='chisquare_cut')\n",
    "\n",
    "def get_percent_greater(a, b):\n",
    "    return 100 * ((a - b)/b)\n",
    "\n",
    "def sanity_check(lc, all_controls, chisquare_cut):\n",
    "    postSN_ix = lc.get_post_SN_ix()\n",
    "    postSN_kept_ix, postSN_cut_ix = get_keptcut_indices(lc.lcs[0], postSN_ix, chisquare_cut)\n",
    "    postSN_percent_cut = len(postSN_cut_ix)/len(postSN_ix)\n",
    "\n",
    "    preSN_ix = lc.get_pre_SN_ix() #.get_ix(control_index=control_index)\n",
    "    preSN_kept_ix, preSN_cut_ix = get_keptcut_indices(lc.lcs[0], preSN_ix, chisquare_cut)\n",
    "    preSN_percent_cut = len(preSN_cut_ix)/len(preSN_ix)\n",
    "\n",
    "    controls_ix = all_controls.t.index.values\n",
    "    controls_kept_ix, controls_cut_ix = get_keptcut_indices(all_controls, controls_ix, chisquare_cut)\n",
    "    controls_percent_cut = len(controls_cut_ix)/len(controls_ix)\n",
    "\n",
    "    postSN_percent_greater = get_percent_greater(postSN_percent_cut, controls_percent_cut)\n",
    "    preSN_percent_greater = get_percent_greater(preSN_percent_cut, controls_percent_cut)\n",
    "\n",
    "    print('\\nSanity check:')\n",
    "    print(f'# {controls_percent_cut:0.4f}% of measurements cut in control light curves')\n",
    "    \n",
    "    out = f'{postSN_percent_cut:0.4f}% of measurements cut in SN light curve'\n",
    "    if postSN_percent_greater >= 50:\n",
    "        out = f'# WARNING: {out} (increase by a factor of approx. {postSN_percent_greater/100+1:0.2f})'\n",
    "        out += f'\\n  Bright SNe may cause the chi-square values to increase even at good measurements due to PSF fitting imperfections. Please double check the plot.'\n",
    "    else:\n",
    "        out = f'# {out}'\n",
    "    print(out)\n",
    "\n",
    "    out = f'{preSN_percent_cut:0.4f}% of measurements cut in pre-SN light curve'\n",
    "    if preSN_percent_greater >= 50:\n",
    "        out = f'# WARNING: {out} (increase by a factor of approx. {preSN_percent_greater/100+1:0.2f})'\n",
    "        out += f'\\n  Bright SNe may cause the chi-square values to increase even at good measurements due to PSF fitting imperfections. Please double check the plot.'\n",
    "    else:\n",
    "        out = f'# {out}'\n",
    "    print(out)\n",
    "\n",
    "answer = input(f'Accept final chi-square cut of {final_cut:0.2f} (y/n), or enter your own cut that minimizes contamination and loss?:')\n",
    "if answer != 'y':\n",
    "    final_cut = float(input('Overriding final chi-square cut; enter manual cut: '))\n",
    "    if use_preSN_lc:\n",
    "        lc_temp = lc.lcs[0]\n",
    "        ix = lc.lcs[0].ix_inrange('MJD', uplim=discovery_date)\n",
    "    else:\n",
    "        lc_temp = all_controls\n",
    "        ix = all_controls.t.index.values\n",
    "    good_ix, bad_ix = get_goodbad_indices(lc_temp, ix)\n",
    "    data = get_lim_cuts_data(lc_temp, final_cut, ix, good_ix, bad_ix)\n",
    "    print(f'Overridden: final cut is now {final_cut}, with contamination {data[\"Pcontamination\"]:0.2f}% and loss {data[\"Ploss\"]:0.2f}%')\n",
    "\n",
    "for control_index in range(lc.num_controls+1):\n",
    "    apply_chisquare_cut(lc, final_cut, control_index=control_index)\n",
    "print('Success')\n",
    "sanity_check(lc, all_controls, final_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Control light curves:  3σ-clipped average cut\n",
    "\n",
    "While the chi-square and uncertainty cuts are effective in cutting out a majority of the bad measurements, tricky cases may require a larger set of control light curves that can be used as a basis of comparison for inconsistent flux. In order to account for this inconsistent flux, we can obtain ~8 quality control forced photometry light curves in a 17\" circle pattern around the SN location OR around a nearby bright object that may be poorly subtracting. Then, we use statistics from these control light curves to cut bad measurements from the SN light curve.\n",
    "\n",
    "For a given epoch, we have 1 SN measurement for which we examine 8 control measurements within the same epoch. We know that if the control light curve measurements are NOT consistent with 0, this indicates something wrong with this epoch, so the SN measurement is unreliable. Therefore, we obtain statistics for the control light curves by calculating the 3σ-clipped average of the control flux. \n",
    "\n",
    "For the given epoch, we cut the SN measurement for which the returned control statistics fulfill any of the following criteria: \n",
    "- A returned chi-square > 2.5\n",
    "- A returned abs(flux/dflux) > 3.0\n",
    "- Number of clipped/\"bad\" measurements in the 3σ-clipped average > 2\n",
    "- Number of used/\"good\" measurements in the 3σ-clipped average < 4\n",
    "\n",
    "Measurements not fulfilling any of the criteria above but with Nclip > 0 are flagged as questionable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the bound for an epoch's maximum chi-square \n",
    "# (if x2 > x2_max, flag SN measurement):\n",
    "x2_max = 2.5\n",
    "\n",
    "# Enter the bound for an epoch's maximum abs(flux/dflux) ratio \n",
    "# (if abs(flux/dflux) > stn_max, flag SN measurement):\n",
    "stn_max = 3.0\n",
    "\n",
    "# Enter the bound for an epoch's maximum number of clipped control measurements\n",
    "# (if Nclip > Nclip_max, flag SN measurement):\n",
    "Nclip_max = 2\n",
    "\n",
    "# Enter the bound for an epoch's minimum number of good control measurements\n",
    "# (if Ngood < Ngood_min, flag SN measurement):\n",
    "Ngood_min = 4\n",
    "\n",
    "# Plot the light curve before and after the applied uncertainty cut?:\n",
    "plot = True\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the control light curve cut plot:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply control light curve cut and save flags in 'Mask' column\n",
    "\n",
    "def get_control_stats(lc):\n",
    "    print('\\nCalculating control light curve statistics...')\n",
    "\n",
    "    len_mjd = len(lc.lcs[0].t['MJD'])\n",
    "\n",
    "    # construct arrays for control lc data\n",
    "    uJy = np.full((lc.num_controls, len_mjd), np.nan)\n",
    "    duJy = np.full((lc.num_controls, len_mjd), np.nan)\n",
    "    Mask = np.full((lc.num_controls, len_mjd), 0, dtype=np.int32)\n",
    "    \n",
    "    for control_index in range(1, lc.num_controls+1):\n",
    "        if (len(lc.lcs[control_index].t) != len_mjd) or (np.array_equal(lc.lcs[0].t['MJD'], lc.lcs[control_index].t['MJD']) is False):\n",
    "            raise RuntimeError(f'ERROR: SN lc not equal to control lc for control_index {control_index}! Rerun or debug verify_mjds().')\n",
    "        else:\n",
    "            uJy[control_index-1,:] = lc.lcs[control_index].t['uJy']\n",
    "            duJy[control_index-1,:] = lc.lcs[control_index].t[lc.dflux_colnames[control_index]]\n",
    "            Mask[control_index-1,:] = lc.lcs[control_index].t['Mask']\n",
    "\n",
    "    c2_param2columnmapping = lc.lcs[0].intializecols4statparams(prefix='c2_',format4outvals='{:.2f}',skipparams=['converged','i'])\n",
    "\n",
    "    for index in range(uJy.shape[-1]):\n",
    "        pda4MJD = pdastrostatsclass()\n",
    "        pda4MJD.t['uJy'] = uJy[0:,index]\n",
    "        pda4MJD.t[lc.dflux_colnames[0]] = duJy[0:,index]\n",
    "        pda4MJD.t['Mask'] = np.bitwise_and(Mask[0:,index], flags['chisquare']|flags['uncertainty'])\n",
    "        \n",
    "        pda4MJD.calcaverage_sigmacutloop('uJy',\n",
    "                                         noisecol=lc.dflux_colnames[0],\n",
    "                                         maskcol='Mask',\n",
    "                                         maskval=(flags['chisquare']|flags['uncertainty']),\n",
    "                                         verbose=1, Nsigma=3.0, median_firstiteration=True)\n",
    "        lc.lcs[0].statresults2table(pda4MJD.statparams, c2_param2columnmapping, destindex=index)\n",
    "\n",
    "    return lc\n",
    "\n",
    "def controls_cut(lc):\n",
    "    print('Flagging SN light curve based on control light curve statistics...')\n",
    "\n",
    "    lc.lcs[0].t['c2_abs_stn'] = lc.lcs[0].t['c2_mean'] / lc.lcs[0].t['c2_mean_err']\n",
    "\n",
    "    # flag measurements according to given bounds\n",
    "    flag_x2_i = lc.lcs[0].ix_inrange(colnames=['c2_X2norm'], lowlim=x2_max, exclude_lowlim=True)\n",
    "    flag_stn_i = lc.lcs[0].ix_inrange(colnames=['c2_abs_stn'], lowlim=stn_max, exclude_lowlim=True)\n",
    "    flag_nclip_i = lc.lcs[0].ix_inrange(colnames=['c2_Nclip'], lowlim=Nclip_max, exclude_lowlim=True)\n",
    "    flag_ngood_i = lc.lcs[0].ix_inrange(colnames=['c2_Ngood'], uplim=Ngood_min, exclude_uplim=True)\n",
    "    lc.lcs[0].t.loc[flag_x2_i,'Mask'] |= flags['controls_x2']\n",
    "    lc.lcs[0].t.loc[flag_stn_i,'Mask'] |= flags['controls_stn']\n",
    "    lc.lcs[0].t.loc[flag_nclip_i,'Mask'] |= flags['controls_Nclip']\n",
    "    lc.lcs[0].t.loc[flag_ngood_i,'Mask'] |= flags['controls_Ngood']\n",
    "\n",
    "    # update mask column with control light curve cut on any measurements flagged according to given bounds\n",
    "    zero_Nclip_ix = lc.lcs[0].ix_equal('c2_Nclip', 0)\n",
    "    flags_temp = (flags['controls_x2'] | flags['controls_stn'] | flags['controls_Nclip'] | flags['controls_Ngood'])\n",
    "    unmasked_ix = lc.lcs[0].ix_unmasked('Mask',maskval=flags_temp) #lc.get_unmasked_ix(flags=flags_temp) #lc.lcs[0].ix_unmasked('Mask', maskval=flag_controls_x2|flag_controls_stn|flag_controls_Nclip|flag_controls_Ngood)\n",
    "    lc.lcs[0].t.loc[AnotB(unmasked_ix,zero_Nclip_ix),'Mask'] |= flags['controls_questionable']\n",
    "    lc.lcs[0].t.loc[AnotB(lc.get_ix(),unmasked_ix),'Mask'] |= flags['controls_bad']\n",
    "\n",
    "    # copy over SN's control cut flags to control light curve 'Mask' column\n",
    "    flags_temp = (flags['controls_questionable'] | flags['controls_x2'] | flags['controls_stn'] | flags['controls_Nclip'] | flags['controls_Ngood'])\n",
    "    flags_arr = np.full(lc.lcs[0].t['Mask'].shape, flags_temp)\n",
    "    flags_to_copy = np.bitwise_and(lc.lcs[0].t['Mask'], flags_arr)\n",
    "    for control_index in range(1,lc.num_controls+1):\n",
    "        lc.lcs[control_index].t['Mask'] = lc.lcs[control_index].t['Mask'].astype(np.int32)\n",
    "        if len(lc.lcs[control_index].t) < 1:\n",
    "            continue\n",
    "        elif len(lc.lcs[control_index].t) == 1:\n",
    "            lc.lcs[control_index].t.loc[0,'Mask']= int(lc.lcs[control_index].t.loc[0,'Mask']) | flags_to_copy\n",
    "        else:\n",
    "            lc.lcs[control_index].t['Mask'] = np.bitwise_or(lc.lcs[control_index].t['Mask'], flags_to_copy)\n",
    "\n",
    "    return lc\n",
    "\n",
    "def print_flag_stats(lc):\n",
    "    percent_cut = 100 * len(lc.lcs[0].ix_masked('Mask', maskval=flags['controls_bad'])) / len(lc.lcs[0].t) \n",
    "    percent_questionable = 100 * len(lc.lcs[0].ix_masked('Mask', maskval=flags['controls_questionable'])) / len(lc.lcs[0].t)\n",
    "\n",
    "    x2_max_pcnt = 100 * len(lc.lcs[0].ix_masked('Mask', maskval=flags['controls_x2'])) / len(lc.lcs[0].t)\n",
    "    stn_max_pcnt = 100 * len(lc.lcs[0].ix_masked('Mask', maskval=flags['controls_stn'])) / len(lc.lcs[0].t)\n",
    "    Nclip_max_pcnt = 100 * len(lc.lcs[0].ix_masked('Mask', maskval=flags['controls_Nclip'])) / len(lc.lcs[0].t)\n",
    "    Ngood_min_pcnt = 100 * len(lc.lcs[0].ix_masked('Mask', maskval=flags['controls_Ngood'])) / len(lc.lcs[0].t)\n",
    "\n",
    "    print('\\nLength of SN light curve: %d' % len(lc.lcs[0].t))\n",
    "    print('Percent of data above x2_max bound: %0.2f%%' % x2_max_pcnt)\n",
    "    print('Percent of data above stn_max bound: %0.2f%%' % stn_max_pcnt)\n",
    "    print('Percent of data above Nclip_max bound: %0.2f%%' % Nclip_max_pcnt)\n",
    "    print('Percent of data below Ngood_min bound: %0.2f%%' % Ngood_min_pcnt)\n",
    "    print('Total percent of data flagged as bad: %0.2f%%' % percent_cut)\n",
    "    print('Total percent of data flagged as questionable: %0.2f%%' % percent_questionable)\n",
    "\n",
    "    f.write(f'\\n\\n## Control light curve cut\\nPercent of data above x2_max bound: {x2_max_pcnt:0.2f}%\\nPercent of data above stn_max bound: {stn_max_pcnt:0.2f}%\\nPercent of data above Nclip_max bound: {Nclip_max_pcnt:0.2f}%\\nPercent of data below Ngood_min bound: {Ngood_min_pcnt:0.2f}%')\n",
    "    f.write(f'\\nTotal percent of data flagged as bad: {percent_cut:0.2f}%\\nTotal percent of data flagged as questionable: {percent_questionable:0.2f}%\\nHex value in \"Mask\" column (flagged as \"bad\"): 0x400000\\nHex value in \"Mask\" column (flagged as \"questionable\"): 0x80000')\n",
    "\n",
    "    if plot or percent_cut > 10:\n",
    "        limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "        plot_cut_lc(lc, 'Control light curve cut', flags['controls_bad'], limits=limits, save_filename='controls_cut')\n",
    "\n",
    "if load_controls:    \n",
    "    lc = get_control_stats(lc)\n",
    "    lc = controls_cut(lc)\n",
    "    print_flag_stats(lc)\n",
    "else:\n",
    "    print('Load_controls set to False! Skipping...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ATLAS light curve with a combination of previous cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cut_lc(lc, 'Uncertainty, chi-square, and control light curve cuts', flags['uncertainty']|flags['chisquare']|flags['controls_bad'], limits=limits, save_filename='all_cut')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Averaging and cutting bad bins\n",
    "\n",
    "Our goal is to identify and cut out bad MJD bins by taking a 3σ-clipped average of each bin. For each bin, we calculate the 3σ-clipped average of any SN measurements falling within that bin and use that average as our flux for that bin. Because the ATLAS survey takes about 4 exposures every 2 days, we usually average together approximately 4 measurements per epoch. However, out of these 4 exposures, only measurements not cut in the previous methods are averaged in the 3σ-clipped average cut. (The exception to this statement would be the case that all 4 measurements are cut in previous methods; in this case, they are averaged anyway and flagged as a bad bin.)\n",
    "\n",
    "Then we cut any measurements in the SN light curve for the given epoch for which statistics fulfill any of the following criteria: \n",
    "- A returned chi-square > 4.0\n",
    "- Number of measurements averaged < 2\n",
    "- Number of measurements clipped > 1\n",
    "\n",
    "For this part of the cleaning, we still need to improve the cutting at the peak of the SN (important epochs are sometimes cut, maybe due to fast rise, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the MJD bin size in days:\n",
    "mjd_bin_size = 1\n",
    "\n",
    "# Should MJD bins with no measurements be translated as NaN (True) \n",
    "# or removed from the averaged light curve (False)?\n",
    "keep_empty_bins = False\n",
    "\n",
    "# After flux is averaged, average magnitudes are calculated using a flux-to-magnitude conversion.\n",
    "# Magnitudes are limits if the dmagnitude is NaN. Enter these magnitudes' sigma limit:\n",
    "flux2mag_sigma_limit = 3\n",
    "\n",
    "# Enter the bound for a bin's maximum number of clipped measurements\n",
    "# (if Nclip > Nclip_max, flag day):\n",
    "Nclip_max = 1\n",
    "\n",
    "# Enter the bound for a bin's minimum number of good measurements\n",
    "# (if Ngood < Ngood_min, flag day):\n",
    "Ngood_min = 2\n",
    "\n",
    "# Enter the bound for a bin's maximum chi-square (if x2 > x2_max, flag day):\n",
    "x2_max = 4.0\n",
    "\n",
    "# Optionally, manually enter the x and y limits for the plot:\n",
    "xlim_lower = None\n",
    "xlim_upper = None\n",
    "ylim_lower = None\n",
    "ylim_upper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_lc(lc, avglc, Nclip_max, Ngood_min, x2_max, mjd_bin_size=1, flux2mag_sigma_limit=3.0, keep_empty_bins=True):\n",
    "    mjd = int(np.amin(lc.lcs[0].t['MJD']))\n",
    "    mjd_max = int(np.amax(lc.lcs[0].t['MJD']))+1\n",
    "\n",
    "    good_ix = lc.lcs[0].ix_unmasked('Mask', maskval=flags['chisquare']|flags['uncertainty'])\n",
    "\n",
    "    while mjd <= mjd_max:\n",
    "        range_ix = lc.lcs[0].ix_inrange(colnames=['MJD'], lowlim=mjd, uplim=mjd+mjd_bin_size, exclude_uplim=True)\n",
    "        range_good_ix = AandB(range_ix,good_ix)\n",
    "\n",
    "        # add new row to avglc if keep_empty_bins or any measurements present\n",
    "        if keep_empty_bins or len(range_ix) >= 1:\n",
    "            new_row = {'MJDbin':mjd+0.5*mjd_bin_size, 'Nclip':0, 'Ngood':0, 'Nexcluded':len(range_ix)-len(range_good_ix), 'Mask':0}\n",
    "            avglc_index = avglc.lcs[0].newrow(new_row)\n",
    "        \n",
    "        # if no measurements present, flag or skip over day\n",
    "        if len(range_ix) < 1:\n",
    "            if keep_empty_bins:\n",
    "                avglc.update_mask_col(flags['avg_badday'], [avglc_index])\n",
    "            mjd += mjd_bin_size\n",
    "            continue\n",
    "        \n",
    "        # if no good measurements, average values anyway and flag\n",
    "        if len(range_good_ix) < 1:\n",
    "            # average flux\n",
    "            lc.lcs[0].calcaverage_sigmacutloop('uJy', noisecol=lc.dflux_colnames[0], indices=range_ix, Nsigma=3.0, median_firstiteration=True)\n",
    "            fluxstatparams = deepcopy(lc.lcs[0].statparams)\n",
    "\n",
    "            # average mjd\n",
    "            # SHOULD NOISECOL HERE BE DUJY OR NONE??\n",
    "            lc.lcs[0].calcaverage_sigmacutloop('MJD', noisecol=lc.dflux_colnames[0], indices=fluxstatparams['ix_good'], Nsigma=0, median_firstiteration=False)\n",
    "            avg_mjd = lc.lcs[0].statparams['mean']\n",
    "\n",
    "            # add row and flag\n",
    "            avglc.lcs[0].add2row(avglc_index, {'MJD':avg_mjd, \n",
    "                                               'uJy':fluxstatparams['mean'], \n",
    "                                               'duJy':fluxstatparams['mean_err'], \n",
    "                                               'stdev':fluxstatparams['stdev'],\n",
    "                                               'x2':fluxstatparams['X2norm'],\n",
    "                                               'Nclip':fluxstatparams['Nclip'],\n",
    "                                               'Ngood':fluxstatparams['Ngood'],\n",
    "                                               'Mask':0})\n",
    "            lc.update_mask_col(flags['avg_badday'], range_ix)\n",
    "            avglc.update_mask_col(flags['avg_badday'], [avglc_index])\n",
    "\n",
    "            mjd += mjd_bin_size\n",
    "            continue\n",
    "        \n",
    "        # average good measurements\n",
    "        lc.lcs[0].calcaverage_sigmacutloop('uJy', noisecol=lc.dflux_colnames[0], indices=range_good_ix, Nsigma=3.0, median_firstiteration=True)\n",
    "        fluxstatparams = deepcopy(lc.lcs[0].statparams)\n",
    "\n",
    "        if fluxstatparams['mean'] is None or len(fluxstatparams['ix_good']) < 1:\n",
    "            lc.update_mask_col(flags['avg_badday'], range_ix)\n",
    "            avglc.update_mask_col(flags['avg_badday'], [avglc_index])\n",
    "            mjd += mjd_bin_size\n",
    "            continue\n",
    "\n",
    "        # average mjd\n",
    "        # SHOULD NOISECOL HERE BE DUJY OR NONE??\n",
    "        lc.lcs[0].calcaverage_sigmacutloop('MJD', noisecol=lc.dflux_colnames[0], indices=fluxstatparams['ix_good'], Nsigma=0, median_firstiteration=False)\n",
    "        avg_mjd = lc.lcs[0].statparams['mean']\n",
    "\n",
    "        # add row\n",
    "        avglc.lcs[0].add2row(avglc_index, {'MJD':avg_mjd, \n",
    "                                           'uJy':fluxstatparams['mean'], \n",
    "                                           'duJy':fluxstatparams['mean_err'], \n",
    "                                           'stdev':fluxstatparams['stdev'],\n",
    "                                           'x2':fluxstatparams['X2norm'],\n",
    "                                           'Nclip':fluxstatparams['Nclip'],\n",
    "                                           'Ngood':fluxstatparams['Ngood'],\n",
    "                                           'Mask':0})\n",
    "        \n",
    "        # flag clipped measurements in lc\n",
    "        if len(fluxstatparams['ix_clip']) > 0:\n",
    "            lc.update_mask_col(flags['avg_ixclip'], fluxstatparams['ix_clip'])\n",
    "        \n",
    "        # if small number within this bin, flag measurements\n",
    "        if len(range_good_ix) < 3:\n",
    "            lc.update_mask_col(flags['avg_smallnum'], range_good_ix) # CHANGE TO RANGE_I??\n",
    "            avglc.update_mask_col(flags['avg_smallnum'], [avglc_index])\n",
    "        # else check sigmacut bounds and flag\n",
    "        else:\n",
    "            is_bad = False\n",
    "            if fluxstatparams['Ngood'] < Ngood_min:\n",
    "                is_bad = True\n",
    "            if fluxstatparams['Nclip'] > Nclip_max:\n",
    "                is_bad = True\n",
    "            if not(fluxstatparams['X2norm'] is None) and fluxstatparams['X2norm'] > x2_max:\n",
    "                is_bad = True\n",
    "            if is_bad:\n",
    "                lc.update_mask_col(flags['avg_badday'], range_ix)\n",
    "                avglc.update_mask_col(flags['avg_badday'], [avglc_index])\n",
    "\n",
    "        mjd += mjd_bin_size\n",
    "\n",
    "    # convert flux to magnitude and dflux to dmagnitude\n",
    "    for col in ['uJy','duJy']: \n",
    "        avglc.lcs[0].t[col] =avglc.lcs[0].t[col].astype(float)\n",
    "    avglc.lcs[0].flux2mag('uJy','duJy','m','dm', zpt=23.9, upperlim_Nsigma=flux2mag_sigma_limit)\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "    avglc.drop_extra_columns()\n",
    "\n",
    "    for col in ['Nclip','Ngood','Nexcluded','Mask']: \n",
    "        avglc.lcs[0].t[col] = avglc.lcs[0].t[col].astype(np.int32)\n",
    "\n",
    "    return avglc\n",
    "\n",
    "if len(lc.lcs[0].t) < 1:\n",
    "    print('ERROR: No data in lc so cannot average; exiting... ')\n",
    "    sys.exit()\n",
    "\n",
    "avglc = atlas_lc(tnsname, is_averaged=True, mjd_bin_size=mjd_bin_size, discdate=lc.discdate)\n",
    "avglc.lcs[0] = pdastrostatsclass(columns=['MJD','MJDbin','uJy','duJy','stdev','x2','Nclip','Ngood','Nexcluded','Mask'],hexcols=['Mask'])\n",
    "print('Averaging light curve with the following criteria: MJD bin size = %0.1f day(s), Nclip_max = %d, Ngood_min = %d, x2_max = %0.2f... ' % (mjd_bin_size, Nclip_max, Ngood_min, x2_max))\n",
    "avglc = average_lc(lc, avglc, Nclip_max, Ngood_min, x2_max, keep_empty_bins=keep_empty_bins)\n",
    "\n",
    "# print statistics and plot\n",
    "num_cut = len(avglc.lcs[0].ix_masked('Mask', flags['avg_badday']))\n",
    "percent_cut = (num_cut/len(avglc.lcs[0].t)) * 100\n",
    "print(f'\\nNumber of cut measurements: {num_cut:d}\\nPercent of total measurements cut: {percent_cut:0.2f}%')\n",
    "f.write(f'\\n\\n## Averaging and cutting bad bins\\nNumber of cut measurements: {num_cut}\\nPercent of total measurements cut: {percent_cut:0.2f}%\\nHex value in \"Mask\" column: 0x800000')\n",
    "if percent_cut > 10:\n",
    "    print(f'WARNING: percent of total measurements cut is greater than 10%')\n",
    "limits = [xlim_lower, xlim_upper, ylim_lower, ylim_upper]\n",
    "plot_cut_lc(avglc, 'Averaged light curve', flags['avg_badday'], limits=limits, save_filename='averaged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Correct for ATLAS reference template changes\n",
    "\n",
    "This notebook takes into account ATLAS's periodic replacement of the difference image reference templates, which may cause step discontinuities in flux. Two template changes have been recorded at MJDs 58417 and 58882. More information can be found here: https://fallingstar-data.com/forcedphot/faq/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True for ATLAS template change correction\n",
    "template_correction = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-panel plot of (1) template regions in different colors and (2) zoom-in on transitions\n",
    "\n",
    "def plot_template_correction(lc, limits=None):\n",
    "    colors = ['salmon', 'sandybrown', 'darkseagreen']\n",
    "    \n",
    "    t1, t2 = 58417, 58882\n",
    "    region1_ix = lc.lcs[0].ix_inrange('MJD', uplim=t1)\n",
    "    region2_ix = lc.lcs[0].ix_inrange('MJD', lowlim=t1, uplim=t2)\n",
    "    region3_ix = lc.lcs[0].ix_inrange('MJD', lowlim=t2)\n",
    "\n",
    "    region1_mean = lc._get_mean(region1_ix[-40:]) # last 40 measurements before t1\n",
    "    region2a_mean = lc._get_mean(region2_ix[:40]) # first 40 measurements after t1\n",
    "    region2b_mean = lc._get_mean(region2_ix[-40:]) # last 40 measurements before t2\n",
    "    region3_mean = lc._get_mean(region3_ix[:40]) # first 40 measurements after t2\n",
    "\n",
    "    gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], hspace=0.35, wspace=0.4)\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(6)\n",
    "    fig.set_figheight(6)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    ax1 = plt.subplot(gs[0, :])\n",
    "    ax1.axvline(x=t1, color='k', linestyle='dotted', label='ATLAS template change', zorder=100)\n",
    "    ax1.axvline(x=t2, color='k', linestyle='dotted', zorder=100)\n",
    "    ax1.axhline(color='k',zorder=0)\n",
    "    limits = set_xylimits(lc, limits)\n",
    "    ax1.set_xlim(limits[0], limits[1])\n",
    "    ax1.set_ylim(limits[2], limits[3])\n",
    "\n",
    "    ax1.errorbar(lc.lcs[0].t.loc[region1_ix,'MJD'], lc.lcs[0].t.loc[region1_ix,'uJy'], yerr=lc.lcs[0].t.loc[region1_ix,lc.dflux_colnames[0]], fmt='none', ecolor=colors[0], elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5, zorder=10)\n",
    "    ax1.scatter(lc.lcs[0].t.loc[region1_ix,'MJD'], lc.lcs[0].t.loc[region1_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color=colors[0], marker='o', alpha=0.5, zorder=10, label='Region 1 flux')\n",
    "    ax1.errorbar(lc.lcs[0].t.loc[region2_ix,'MJD'], lc.lcs[0].t.loc[region2_ix,'uJy'], yerr=lc.lcs[0].t.loc[region2_ix,lc.dflux_colnames[0]], fmt='none', ecolor=colors[1], elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5, zorder=10)\n",
    "    ax1.scatter(lc.lcs[0].t.loc[region2_ix,'MJD'], lc.lcs[0].t.loc[region2_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color=colors[1], marker='o', alpha=0.5, zorder=10, label='Region 2 flux')\n",
    "    ax1.errorbar(lc.lcs[0].t.loc[region3_ix,'MJD'], lc.lcs[0].t.loc[region3_ix,'uJy'], yerr=lc.lcs[0].t.loc[region3_ix,lc.dflux_colnames[0]], fmt='none', ecolor=colors[2], elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5, zorder=10)\n",
    "    ax1.scatter(lc.lcs[0].t.loc[region3_ix,'MJD'], lc.lcs[0].t.loc[region3_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color=colors[2], marker='o', alpha=0.5, zorder=10, label='Region 2 flux')\n",
    "\n",
    "    ax1.legend(facecolor='white', framealpha=1, loc='upper left',  bbox_to_anchor=(1, 1))\n",
    "\n",
    "    ax2 = plt.subplot(gs[1, 0])\n",
    "    ax2.axvline(x=t1, color='k', linestyle='dotted', zorder=100)\n",
    "    ax2.axhline(color='k',zorder=0)\n",
    "    ax2.set_xlim(lc.lcs[0].t.loc[region1_ix[-40:][0], 'MJD'], lc.lcs[0].t.loc[region2_ix[:40][-1], 'MJD'])\n",
    "\n",
    "    ax2.errorbar(lc.lcs[0].t.loc[region1_ix,'MJD'], lc.lcs[0].t.loc[region1_ix,'uJy'], yerr=lc.lcs[0].t.loc[region1_ix,lc.dflux_colnames[0]], fmt='none', ecolor=colors[0], elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5, zorder=10)\n",
    "    ax2.scatter(lc.lcs[0].t.loc[region1_ix,'MJD'], lc.lcs[0].t.loc[region1_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color=colors[0], marker='o', alpha=0.5, zorder=10)\n",
    "    ax2.errorbar(lc.lcs[0].t.loc[region2_ix,'MJD'], lc.lcs[0].t.loc[region2_ix,'uJy'], yerr=lc.lcs[0].t.loc[region2_ix,lc.dflux_colnames[0]], fmt='none', ecolor=colors[1], elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5, zorder=10)\n",
    "    ax2.scatter(lc.lcs[0].t.loc[region2_ix,'MJD'], lc.lcs[0].t.loc[region2_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color=colors[1], marker='o', alpha=0.5, zorder=10)\n",
    "\n",
    "    ax2.axhline(y=region1_mean, color=colors[0], linestyle='dashed', label='Region 1 mean')\n",
    "    ax2.axhline(y=region2a_mean, color=colors[1], linestyle='dashed', label='Region 2 mean')\n",
    "\n",
    "    limits = set_xylimits(lc, limits, indices=AorB(region1_ix[-40:], region2_ix[:40]))\n",
    "    ax2.set_ylim(limits[2], limits[3])\n",
    "    ax2.legend(facecolor='white', framealpha=1)\n",
    "\n",
    "    ax3 = plt.subplot(gs[1, 1]) \n",
    "    ax3.axvline(x=t2, color='k', linestyle='dotted', zorder=100)\n",
    "    ax3.axhline(color='k',zorder=0)\n",
    "    ax3.set_xlim(lc.lcs[0].t.loc[region2_ix[-40:][0], 'MJD'], lc.lcs[0].t.loc[region3_ix[:40][-1], 'MJD'])\n",
    "    ax3.set_ylim(limits[2], limits[3])\n",
    "\n",
    "    ax3.errorbar(lc.lcs[0].t.loc[region2_ix,'MJD'], lc.lcs[0].t.loc[region2_ix,'uJy'], yerr=lc.lcs[0].t.loc[region2_ix,lc.dflux_colnames[0]], fmt='none', ecolor=colors[1], elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5, zorder=10)\n",
    "    ax3.scatter(lc.lcs[0].t.loc[region2_ix,'MJD'], lc.lcs[0].t.loc[region2_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color=colors[1], marker='o', alpha=0.5, zorder=10)\n",
    "    ax3.errorbar(lc.lcs[0].t.loc[region3_ix,'MJD'], lc.lcs[0].t.loc[region3_ix,'uJy'], yerr=lc.lcs[0].t.loc[region3_ix,lc.dflux_colnames[0]], fmt='none', ecolor=colors[2], elinewidth=1, capsize=1.2, c=sn_flux, alpha=0.5, zorder=10)\n",
    "    ax3.scatter(lc.lcs[0].t.loc[region3_ix,'MJD'], lc.lcs[0].t.loc[region3_ix,'uJy'], s=marker_size, lw=marker_edgewidth, color=colors[2], marker='o', alpha=0.5, zorder=10)\n",
    "\n",
    "    ax3.axhline(y=region2b_mean, color=colors[1], linestyle='dashed', label='Region 2 mean')\n",
    "    ax3.axhline(y=region3_mean, color=colors[2], linestyle='dashed', label='Region 3 mean')\n",
    "\n",
    "    limits = set_xylimits(lc, limits, indices=AorB(region2_ix[-40:], region3_ix[:40]))\n",
    "    ax3.set_ylim(limits[2], limits[3])\n",
    "    ax3.legend(facecolor='white', framealpha=1)\n",
    "\n",
    "    for ax in (ax1, ax2, ax3):\n",
    "        ax.minorticks_on()\n",
    "        ax.tick_params(direction='in', which='both')\n",
    "        ax.set_xlabel('MJD')\n",
    "        ax.set_ylabel('uJy')\n",
    "\n",
    "if template_correction:\n",
    "    plot_template_correction(lc, limits=[None]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally enter manual offsets for each region (set each to None for automatic correction)\n",
    "global_offset = None\n",
    "region1_offset = None\n",
    "region2_offset = None\n",
    "region3_offset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if template_correction:\n",
    "    output = lc.template_correction(maskval=0x1|0x2|0x400000|0x800000, \n",
    "                                    region1_offset=region1_offset, \n",
    "                                    region2_offset=region2_offset, \n",
    "                                    region3_offset=region3_offset)\n",
    "    f.write(f'\\n\\n## ATLAS template change correction\\n{output}')\n",
    "else:\n",
    "    print('Skipping template correction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if template_correction:\n",
    "    plot_template_correction(lc, limits=[None]*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: save the SN light curve with the new `'Mask'` and `'duJy_new'` columns\n",
    "\n",
    "Hex values in the `'Mask'` column for each cut's flag:\n",
    "- Uncertainty cut: 0x2\n",
    "- Chi-square cut: 0x1\n",
    "- Control light curve cut: 0x400000\n",
    "- Bad day (for averaged light curves): 0x800000\n",
    "\n",
    "You can combine these hex values together to create certain combinations of cuts that define a \"bad\" measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the SN and control light curves\n",
    "save_lc = False\n",
    "\n",
    "# save the SN and control averaged light curves\n",
    "save_avglc = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_lc:\n",
    "    print('Saving light curve with updated mask column...')\n",
    "    lc._save(source_dir, filt=filt, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_avglc:\n",
    "    print('Saving averaged light curve with updated mask column...')\n",
    "    avglc._save(source_dir, filt=filt, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary of all cuts and corrections\n",
    "\n",
    "f.close()\n",
    "f1 = open(f'{source_dir}/{tnsname}/{tnsname}_output.md')\n",
    "content = f1.read()\n",
    "print()\n",
    "print(content)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
