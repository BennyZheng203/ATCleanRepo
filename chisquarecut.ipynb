{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run preliminary imports, set directories, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, requests, json, time\n",
    "from pdastro import pdastrostatsclass, AandB, AnotB, AorB, not_AandB\n",
    "\n",
    "# for interactive sliders, etc.\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "# for getting discovery date from TNS\n",
    "from collections import OrderedDict\n",
    "from astropy.time import Time\n",
    "\n",
    "# plotting\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as matlib\n",
    "import warnings\n",
    "warnings.simplefilter('error', RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.figsize'] = [12, 7]\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Times'\n",
    "\n",
    "# ATLAS template changes\n",
    "global tchange1\n",
    "global tchange2\n",
    "tchange1 = 58417\n",
    "tchange2 = 58882\n",
    "\n",
    "#https://towardsdatascience.com/5-powerful-tricks-to-visualize-your-data-with-matplotlib-16bc33747e05\n",
    "\n",
    "def dataPlot(x, y, dx=None, dy=None, sp=None, label=None, fmt='bo', ecolor='k', elinewidth=None, barsabove = False, capsize=1, logx=False, logy=False):\n",
    "    if sp == None:\n",
    "        sp = matlib.subplot(111)\n",
    "    if dx is None and dy is None:\n",
    "        if logy:\n",
    "            if logx:\n",
    "                plot, = sp.loglog(x, y, fmt)\n",
    "            else:\n",
    "                plot, = sp.semilogy(x, y, fmt)\n",
    "        elif logx:\n",
    "            plot, = sp.semilogx(x, y, fmt)\n",
    "        else:\n",
    "            if barsabove:\n",
    "                plot, dplot,dummy = sp.errorbar(x, y, label=label, fmt=fmt, capsize=capsize, barsabove=barsabove)\n",
    "            else:\n",
    "                plot, = sp.plot(x, y, fmt)\n",
    "        return sp, plot, None\n",
    "    else:\n",
    "        if logy:\n",
    "            sp.set_yscale(\"log\", nonposx='clip')\n",
    "        if logx:\n",
    "            sp.set_xscale(\"log\", nonposx='clip')\n",
    "        plot, dplot, dummy = sp.errorbar(x, y, xerr=dx, yerr=dy, label=label, fmt=fmt, ecolor=ecolor, elinewidth=elinewidth, capsize=capsize, barsabove=barsabove)\n",
    "        return sp, plot, dplot\n",
    "\n",
    "lc_info = pdastrostatsclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your data directory (where the SN light curves are located) here:\n",
    "global dir\n",
    "dir = '/Users/sofiarest/Google Drive/My Drive/College/STScI Research Paper/atlaslc_chisquare/brightsne/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about template changes at MJDs 58417 and 58882: https://fallingstar-data.com/forcedphot/faq/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in light curves\n",
    "Enter the TNS name for each SN you wish to load in and examine/cut in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter each SN's TNS name here:\n",
    "#lc_info.t['tnsname'] = ['2019vxm']\n",
    "\n",
    "# Optionally, enter each SN's discovery date (start of SN) here:\n",
    "#lc_info.t['mjdstart'] = [59005.264,58818.04,58975.20256,58152.631331,58014.31,58153.49,None]\n",
    "\n",
    "lc_info.t['tnsname'] = ['2020lse','2019vxm','2020jfo','2017gjn','2017glq','2017gup','2017guu','2017guv','2017haf','2017gqr','2017guh','2017gvp','2017ghu','2017hgz','2017hjw','2017hjy','2017hoq','2017hpa','2017hou','2017igf','2017isq','2017gxq','2017isj','2017iyb','2017iji','2017jav','2017iyw','2017jyl','2018cqw','2020ejm','2017izu','2019syd','2017gjd','2018jaj','2018jov','2018jaz','2018jaz','2018imd','2018kfv','2017jd','2018pc','2020fcw','2018hkq','2019wdx','2018lqy','2018gfi','2018K','2020dkm','2018yh','2020afp','2019wrz','2018gv','2018iq','2018gl','2018kp','2018oh','2018pv','2018xx','2018yu','2018aaz','2018zz','2018ajp','2018aqh','2018aoz','2018aqi','2018azu','2018aye','2018cnj','2018cqj','2018cuw','2018dda','2018dzy','2018ebk','2018ddi','2018enc','2018epx','2018etm','2018feb','2018fop','2018fli','2018fnq','2018fhx']\n",
    "lc_info.t['mjdstart'] = [58955.26,58818.04,58925.20,57994.85992,57999.35754,58013.556,58009.58899,58011.305,58026.5,58005.1567,58014.31,58019.48,57991.4866,58036.02,58040.57399,58040.49699,58047.551,58051.34602,58050.37014,58075.59,58091.618,58013.453,58089.34568,58103.44699,58077.64,58106.24301,58105.3,58117.546,58287.2,58919.23791,58101.97699,58767.10699,57994.09,58447.48199,58460.583,58442.51023,58442.51023,58436.8626,58468.28699,57762.21399,58152.75,58936.437,58406.37,58824.38,58358.39,58373.4,58121.13589,58904.44051,58173.25,58866.66,58832.48,58133.68132,58137.49453,58131.56999,58142.36257,58153.49,58152.63133,58170.38199,58178.223,58183.56689,58180.92,58195.81,58213.44899,58210.30899,58214.07,58220.28699,58229.31,58266.61199,58282.268,58293.515,58303.96,58315.59399,58315.55199,58301.44,58332.32699,58335.605,58338.343,58346.16399,58351.56899,58343.15199,58361.08,58351.38]\n",
    "#lc_info.t['mjdstart'] = [None] * len(lc_info.t['tnsname'])\n",
    "lc_info.t['classification'] = [None] * len(lc_info.t['tnsname'])\n",
    "\n",
    "# Enter api_key if mjdstarts not entered in manually and accessing TNS for discovery dates\n",
    "global api_key\n",
    "api_key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the rest of lc_info (discovery date, loading in lc, etc.)\n",
    "\n",
    "def get_tns_data(tnsname):\n",
    "\ttry:\n",
    "\t\tget_obj = [(\"objname\",tnsname), (\"objid\",\"\"), (\"photometry\",\"1\"), (\"spectra\",\"1\")]\n",
    "\t\tget_url = 'https://www.wis-tns.org/api/get/object'\n",
    "\t\tjson_file = OrderedDict(get_obj)\n",
    "\t\tget_data = {'api_key':api_key,'data':json.dumps(json_file)}\n",
    "\t\tresponse = requests.post(get_url, data=get_data, headers={'User-Agent':'tns_marker{\"tns_id\":104739,\"type\": \"bot\", \"name\":\"Name and Redshift Retriever\"}'})\n",
    "\t\tjson_data = json.loads(response.text,object_pairs_hook=OrderedDict)\n",
    "\t\treturn json_data\n",
    "\texcept Exception as e:\n",
    "\t\treturn 'Error: \\n'+str(e)\n",
    "\n",
    "def get_disc_date(tnsname):\n",
    "\tjson_data = get_tns_data(tnsname)\n",
    "\tdiscoverydate = json_data['data']['reply']['discoverydate']\n",
    "\tdate = list(discoverydate.partition(' '))[0]\n",
    "\ttime = list(discoverydate.partition(' '))[2]\n",
    "\tdisc_date_format = date+'T'+time\n",
    "\tdateobjects = Time(disc_date_format, format='isot', scale='utc')\n",
    "\tdisc_date = dateobjects.mjd\n",
    "\treturn disc_date\n",
    "\n",
    "def get_disc_date_loop(lc_info):\n",
    "\tfor index in lc_info.ix_null(colnames=['mjdstart']):\n",
    "\t\tdisc_date = get_disc_date(lc_info.t.loc[index,'tnsname'])\n",
    "\t\tlc_info.t.loc[index,'mjdstart'] = disc_date\n",
    "\t\ttime.sleep(10)\n",
    "\treturn lc_info\n",
    "\n",
    "def correct4template(lc_info, index):\n",
    "\t#lc_info.t.loc[index,'lc'].getindices()\n",
    "\t#print(lc_info.t.loc[index,'lc'], lc_info.t.loc[index,'lc'].t)\n",
    "\tlc = lc_info.t.loc[index,'lc']\n",
    "\tbaseline = lc.ix_inrange(colnames=['MJD'],uplim=lc_info.t.loc[index,'mjdstart'],exclude_uplim=True) \n",
    "\tb_goodx2_i = lc.ix_inrange(colnames=['chi/N'],uplim=5,indices=baseline)\n",
    "\t#b_badx2_i = AnotB(baseline,b_goodx2_i)\n",
    "\n",
    "\tmedian_bflux = np.median(lc_info.t.loc[index,'lc'].t.loc[b_goodx2_i,'uJy'])\n",
    "\n",
    "\t# subtract median of baseline flux with x2 < 5 from all measurements to account for template shifts\n",
    "\tprint(f'Subtracting median of baseline flux with chi-square â‰¤ %d {median_bflux:0.1f} uJy from light curve flux due to potential flux in the template...' % 5)\n",
    "\tlc_info.t.loc[index,'lc'].t['uJy'] -= median_bflux\n",
    "\t\n",
    "\treturn lc_info\n",
    "\n",
    "def load_lc_loop(lc_info):\n",
    "\tfor index in range(0,len(lc_info.t)):\n",
    "\t\tlc_info.t.loc[index,'lc'] = pdastrostatsclass()\n",
    "\t\tfilename = dir+lc_info.t.loc[index,'tnsname']+'/'+lc_info.t.loc[index,'tnsname']+'_i000.o.lc.txt'\n",
    "\t\ttry:\n",
    "\t\t\tlc_info.t.loc[index,'lc'].load_spacesep(filename,delim_whitespace=True)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint('Could not load light curve for SN %s at %s: %s' % (lc_info.t.loc[index,'tnsname'], filename, str(e)))\n",
    "\t\t\tsys.exit()\n",
    "\n",
    "\t\tlc_info = correct4template(lc_info, index)\n",
    "\n",
    "\t\t# add flux/dflux column\n",
    "\t\tprint('Adding uJy/duJy column to light curve...')\n",
    "\t\tlc_info.t.loc[index,'lc'].t['uJy/duJy'] = lc_info.t.loc[index,'lc'].t['uJy']/lc_info.t.loc[index,'lc'].t['duJy']\n",
    "\t\tlc_info.t.loc[index,'lc'].t = lc_info.t.loc[index,'lc'].t.replace([np.inf, -np.inf], np.nan)\n",
    "\t\n",
    "\tprint('Success')\n",
    "\treturn lc_info\n",
    "\n",
    "print('Obtaining discovery date information from TNS if needed...')\n",
    "lc_info.t['lc'] = [None] * len(lc_info.t)\n",
    "lc_info = get_disc_date_loop(lc_info)\n",
    "lc_info.write(columns=['tnsname','mjdstart','classification'])\n",
    "print('\\nLoading in light curves...')\n",
    "lc_info = load_lc_loop(lc_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the scaling parameter for the plots' upper y limit (ylim_upper = scale * 95th percentile flux)\n",
    "global scale\n",
    "scale = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each light curve\n",
    "\n",
    "def get_xth_percentile_flux(info, percentile, indices):\n",
    "    if len(indices)==0: \n",
    "        return None\n",
    "    else:\n",
    "        return np.percentile(info['lc'].t.loc[indices, 'uJy'], percentile)\n",
    "\n",
    "def prelim_plot_lc(info, xlim_lower=None, xlim_upper=None, ylim_lower=None, ylim_upper=None):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "    ax.axhline(linewidth=1,color='k')\n",
    "    ax.set_ylabel('uJy')\n",
    "    ax.set_xlabel('MJD')\n",
    "    ax.set_title('SN '+info['tnsname']+' Light Curve')\n",
    "    ax.axvline(x=tchange1,color='magenta', label='ATLAS template change')\n",
    "    ax.axvline(x=tchange2,color='magenta')\n",
    "\n",
    "    if xlim_lower is None: xlim_lower = info['mjdstart']-100\n",
    "    if xlim_upper is None: xlim_upper = info['mjdstart']+400\n",
    "    ax.set_xlim(xlim_lower, xlim_upper)\n",
    "\n",
    "    if ylim_lower is None: ylim_lower = -200\n",
    "    if ylim_upper is None: ylim_upper = scale*get_xth_percentile_flux(info, 95, np.where(info['lc'].t['MJD'] > info['mjdstart'])[0])\n",
    "    ax.set_ylim(ylim_lower,ylim_upper)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "def plot_lc_mjdstart(info):\n",
    "    baseline = np.where(info['lc'].t['MJD'] < info['mjdstart'])[0]\n",
    "    duringsn = np.where(info['lc'].t['MJD'] > info['mjdstart'])[0]\n",
    "\n",
    "    fig, ax = prelim_plot_lc(info)\n",
    "\n",
    "    sp, plot_baseline, dplot = dataPlot(info['lc'].t.loc[baseline,'MJD'], info['lc'].t.loc[baseline,'uJy'], sp=ax)\n",
    "    matlib.setp(plot_baseline,ms=5,color='b',marker='o',label='Baseline flux')\n",
    "\n",
    "    sp, plot_aftersn, dplot = dataPlot(info['lc'].t.loc[duringsn,'MJD'], info['lc'].t.loc[duringsn,'uJy'], sp=ax)\n",
    "    matlib.setp(plot_aftersn,ms=5,color='c',marker='o',label='During and after SN')\n",
    "\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    return info\n",
    "\n",
    "for index in range(0,len(lc_info.t)):\n",
    "    plot_lc_mjdstart(lc_info.t.loc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove faint SNe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the limit for the standard deviation calculated during the 3-sigma cut\n",
    "# that will determine a SN's classification (\"good\" or \"bad\").\n",
    "global stdev_limit\n",
    "stdev_limit = 1.5\n",
    "\n",
    "# Enter the limit for the percent of measurements clipped during the 3-sigma cut \n",
    "# that will determine a SN's classification (\"good\" or \"bad\").\n",
    "global clippercent_limit\n",
    "clippercent_limit = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate rough brightness and confirm/change classification of each SN\n",
    "# estimate rough brightness of each SN by getting 90th percentile flux from mjdstart to mjdstart+60 and determine if SN is a good candidate\n",
    "# to confirm their classifications, take the baseline uJy/duJy and apply a 3-sigma cut. \"Bad\" SNe will have: sigma > 1.5, % data cut > 5%\n",
    "\n",
    "def remove_SN(lc_info,index):\n",
    "    print('Removing SN at index %d...' % index)\n",
    "    lc_info.t = lc_info.t.drop([index])\n",
    "    lc_info.t = lc_info.t.reset_index(drop=True)\n",
    "    return lc_info\n",
    "\n",
    "def get_xth_percentile_flux(lc_info, percentile, indices):\n",
    "    if len(indices)==0: \n",
    "        return None\n",
    "    else:\n",
    "        return np.percentile(lc_info['lc'].t.loc[indices, 'uJy'], percentile)\n",
    "\n",
    "def get_90th_percentile_SNflux_loop(lc_info):\n",
    "    index = 0\n",
    "    while index < len(lc_info.t):\n",
    "        indices = AandB(np.where(lc_info.t.loc[index,'lc'].t['MJD']>lc_info.t.loc[index,'mjdstart'])[0], np.where(lc_info.t.loc[index,'lc'].t['MJD']<lc_info.t.loc[index,'mjdstart']+60)[0])\n",
    "        flux = get_xth_percentile_flux(lc_info.t.loc[index], 90, indices)\n",
    "        if flux is None:\n",
    "            print('WARNING: For %s, 90th percentile flux not found' % (lc_info.t.loc[index,'tnsname']))\n",
    "            lc_info = remove_SN(lc_info,index)\n",
    "        elif(flux > 1000):\n",
    "            print('For %s, 90th percentile flux %0.2f is over 1000 ' % (lc_info.t.loc[index,'tnsname'], flux) + u'\\u2713')\n",
    "            index += 1\n",
    "        else:\n",
    "            print('WARNING: For %s, 90th percentile flux %0.2f is under 1000\\nRemoving SN from list? (y/n)' % (lc_info.t.loc[index,'tnsname'], flux))\n",
    "            #answer = input()\n",
    "            #if (answer == 'y'):\n",
    "            lc_info = remove_SN(lc_info,index)\n",
    "            #else:\n",
    "                #index += 1\n",
    "    return lc_info\n",
    "\n",
    "def sigmacut_lc(info):\n",
    "    baseline = np.where(info['lc'].t['MJD'] < info['mjdstart'])[0]\n",
    "    info['lc'].calcaverage_sigmacutloop('uJy/duJy',noisecol=None,indices=baseline,verbose=1,Nsigma=3.0,median_firstiteration=True)\n",
    "    print('stdev: ',info['lc'].statparams['stdev'],', % data clipped: ',100*info['lc'].statparams['Nclip']/len(info['lc'].t))\n",
    "    output = 'Classified as '\n",
    "    if info['lc'].statparams['stdev'] is None or info['lc'].statparams['Nclip'] is None:\n",
    "        classification = 'bad'\n",
    "    elif(info['lc'].statparams['stdev']<stdev_limit and (100*info['lc'].statparams['Nclip']/len(info['lc'].t))<clippercent_limit):\n",
    "        classification = 'good'\n",
    "    else: \n",
    "        classification = 'bad'\n",
    "    output += classification + ' SN; '\n",
    "    if(info['classification']==classification):\n",
    "        output += 'consistent with preliminary classification ' + u'\\u2713'\n",
    "    else:\n",
    "        output += 'WARNING: not consistent with preliminary classification (\\'%s\\')\\nReclassifying in table...' % info['classification']\n",
    "        info['classification'] = classification\n",
    "    print(output)\n",
    "    return info\n",
    "\n",
    "def sigmacut_lc_loop(lc_info):\n",
    "    for index in range(0, len(lc_info.t)):\n",
    "        print('Sigmacutting %s...' % lc_info.t.loc[index,'tnsname'])\n",
    "        lc_info.t.loc[index] = sigmacut_lc(lc_info.t.loc[index])\n",
    "    return lc_info\n",
    "\n",
    "lc_info = get_90th_percentile_SNflux_loop(lc_info)\n",
    "print('\\nRevised sample of SNe (only SNe with high flux):')\n",
    "lc_info.write(columns=['tnsname','mjdstart','classification'])\n",
    "\n",
    "print('\\nstdev limit: %0.1f, %% data clipped limit: %0.1f' % (stdev_limit, clippercent_limit))\n",
    "lc_info = sigmacut_lc_loop(lc_info)\n",
    "print('\\nRevised sample of bright SNe (all SNe reclassified as good or bad):')\n",
    "print(lc_info.t[['tnsname','mjdstart','classification']].to_string())\n",
    "#lc_info.write(columns=['tnsname','mjdstart','classification'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot S/N and Chi-Square Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stn_hist(info, stn_xlim=None, x2_goodstn_xlim=None, x2_badstn_xlim=None):\n",
    "    baseline = np.where(info['lc'].t['MJD'] < info['mjdstart'])[0]\n",
    "    good_stn_b = AandB(baseline,np.where(info['lc'].t['uJy/duJy'] < 3.0)[0])\n",
    "    bad_stn_b = AandB(baseline,np.where(info['lc'].t['uJy/duJy'] >= 3.0)[0])\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "    stn, x2_goodstn, x2_badstn = axes.flatten()\n",
    "    plt.suptitle('SN %s uJy/duJy and Chi-Square Distributions' % lc_info.t.loc[index,'tnsname'],fontsize=15, y=1)\n",
    "\n",
    "    stn.set_title('uJy/duJy Distribution')\n",
    "    stn.set_xlabel('uJy/duJy')\n",
    "    if not(stn_xlim is None) and len(baseline)>0: \n",
    "        if stn_xlim[0] is None: stn_xlim[0] = min(info['lc'].t.loc[baseline, 'uJy/duJy'])\n",
    "        if stn_xlim[1] is None: stn_xlim[1] = max(info['lc'].t.loc[baseline, 'uJy/duJy'])\n",
    "        stn.hist(info['lc'].t.loc[baseline, 'uJy/duJy'], bins=30, range=(stn_xlim[0],stn_xlim[1]))\n",
    "    else:\n",
    "        stn.hist(info['lc'].t.loc[baseline, 'uJy/duJy'], bins=30)\n",
    "\n",
    "    x2_goodstn.set_title('Chi-Square Distribution for Data with uJy/duJy<3')\n",
    "    x2_goodstn.set_xlabel('Chi-Square')\n",
    "    if not(x2_goodstn_xlim is None) and len(good_stn_b)>0: \n",
    "        if x2_goodstn_xlim[0] is None: x2_goodstn_xlim[0] = min(info['lc'].t.loc[good_stn_b, 'chi/N'])\n",
    "        if x2_goodstn_xlim[1] is None: x2_goodstn_xlim[1] = max(info['lc'].t.loc[good_stn_b, 'chi/N'])\n",
    "        x2_goodstn.hist(info['lc'].t.loc[good_stn_b, 'chi/N'], bins=30, range=(x2_goodstn_xlim[0], x2_goodstn_xlim[1]))\n",
    "    else:\n",
    "        x2_goodstn.hist(info['lc'].t.loc[good_stn_b, 'chi/N'], bins=30)\n",
    "\n",
    "    x2_badstn.set_title('Chi-Square Distribution for Data with uJy/duJy>=3')\n",
    "    x2_badstn.set_xlabel('Chi-Square')\n",
    "    if not(x2_badstn_xlim is None) and len(bad_stn_b)>0: \n",
    "        if x2_badstn_xlim[0] is None: x2_badstn_xlim[0] = min(info['lc'].t.loc[bad_stn_b, 'chi/N'])\n",
    "        if x2_badstn_xlim[1] is None: x2_badstn_xlim[1] = max(info['lc'].t.loc[bad_stn_b, 'chi/N'])\n",
    "        x2_badstn.hist(info['lc'].t.loc[bad_stn_b, 'chi/N'], bins=30, range=(x2_badstn_xlim[0], x2_badstn_xlim[1]))\n",
    "    else:\n",
    "        x2_badstn.hist(info['lc'].t.loc[bad_stn_b, 'chi/N'], bins=30)\n",
    "\n",
    "stn_xlims = [[None,300], None, None, None, None]\n",
    "x2_goodstn_xlims = [[None,20], None, None, None, None]\n",
    "x2_badstn_xlims = [[None,2500], [None,None], [None,None], [None,None], [None,None]]\n",
    "\n",
    "for index in range(0,len(lc_info.t)):\n",
    "    plot_stn_hist(lc_info.t.loc[index], stn_xlims[index], x2_goodstn_xlims[index]) #, x2_badstn_xlims[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stn_hist(info, stn_xlim=None, x2_xlim=None):\n",
    "    baseline = np.where(info['lc'].t['MJD'] < info['mjdstart'])[0]\n",
    "    good_stn_b = AandB(baseline,np.where(info['lc'].t['uJy/duJy'] < 3.0)[0])\n",
    "    bad_stn_b = AandB(baseline,np.where(info['lc'].t['uJy/duJy'] >= 3.0)[0])\n",
    "    good_x2_b = AandB(baseline,np.where(info['lc'].t['chi/N'] < 5.0)[0])\n",
    "    bad_x2_b = AandB(baseline,np.where(info['lc'].t['chi/N'] >= 5.0)[0])\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "    stn, x2 = axes.flatten()\n",
    "    plt.suptitle('SN %s uJy/duJy and Chi-Square Distributions' % lc_info.t.loc[index,'tnsname'],fontsize=15, y=1)\n",
    "\n",
    "    stn.set_title('uJy/duJy Distribution')\n",
    "    stn.set_xlabel('uJy/duJy')\n",
    "    red = mpatches.Patch(color='green', label='Data with Chi-Square<5')\n",
    "    green = mpatches.Patch(color='red', label='Data with Chi-Square>=5')\n",
    "    stn.legend(handles=[red, green])\n",
    "    if not(stn_xlim is None) and len(baseline)>0: \n",
    "        if stn_xlim[0] is None: stn_xlim[0] = min(info['lc'].t.loc[baseline, 'uJy/duJy'])\n",
    "        if stn_xlim[1] is None: stn_xlim[1] = max(info['lc'].t.loc[baseline, 'uJy/duJy'])\n",
    "        stn.hist(info['lc'].t.loc[good_x2_b, 'uJy/duJy'], bins=30, color='green', alpha=0.5, range=(stn_xlim[0],stn_xlim[1]), density=True)\n",
    "        stn.hist(info['lc'].t.loc[bad_x2_b, 'uJy/duJy'], bins=30, color='red', alpha=0.5, range=(stn_xlim[0],stn_xlim[1]), density=True)\n",
    "    else:\n",
    "        stn.hist(info['lc'].t.loc[good_x2_b, 'uJy/duJy'], bins=30, color='green', alpha=0.5, density=True)\n",
    "        stn.hist(info['lc'].t.loc[bad_x2_b, 'uJy/duJy'], bins=30, color='red', alpha=0.5, density=True)\n",
    "\n",
    "    x2.set_title('Chi-Square Distribution')\n",
    "    x2.set_xlabel('Chi-Square')\n",
    "    red = mpatches.Patch(color='green', label='Data with uJy/duJy<3')\n",
    "    green = mpatches.Patch(color='red', label='Data with uJy/duJy>=3')\n",
    "    x2.legend(handles=[red, green])\n",
    "    if not(x2_xlim is None) and len(baseline)>0:\n",
    "        if x2_xlim[0] is None: x2_xlim[0] = min(info['lc'].t.loc[baseline, 'chi/N'])\n",
    "        if x2_xlim[1] is None: x2_xlim[1] = max(info['lc'].t.loc[baseline, 'chi/N'])\n",
    "        x2.hist(info['lc'].t.loc[good_stn_b, 'chi/N'], bins=30, color='green', alpha=0.5, range=(x2_xlim[0],x2_xlim[1]), density=True)\n",
    "        x2.hist(info['lc'].t.loc[bad_stn_b, 'chi/N'], bins=30, color='red', alpha=0.5, range=(x2_xlim[0],x2_xlim[1]), density=True)\n",
    "    else:\n",
    "        x2.hist(info['lc'].t.loc[good_stn_b, 'chi/N'], bins=30, color='green', alpha=0.5, density=True)\n",
    "        x2.hist(info['lc'].t.loc[bad_stn_b, 'chi/N'], bins=30, color='red', alpha=0.5, density=True)\n",
    "\n",
    "stn_xlims = [[None,300], None, None, None, None]\n",
    "x2_xlims = [[None,3000], None, [-1,10], [0,11], None]\n",
    "\n",
    "for index in range(0,len(lc_info.t)):\n",
    "    plot_stn_hist(lc_info.t.loc[index], stn_xlims[index], x2_xlims[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine each region defined by the ATLAS template changes and its chi-squares\n",
    "There are two ATLAS template changes, one at MJD = 58417 and one at MJD = 58882."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global hockeystick_cut\n",
    "global hockeystick_cut_pcutoff\n",
    "global extendedtail_cut\n",
    "global extendedtail_cut_pcutoff\n",
    "# if % data with x2 greater than hockeystick_cut is >hockeystick_cut_pcutoff%, classify as hockeystick.\n",
    "hockeystick_cut = 100\n",
    "hockeystick_cut_pcutoff = 1.0\n",
    "# if % data with x2 greater than extendedtail_cut and smaller than hockeystick_cut is >extendedtail_cut_pcutoff%, classify as extended tail.\n",
    "extendedtail_cut = 5\n",
    "extendedtail_cut_pcutoff = 1.0\n",
    "# else classify as well-behaved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide light curve into 6 MJD regions: baseline (t0, t1, t2), SN (t0, t1, t2).\n",
    "# for each MJD region, calculate the number of detections. If n < 50, disregard. If n > 50, continue.\n",
    "# hockeystick: % data with x2>100\n",
    "# extended tail: % data with 5<x2<100\n",
    "# well-behaved: % data with x2<5\n",
    "\n",
    "def get_region_table_plot_colors(regionstates):\n",
    "    colors = []\n",
    "    for state in regionstates.loc[[1,2,3,5,6,7],'state']:\n",
    "        if state == 'hockeystick':\n",
    "            colors.append('red')\n",
    "        elif state == 'extended tail':\n",
    "            colors.append('yellow')\n",
    "        else:\n",
    "            colors.append('green')\n",
    "    return colors \n",
    "\n",
    "def get_regionstates_allsne_table_colors(val):\n",
    "    if val == 'hockeystick':\n",
    "        color = 'red'\n",
    "    elif val == 'extended tail':\n",
    "        color = 'yellow'\n",
    "    elif val == 'well-behaved':\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'white'\n",
    "    return 'background-color: %s' % color\n",
    "\n",
    "def get_ranges(info):\n",
    "    baseline = np.where(info['lc'].t['MJD'] < info['mjdstart'])[0]\n",
    "    aftersnstart = AnotB(info['lc'].getindices(),baseline)\n",
    "    ranges = {}\n",
    "    ranges[0] = baseline\n",
    "    ranges[1] = AandB(baseline, np.where(info['lc'].t['MJD'] < tchange1)[0]) # b_t0\n",
    "    ranges[2] = AandB(baseline, AandB(np.where(info['lc'].t['MJD'] > tchange1)[0], np.where(info['lc'].t['MJD'] < tchange2)[0])) # b_t1\n",
    "    ranges[3] = AandB(baseline, np.where(info['lc'].t['MJD'] > tchange2)[0]) # b_t2\n",
    "    ranges[4] = aftersnstart\n",
    "    ranges[5] = AandB(aftersnstart, np.where(info['lc'].t['MJD'] < tchange1)[0]) # sn_t0\n",
    "    ranges[6] = AandB(aftersnstart, AandB(np.where(info['lc'].t['MJD'] > tchange1)[0], np.where(info['lc'].t['MJD'] < tchange2)[0])) # sn_t1\n",
    "    ranges[7] = AandB(aftersnstart, np.where(info['lc'].t['MJD'] > tchange2)[0]) # sn_t2\n",
    "    return ranges\n",
    "\n",
    "def get_region_table(info):\n",
    "    regionstates = pdastrostatsclass(columns=['region','n','start_index','end_index','mjd_start','Phockeystick','Pextendedtail','Pwellbehaved','state'])\n",
    "    regionstates.t['region'] = ['b','b_t0','b_t1','b_t2','sn','sn_t0','sn_t1','sn_t2']\n",
    "    ranges = get_ranges(info)\n",
    "\n",
    "    for index in range(0,len(ranges)):\n",
    "        regionstates.t.loc[index,'n'] = len(ranges[index])\n",
    "\n",
    "        if len(ranges[index] > 0): \n",
    "            regionstates.t.loc[index,'mjd_start'] = info['lc'].t.loc[ranges[index][0],'MJD']\n",
    "            regionstates.t.loc[index,'start_index'] = ranges[index][0]\n",
    "            regionstates.t.loc[index,'end_index'] = ranges[index][-1]\n",
    "        else:\n",
    "            regionstates.t.loc[index,'mjd_start'] = np.nan\n",
    "            regionstates.t.loc[index,'start_index'] = np.nan\n",
    "            regionstates.t.loc[index,'end_index'] = np.nan\n",
    "        if len(ranges[index]) < 50:\n",
    "            regionstates.t.loc[index,'Phockeystick'] = np.nan\n",
    "            regionstates.t.loc[index,'Pextendedtail'] = np.nan\n",
    "            regionstates.t.loc[index,'Pwellbehaved'] = np.nan\n",
    "            regionstates.t.loc[index,'state'] = np.nan\n",
    "            continue\n",
    "\n",
    "        Phockeystick_i = AandB(ranges[index], np.where(info['lc'].t['chi/N']>=hockeystick_cut)[0])\n",
    "        Pextendedtail_i = AandB(ranges[index], AandB(np.where(info['lc'].t['chi/N']>=extendedtail_cut)[0], np.where(info['lc'].t['chi/N']<hockeystick_cut)[0]))\n",
    "        Pwellbehaved_i = AandB(ranges[index], np.where(info['lc'].t['chi/N']<extendedtail_cut)[0])\n",
    "\n",
    "        regionstates.t.loc[index,'Phockeystick'] = 100.0*len(Phockeystick_i)/len(ranges[index])\n",
    "        regionstates.t.loc[index,'Pextendedtail'] = 100.0*len(Pextendedtail_i)/len(ranges[index])\n",
    "        regionstates.t.loc[index,'Pwellbehaved'] = 100.0*len(Pwellbehaved_i)/len(ranges[index])\n",
    "        \n",
    "        if(regionstates.t.loc[index,'Phockeystick'] > hockeystick_cut_pcutoff):\n",
    "            regionstates.t.loc[index,'state'] = 'hockeystick'\n",
    "        elif(regionstates.t.loc[index,'Pextendedtail'] > extendedtail_cut_pcutoff):\n",
    "            regionstates.t.loc[index,'state'] = 'extended tail'\n",
    "        else:\n",
    "            regionstates.t.loc[index,'state'] = 'well-behaved'\n",
    "\n",
    "    return regionstates\n",
    "\n",
    "def get_region_table_loop(lc_info):\n",
    "    regionstates_allsne = pd.DataFrame(columns=['tnsname','classification','b_t0','b_t1','b_t2','sn_t0','sn_t1','sn_t2'])\n",
    "    \n",
    "    # prelim plots of distribution for percent cut\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "    et, hs, eths = axes.flatten()\n",
    "    plt.suptitle('Chi-Square Distribution for All SNe',fontsize=15, y=1)\n",
    "    et.set_title('Distribution for Extended Tail')\n",
    "    et.set_ylabel('Percent Data with %d<x2<%d' % (extendedtail_cut, hockeystick_cut))\n",
    "    et.set_xlabel('Percent Data with x2<%d' % extendedtail_cut)\n",
    "    hs.set_title('Distribution for Hockeystick')\n",
    "    hs.set_ylabel('Percent Data with x2>%d' % hockeystick_cut)\n",
    "    hs.set_xlabel('Percent Data with x2<%d' % extendedtail_cut)\n",
    "    eths.set_title('Distribution for Extended Tail and Hockeystick')\n",
    "    eths.set_xlabel('Percent Data with %d<x2<%d' % (extendedtail_cut, hockeystick_cut))\n",
    "    eths.set_ylabel('Percent Data with x2>%d' % hockeystick_cut)\n",
    "\n",
    "    hockeystick_counter = 0\n",
    "    extendedtail_counter = 0\n",
    "    wellbehaved_counter = 0\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        # get region table\n",
    "        regionstates = get_region_table(lc_info.t.loc[index])\n",
    "        print('SN %s MJD region states w.r.t. PSF chi-square values:' % (lc_info.t.loc[index,'tnsname'])) \n",
    "        print(regionstates.t.to_string(),'\\n')\n",
    "\n",
    "        ix = [1,2,3,5,6,7] # don't count repeat b and sn regions\n",
    "\n",
    "        hockeystick_counter += len(np.where(regionstates.t.loc[ix,'state']=='hockeystick')[0])\n",
    "        extendedtail_counter += len(np.where(regionstates.t.loc[ix,'state']=='extended tail')[0])\n",
    "        wellbehaved_counter += len(np.where(regionstates.t.loc[ix,'state']=='well-behaved')[0])\n",
    "\n",
    "        # plot\n",
    "        colors = get_region_table_plot_colors(regionstates.t)\n",
    "        et.scatter(regionstates.t.loc[ix,'Pwellbehaved'], regionstates.t.loc[ix,'Pextendedtail'], c=colors)\n",
    "        hs.scatter(regionstates.t.loc[ix,'Pwellbehaved'], regionstates.t.loc[ix,'Phockeystick'], c=colors)\n",
    "        eths.scatter(regionstates.t.loc[ix,'Pextendedtail'], regionstates.t.loc[ix,'Phockeystick'], c=colors)\n",
    "\n",
    "        # classify each region\n",
    "        regionstates_allsne.loc[index,'tnsname'] = lc_info.t.loc[index,'tnsname']\n",
    "        regionstates_allsne.loc[index,'classification'] = lc_info.t.loc[index,'classification']\n",
    "        for i in range(0,len(regionstates.t['region'])):\n",
    "            regionstates_allsne.loc[index,regionstates.t.loc[i,'region']] = regionstates.t.loc[i,'state']\n",
    "\n",
    "        # update lc_info\n",
    "        et_colname = 'Pextendedtail_'\n",
    "        hs_colname = 'Phockeystick_'\n",
    "        for i in range(0,4):\n",
    "            lc_info.t.loc[index, et_colname+regionstates.t.loc[i,'region']] = regionstates.t.loc[i,'Pextendedtail']\n",
    "            lc_info.t.loc[index, hs_colname+regionstates.t.loc[i,'region']] = regionstates.t.loc[i,'Phockeystick']\n",
    "    \n",
    "    print('Hockeystick ranges: %d\\nExtended tail ranges: %d\\nWell-behaved ranges: %d\\n' % (hockeystick_counter, extendedtail_counter, wellbehaved_counter))\n",
    "    \n",
    "    # legends for plots of distribution for percent cut\n",
    "    red = mpatches.Patch(color='red', label='Hockeystick')\n",
    "    yellow = mpatches.Patch(color='yellow', label='Extended tail')\n",
    "    green = mpatches.Patch(color='green', label='Well-behaved')\n",
    "    et.legend(handles=[red, yellow, green])\n",
    "    hs.legend(handles=[red, yellow, green])\n",
    "    eths.legend(handles=[red, yellow, green])\n",
    "\n",
    "    return regionstates_allsne, lc_info\n",
    "\n",
    "regionstates_allsne = pdastrostatsclass(columns=['tnsname','classification','b_t0','b_t1','b_t2','sn_t0','sn_t1','sn_t2'])\n",
    "\n",
    "print('Template changes at %d and %d MJD' % (tchange1,tchange2))\n",
    "print('PSF chi-square cut for hockeystick classification: %d; PSF chi-square cut for extended tail classification: %d' % (hockeystick_cut,extendedtail_cut))\n",
    "print('PSF chi-square hockeystick cut percent cutoff: %0.1f%%; PSF chi-square extended tail cut percent cutoff: %0.1f%%\\n' % (hockeystick_cut_pcutoff, extendedtail_cut_pcutoff))\n",
    "regionstates_allsne.t, lc_info = get_region_table_loop(lc_info)\n",
    "print(lc_info.t)\n",
    "regionstates_allsne.t.style.applymap(get_regionstates_allsne_table_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine each region defined by the ATLAS template changes and its chi-squares in relation to S/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each well-behaved baseline region of all SNe, 3 histograms.\n",
    "# histogram 1: S/N of all measurements with x2>hockeystick_cut.\n",
    "# histogram 2: S/N of all measurements with hockeystick_cut>x2>extendedtail_cut.\n",
    "# histogram 3: S/N of all measurements with extendedtail_cut>x2.\n",
    "# repeat for each extended tail baseline region and for each hockeystick baseline region.\n",
    "\n",
    "def get_state_expected_n(state):\n",
    "    counter = 0\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        regionstates = get_region_table(lc_info.t.loc[index])\n",
    "        for b_region_i in range(0,3):\n",
    "            if regionstates.t.loc[b_region_i,'state'] == state:\n",
    "                counter += regionstates.t.loc[b_region_i,'n']\n",
    "    return counter\n",
    "\n",
    "def get_state_x2_expected_n(state, x2_lowlim=None, x2_uplim=None):\n",
    "    counter = 0\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        regionstates = get_region_table(lc_info.t.loc[index])\n",
    "        for b_region_i in range(0,3):\n",
    "            if regionstates.t.loc[b_region_i,'state'] == state:\n",
    "                s = regionstates.t.loc[b_region_i,'start_index']\n",
    "                e = regionstates.t.loc[b_region_i,'end_index']\n",
    "                if not(x2_lowlim is None) and not(x2_uplim is None):\n",
    "                    counter += len(AandB(np.where(lc_info.t.loc[index,'lc'].t.loc[range(s,e+1),'chi/N']>= x2_lowlim)[0], np.where(lc_info.t.loc[index,'lc'].t.loc[range(s,e+1),'chi/N']<= x2_uplim)[0]))\n",
    "                elif not(x2_lowlim is None):\n",
    "                    counter += len(np.where(lc_info.t.loc[index,'lc'].t.loc[range(s,e+1),'chi/N']>= x2_lowlim)[0])\n",
    "                elif not(x2_uplim is None):\n",
    "                    counter += len(np.where(lc_info.t.loc[index,'lc'].t.loc[range(s,e+1),'chi/N']<= x2_uplim)[0])\n",
    "    return counter\n",
    "\n",
    "def get_state_baseline_allsne(state):\n",
    "    state_baseline_allsne = pdastrostatsclass()\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        #print('SN ', lc_info.t.loc[index,'tnsname'])\n",
    "        regionstates = get_region_table(lc_info.t.loc[index])\n",
    "        #print(regionstates.t.to_string())\n",
    "        # for each baseline region, check if state matches and if so add to state_baseline_allsne table\n",
    "        for b_region_i in range(0,3):\n",
    "            #print(b_region_i,regionstates.t.loc[b_region_i,'state'])\n",
    "            if regionstates.t.loc[b_region_i,'state'] == state:\n",
    "                #print('passed, adding: ', regionstates.t.loc[b_region_i,'start_index'], regionstates.t.loc[b_region_i,'end_index'])\n",
    "                # add measurements within that region to state_baseline_allsne table\n",
    "                state_baseline_allsne.t = pd.concat([state_baseline_allsne.t, \n",
    "                                                    lc_info.t.loc[index,'lc'].t.loc[range(regionstates.t.loc[b_region_i,'start_index'], 1+regionstates.t.loc[b_region_i,'end_index'])]], \n",
    "                                                    ignore_index=True)\n",
    "    state_baseline_allsne.t['uJy/duJy'] = state_baseline_allsne.t['uJy']/state_baseline_allsne.t['duJy']\n",
    "    state_baseline_allsne.t = state_baseline_allsne.t.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    return state_baseline_allsne\n",
    "\n",
    "def plot_stn_hist(state, state_baseline_allsne, hist, x2_lowlim=None, x2_uplim=None):\n",
    "    indices = state_baseline_allsne.ix_inrange(colnames=['chi/N'], lowlim=x2_lowlim, uplim=x2_uplim)\n",
    "    print('Expected %d indices and found %d indices for chi-square range ' % (get_state_x2_expected_n(state,x2_lowlim,x2_uplim), len(indices)), x2_lowlim, '-', x2_uplim)\n",
    "\n",
    "    #print(state_baseline_allsne.t.loc[indices, 'uJy/duJy'].to_string())\n",
    "    hist.hist(state_baseline_allsne.t.loc[indices, 'uJy/duJy'], bins=30) #, range=(0,20))\n",
    "    \n",
    "    title = 'For data with '\n",
    "    if not(x2_uplim is None): title += '%0.1f>' % x2_uplim\n",
    "    title += 'x2'\n",
    "    if not(x2_lowlim is None): title += '>%0.1f' % x2_lowlim\n",
    "    hist.set_title(title)\n",
    "    hist.set_xlabel('uJy/duJy')\n",
    "\n",
    "def plot_region_stn_hist(state):\n",
    "    print('\\nPlotting S/N Histograms for %s MJD Regions' % state.title())\n",
    "    state_baseline_allsne = get_state_baseline_allsne(state)\n",
    "    #print(state_baseline_allsne.t)\n",
    "    print('Expected indices for this state: %d' % get_state_expected_n(state))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "    plt.suptitle('S/N Frequencies for Chi-Square Ranges in %s MJD Regions' % state.title(), fontsize=15, y=1)\n",
    "    hs, et, wb = axes.flatten()\n",
    "\n",
    "    plot_stn_hist(state, state_baseline_allsne, hs, x2_lowlim=hockeystick_cut)\n",
    "    plot_stn_hist(state, state_baseline_allsne, et, x2_lowlim=extendedtail_cut, x2_uplim=hockeystick_cut)\n",
    "    plot_stn_hist(state, state_baseline_allsne, wb, x2_uplim=extendedtail_cut)\n",
    "\n",
    "\n",
    "plot_region_stn_hist('well-behaved') \n",
    "plot_region_stn_hist('extended tail')\n",
    "plot_region_stn_hist('hockeystick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recalculate cut limits for each SN based on contamination and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the contamination limit (contamination = Nbad,kept/Nkept must be <= contam_lim%):\n",
    "global contam_lim\n",
    "contam_lim = 15.0\n",
    "\n",
    "# Enter the loss limit (loss = Ngood,cut/Ngood must be >= loss_lim%):\n",
    "global loss_lim\n",
    "loss_lim = 10.0\n",
    "\n",
    "# Enter the limit to prioritize (must be 'loss_lim' or 'contam_lim', based on which is more important):\n",
    "global lim_to_prioritize\n",
    "lim_to_prioritize = 'loss_lim'\n",
    "\n",
    "# Enter the parameters for the chi-square cut (minimum cut, maximum cut, and step):\n",
    "global cut_start\n",
    "global cut_stop\n",
    "global cut_step\n",
    "cut_start = 3 # this is inclusive\n",
    "cut_stop = 50 # this is inclusive\n",
    "cut_step = 1\n",
    "\n",
    "# Enter the signal-to-noise cut that will determine a \"good\" measurement from\n",
    "# a \"bad\" measurement:\n",
    "global stn_cut\n",
    "stn_cut = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. static cut at x2=50; get percent cut before and after mjdstart\n",
    "# 2. for different x2 cuts decreasing from cut_stop and cut at stn_cut\n",
    "#    and for only baseline calculate contam% and loss%\n",
    "\n",
    "def plot_lim_cuts(info, lim_cuts, contam_lim_cut, loss_lim_cut, add2title=None):\n",
    "    fig = plt.figure()\n",
    "    plt.title('SN %s Dynamic PSF Chi-Square Cut (Baseline Only)' % (info['tnsname']))\n",
    "\n",
    "    plt.axhline(linewidth=1,color='k')\n",
    "    plt.xlabel('PSF Chi-Square Cut')\n",
    "    plt.ylabel('% of Baseline Measurements')\n",
    "\n",
    "    plt.axhline(loss_lim,linewidth=1,color='r',linestyle='--',label='Loss Limit')\n",
    "    plt.plot(lim_cuts.t['PSF Chi-Square Cut'], lim_cuts.t['Loss'],ms=5,color='r',marker='o',label='Loss')\n",
    "    plt.axvline(x=loss_lim_cut,color='r',label='Loss Cut')\n",
    "    plt.axvspan(loss_lim_cut, cut_stop, alpha=0.2, color='r')\n",
    "\n",
    "    plt.axhline(contam_lim,linewidth=1,color='g',linestyle='--',label='Contamination Limit')\n",
    "    plt.plot(lim_cuts.t['PSF Chi-Square Cut'], lim_cuts.t['Contamination'],ms=5,color='g',marker='o',label='Contamination')\n",
    "    plt.axvline(x=contam_lim_cut,color='g',label='Contamination Cut')\n",
    "    plt.axvspan(cut_start, contam_lim_cut, alpha=0.2, color='g')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    fig.tight_layout()\n",
    "\n",
    "def get_lim_cuts_data(lc_info, index, cut, colname):\n",
    "    indices = lc_info.t.loc[index,'lc'].ix_inrange(colnames=['MJD'],uplim=lc_info.t.loc[index,'mjdstart'],exclude_uplim=True)\n",
    "\n",
    "    b_good_i = lc_info.t.loc[index,'lc'].ix_inrange(colnames=['uJy/duJy'],lowlim=-stn_cut,uplim=stn_cut,indices=indices)\n",
    "    b_bad_i = AnotB(indices, b_good_i)\n",
    "    b_kept_i = lc_info.t.loc[index,'lc'].ix_inrange(colnames=['chi/N'],uplim=cut,indices=indices)\n",
    "    b_cut_i = AnotB(indices, b_kept_i)\n",
    "\n",
    "    lc_info.t.loc[index,'%s_Ngood' % colname] = len(b_good_i)\n",
    "    lc_info.t.loc[index,'%s_Nbad' % colname] = len(b_bad_i)\n",
    "    lc_info.t.loc[index,'%s_Nkept' % colname] = len(b_kept_i)\n",
    "    lc_info.t.loc[index,'%s_Ncut' % colname] = len(b_cut_i)\n",
    "    lc_info.t.loc[index,'%s_Ngood,kept' % colname] = len(AandB(b_good_i,b_kept_i))\n",
    "    lc_info.t.loc[index,'%s_Ngood,cut' % colname] = len(AandB(b_good_i,b_cut_i))\n",
    "    lc_info.t.loc[index,'%s_Nbad,kept' % colname] = len(AandB(b_bad_i,b_kept_i))\n",
    "    lc_info.t.loc[index,'%s_Nbad,cut' % colname] = len(AandB(b_bad_i,b_cut_i))\n",
    "    lc_info.t.loc[index,'%s_Pgood,kept' % colname] = 100*len(AandB(b_good_i,b_kept_i))/len(indices)\n",
    "    lc_info.t.loc[index,'%s_Pgood,cut' % colname] = 100*len(AandB(b_good_i,b_cut_i))/len(indices)\n",
    "    lc_info.t.loc[index,'%s_Pbad,kept' % colname] = 100*len(AandB(b_bad_i,b_kept_i))/len(indices)\n",
    "    lc_info.t.loc[index,'%s_Pbad,cut' % colname] = 100*len(AandB(b_bad_i,b_cut_i))/len(indices)\n",
    "    lc_info.t.loc[index,'%s_Ngood,kept/Ngood' % colname] = 100*len(AandB(b_good_i,b_kept_i))/len(b_good_i)\n",
    "    lc_info.t.loc[index,'%s_Ploss' % colname] = 100*len(AandB(b_good_i,b_cut_i))/len(b_good_i)\n",
    "    lc_info.t.loc[index,'%s_Pcontamination' % colname] = 100*len(AandB(b_bad_i,b_kept_i))/len(b_kept_i)\n",
    "\n",
    "    return lc_info\n",
    "\n",
    "def get_lim_cuts(lc_info, index, lim_cuts):\n",
    "    contam_lim_cut = None\n",
    "    loss_lim_cut = None\n",
    "    contam_case = None\n",
    "    loss_case = None\n",
    "\n",
    "    sortby_loss = lim_cuts.t.iloc[(lim_cuts.t['Ploss']).argsort()].reset_index()\n",
    "    min_loss = sortby_loss.loc[0,'Ploss']\n",
    "    max_loss = sortby_loss.loc[len(sortby_loss)-1,'Ploss']\n",
    "    # if all loss below lim, loss_lim_cut is min cut\n",
    "    if min_loss < loss_lim and max_loss < loss_lim:\n",
    "        loss_case = 'below lim'\n",
    "        loss_lim_cut = lim_cuts.t.loc[0,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all loss above lim, loss_lim_cut is min cut with min% loss\n",
    "        if min_loss > loss_lim and max_loss > loss_lim:\n",
    "            loss_case = 'above lim'\n",
    "            a = np.where(lim_cuts.t['Ploss'] == min_loss)[0]\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            loss_lim_cut = c.loc[0,'PSF Chi-Square Cut']\n",
    "        # else if loss crosses lim at some point, loss_lim_cut is min cut with max% loss <= loss_lim\n",
    "        else:\n",
    "            loss_case = 'crosses lim'\n",
    "            valid_cuts = sortby_loss[sortby_loss['Ploss'] <= loss_lim]\n",
    "            a = np.where(lim_cuts.t['Ploss'] == valid_cuts.loc[len(valid_cuts)-1,'Ploss'])[0]\n",
    "            # sort by cuts\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            # get midpoint of loss1 and loss2 (two points on either side of lim)\n",
    "            loss1_i = np.where(lim_cuts.t['PSF Chi-Square Cut'] == c.loc[0,'PSF Chi-Square Cut'])[0][0]\n",
    "            if lim_cuts.t.loc[loss1_i,'Ploss'] == loss_lim:\n",
    "                loss_lim_cut = lim_cuts.t.loc[loss1_i,'PSF Chi-Square Cut']\n",
    "            else:\n",
    "                loss2_i = loss1_i - 1\n",
    "                x = np.array([lim_cuts.t.loc[loss1_i,'PSF Chi-Square Cut'], lim_cuts.t.loc[loss2_i,'PSF Chi-Square Cut']])\n",
    "                contam_y = np.array([lim_cuts.t.loc[loss1_i,'Pcontamination'], lim_cuts.t.loc[loss2_i,'Pcontamination']])\n",
    "                loss_y = np.array([lim_cuts.t.loc[loss1_i,'Ploss'], lim_cuts.t.loc[loss2_i,'Ploss']])\n",
    "                contam_line = np.polyfit(x,contam_y,1)\n",
    "                loss_line = np.polyfit(x,loss_y,1)\n",
    "                loss_lim_cut = (loss_lim-loss_line[1])/loss_line[0]\n",
    "\n",
    "    sortby_contam = lim_cuts.t.iloc[(lim_cuts.t['Pcontamination']).argsort()].reset_index()\n",
    "    min_contam = sortby_contam.loc[0,'Pcontamination']\n",
    "    max_contam = sortby_contam.loc[len(sortby_contam)-1,'Pcontamination']\n",
    "    # if all contam below lim, contam_lim_cut is max cut\n",
    "    if min_contam < contam_lim and max_contam < contam_lim:\n",
    "        contam_case = 'below lim'\n",
    "        contam_lim_cut = lim_cuts.t.loc[len(lim_cuts.t)-1,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all contam above lim, contam_lim_cut is max cut with min% contam\n",
    "        if min_contam > contam_lim and max_contam > contam_lim:\n",
    "            contam_case = 'above lim'\n",
    "            a = np.where(lim_cuts.t['Pcontamination'] == min_contam)[0]\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            contam_lim_cut = c.loc[len(c)-1,'PSF Chi-Square Cut']\n",
    "        # else if contam crosses lim at some point, contam_lim_cut is max cut with max% contam <= contam_lim\n",
    "        else:\n",
    "            contam_case = 'crosses lim'\n",
    "            valid_cuts = sortby_contam[sortby_contam['Pcontamination'] <= contam_lim]\n",
    "            a = np.where(lim_cuts.t['Pcontamination'] == valid_cuts.loc[len(valid_cuts)-1,'Pcontamination'])[0]\n",
    "            # sort by cuts\n",
    "            b = lim_cuts.t.iloc[a]\n",
    "            c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "            # get midpoint of contam1 and contam2 (two points on either side of lim)\n",
    "            contam1_i = np.where(lim_cuts.t['PSF Chi-Square Cut'] == c.loc[len(c)-1,'PSF Chi-Square Cut'])[0][0]\n",
    "            if lim_cuts.t.loc[contam1_i,'Pcontamination'] == contam_lim:\n",
    "                contam_lim_cut = lim_cuts.t.loc[contam1_i,'PSF Chi-Square Cut']\n",
    "            else:\n",
    "                contam2_i = contam1_i + 1\n",
    "                x = np.array([lim_cuts.t.loc[contam1_i,'PSF Chi-Square Cut'], lim_cuts.t.loc[contam2_i,'PSF Chi-Square Cut']])\n",
    "                contam_y = np.array([lim_cuts.t.loc[contam1_i,'Pcontamination'], lim_cuts.t.loc[contam2_i,'Pcontamination']])\n",
    "                loss_y = np.array([lim_cuts.t.loc[contam1_i,'Ploss'], lim_cuts.t.loc[contam2_i,'Ploss']])\n",
    "                contam_line = np.polyfit(x,contam_y,1)\n",
    "                loss_line = np.polyfit(x,loss_y,1)\n",
    "                contam_lim_cut = (contam_lim-contam_line[1])/contam_line[0]\n",
    "\n",
    "    lc_info = get_lim_cuts_data(lc_info, index, loss_lim_cut, 'loss_lim_cut')\n",
    "    lc_info = get_lim_cuts_data(lc_info, index, contam_lim_cut, 'contam_lim_cut')\n",
    "\n",
    "    return lc_info, contam_lim_cut, loss_lim_cut, contam_case, loss_case\n",
    "\n",
    "def choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case):\n",
    "# case 1 and 1: final_cut = 3\n",
    "    # case 1 and 2: take limit of case 2\n",
    "    # case 1 and 3: take limit of case 3\n",
    "    # case 2 and 2: print lims don't work\n",
    "    # case 2 and 3: choose_btwn_lim_cuts\n",
    "    # case 3 and 3: choose_btwn_lim_cuts\n",
    "\n",
    "    case1 = loss_case == 'below lim' or contam_case == 'below lim'\n",
    "    case2 = loss_case == 'above lim' or contam_case == 'above lim'\n",
    "    case3 = loss_case == 'crosses lim' or contam_case == 'crosses lim'\n",
    "\n",
    "    final_cut = None\n",
    "    if case1 and not case2 and not case3: # 1 and 1\n",
    "        print('Valid chi-square cut range from %0.2f to %0.2f! Setting to 3...' % (loss_lim_cut, contam_lim_cut))\n",
    "        final_cut = cut_start\n",
    "    elif case1: # 1\n",
    "        if case2: # and 2\n",
    "            if loss_case == 'above lim':\n",
    "                print('WARNING: contam_lim_cut <= %0.2f falls below limit %0.2f%%, but loss_lim_cut >= %0.2f falls above limit %0.2f%%! Setting to %0.2f...' % (contam_lim_cut, contam_lim, loss_lim_cut, loss_lim, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('WARNING: loss_lim_cut <= %0.2f falls below limit %0.2f%%, but contam_lim_cut >= %0.2f falls above limit %0.2f%%! Setting to %0.2f...' % (loss_lim_cut, loss_lim, contam_lim_cut, contam_lim, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "        else: # and 3\n",
    "            if loss_case == 'crosses lim':\n",
    "                print('Contam_lim_cut <= %0.2f falls below limit %0.2f%% and loss_lim_cut >= %0.2f crosses limit %0.2f%%, setting to %0.2f...' % (contam_lim_cut, contam_lim, loss_lim_cut, loss_lim, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('Loss_lim_cut <= %0.2f falls below limit %0.2f%% and contam_lim_cut >= %0.2f crosses limit %0.2f%%, setting to %0.2f...' % (loss_lim_cut, loss_lim, contam_lim_cut, contam_lim, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "    elif case2 and not case3: # 2 and 2\n",
    "        print('ERROR: chi-square loss_lim_cut >= %0.2f and contam_lim_cut <= %0.2f both fall above limits %0.2f%% and %0.2f%%! Try setting less strict limits. Setting final cut to nan.' % (loss_lim_cut, contam_lim_cut, loss_lim, contam_lim))\n",
    "        final_cut = np.nan\n",
    "    else: # 2 and 3 or 3 and 3\n",
    "        if loss_lim_cut > contam_lim_cut:\n",
    "            print('WARNING: chi-square loss_lim_cut >= %0.2f and contam_lim_cut <= %0.2f do not overlap! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %0.2f...' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "        else:\n",
    "            print('Valid chi-square cut range from %0.2f to %0.2f! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %0.2f... ' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "    return final_cut\n",
    "\n",
    "def get_lim_cuts_table(lc_info, index):\n",
    "    print('abs(uJy/duJy) cut at %0.2f \\nx2 cut from %0.2f to %0.2f inclusive, with step size %d' % (stn_cut,cut_start,cut_stop,cut_step))\n",
    "\n",
    "    lim_cuts = pdastrostatsclass(columns=['PSF Chi-Square Cut', 'N', 'Ngood', 'Nbad', 'Nkept', 'Ncut', 'Ngood,kept', 'Ngood,cut', 'Nbad,kept', 'Nbad,cut',\n",
    "                                          'Pgood,kept', 'Pgood,cut', 'Pbad,kept', 'Pbad,cut', 'Ngood,kept/Ngood', 'Ploss', 'Pcontamination',\n",
    "                                          'Nbad,cut 3<stn<=5', 'Nbad,cut 5<stn<=10', 'Nbad,cut 10<stn', 'Nbad,kept 3<stn<=5', 'Nbad,kept 5<stn<=10', 'Nbad,kept 10<stn'])\n",
    "    \n",
    "    lc = lc_info.t.loc[index,'lc']\n",
    "    indices = lc.ix_inrange(colnames=['MJD'],uplim=lc_info.t.loc[index,'mjdstart'],exclude_uplim=True) \n",
    "\n",
    "    # static cut at x2 = 50\n",
    "    #x2cut_50 = np.where(lc.t['chi/N'] < 50)[0]\n",
    "    #print('Static chi square cut at 50: %0.2f%% cut for baseline' % (100*len(AnotB(indices,x2cut_50))/len(indices)))\n",
    "\n",
    "    # good baseline measurement indices\n",
    "    b_good_i = lc.ix_inrange(colnames=['uJy/duJy'],lowlim=-stn_cut,uplim=stn_cut,indices=indices)\n",
    "    b_bad_i = AnotB(indices, b_good_i)\n",
    "    # for different x2 cuts decreasing from 50\n",
    "    for cut in range(cut_start,cut_stop+1,cut_step):\n",
    "        # kept baseline measurement indices\n",
    "        b_kept_i = lc.ix_inrange(colnames=['chi/N'],uplim=cut,indices=indices)\n",
    "        b_cut_i = AnotB(indices, b_kept_i)\n",
    "\n",
    "        df = pd.DataFrame([[cut, len(indices), # N\n",
    "                            len(b_good_i), # Ngood\n",
    "                            len(b_bad_i), # Nbad\n",
    "                            len(b_kept_i), # Nkept\n",
    "                            len(b_cut_i), # Ncut\n",
    "                            len(AandB(b_good_i,b_kept_i)), # Ngood,kept\n",
    "                            len(AandB(b_good_i,b_cut_i)), # Ngood,cut\n",
    "                            len(AandB(b_bad_i,b_kept_i)), # Nbad,kept\n",
    "                            len(AandB(b_bad_i,b_cut_i)), # Nbad,cut\n",
    "                            100*len(AandB(b_good_i,b_kept_i))/len(indices), # Ngood,kept/Nbaseline\n",
    "                            100*len(AandB(b_good_i,b_cut_i))/len(indices), # Ngood,cut/Nbaseline \n",
    "                            100*len(AandB(b_bad_i,b_kept_i))/len(indices), # Nbad,kept/Nbaseline\n",
    "                            100*len(AandB(b_bad_i,b_cut_i))/len(indices), # Nbad,cut/Nbaseline\n",
    "                            100*len(AandB(b_good_i,b_kept_i))/len(b_good_i), # Ngood,kept/Ngood\n",
    "                            100*len(AandB(b_good_i,b_cut_i))/len(b_good_i), # Ngood,cut/Ngood = Loss\n",
    "                            100*len(AandB(b_bad_i,b_kept_i))/len(b_kept_i), # Nbad,kept/Nkept = Contamination\n",
    "                            len(AandB(AandB(b_bad_i,b_cut_i), lc.ix_inrange(colnames=['uJy/duJy'],lowlim=-3,uplim=5,exclude_lowlim=True))), # Nbad,cut 3<stn<=5\n",
    "                            len(AandB(AandB(b_bad_i,b_cut_i), lc.ix_inrange(colnames=['uJy/duJy'],lowlim=5,uplim=10,exclude_lowlim=True))), # Nbad,cut 5<stn<=10\n",
    "                            len(AandB(AandB(b_bad_i,b_cut_i), lc.ix_inrange(colnames=['uJy/duJy'],lowlim=10,exclude_lowlim=True))), # Nbad,cut 10<stn \n",
    "                            len(AandB(AandB(b_bad_i,b_kept_i), lc.ix_inrange(colnames=['uJy/duJy'],lowlim=-3,uplim=5,exclude_lowlim=True))), # Nbad,kept 3<stn<=5\n",
    "                            len(AandB(AandB(b_bad_i,b_kept_i), lc.ix_inrange(colnames=['uJy/duJy'],lowlim=5,uplim=10,exclude_lowlim=True))), # Nbad,kept 5<stn<=10\n",
    "                            len(AandB(AandB(b_bad_i,b_kept_i), lc.ix_inrange(colnames=['uJy/duJy'],lowlim=10,exclude_lowlim=True))), # Nbad,kept 10<stn \n",
    "                            ]], columns=['PSF Chi-Square Cut', 'N', 'Ngood', 'Nbad', 'Nkept', 'Ncut', 'Ngood,kept', 'Ngood,cut', 'Nbad,kept', 'Nbad,cut',\n",
    "                                         'Pgood,kept', 'Pgood,cut', 'Pbad,kept', 'Pbad,cut', 'Ngood,kept/Ngood', 'Ploss', 'Pcontamination',\n",
    "                                         'Nbad,cut 3<stn<=5', 'Nbad,cut 5<stn<=10', 'Nbad,cut 10<stn', 'Nbad,kept 3<stn<=5', 'Nbad,kept 5<stn<=10', 'Nbad,kept 10<stn'])\n",
    "        lim_cuts.t = pd.concat([lim_cuts.t,df],ignore_index=True)\n",
    "    return lim_cuts\n",
    "\n",
    "def get_lim_cuts_loop(lc_info):\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        print('\\nCutting %s...' % lc_info.t.loc[index,'tnsname'])\n",
    "        lim_cuts = get_lim_cuts_table(lc_info, index)\n",
    "\n",
    "        lc_info, contam_lim_cut, loss_lim_cut, contam_case, loss_case = get_lim_cuts(lc_info, index, lim_cuts)\n",
    "        print('Contamination cut according to given contam_limit: ',contam_lim_cut,'\\nLoss cut according to given loss_limit: ',loss_lim_cut)\n",
    "        lc_info.t.loc[index,'contam_lim_cut'] = contam_lim_cut\n",
    "        lc_info.t.loc[index,'loss_lim_cut'] = loss_lim_cut\n",
    "        lc_info.t.loc[index,'contam_case'] = contam_case\n",
    "        lc_info.t.loc[index,'loss_case'] = loss_case\n",
    "\n",
    "        final_cut = choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case)\n",
    "        lc_info.t.loc[index,'final_cut'] = final_cut\n",
    "        \n",
    "        if np.isnan(final_cut):\n",
    "            print('Final chi-square cut could not be determined.')\n",
    "            lc_info.t.loc[index,'Pcontamination'] = np.nan\n",
    "            lc_info.t.loc[index,'Ploss'] = np.nan\n",
    "        else:\n",
    "            if final_cut==contam_lim_cut:\n",
    "                lc_info.t.loc[index,'Pcontamination'] = lc_info.t.loc[index,'contam_lim_cut_Pcontamination']\n",
    "                lc_info.t.loc[index,'Ploss'] = lc_info.t.loc[index,'contam_lim_cut_Ploss']\n",
    "            else:\n",
    "                lc_info.t.loc[index,'Pcontamination'] = lc_info.t.loc[index,'loss_lim_cut_Pcontamination']\n",
    "                lc_info.t.loc[index,'Ploss'] = lc_info.t.loc[index,'loss_lim_cut_Ploss']\n",
    "            #lc_info.t.loc[index,'Pcontamination'] = lc_info.t.loc[index,'loss_lim_cut_Pcontamination']\n",
    "            #lc_info.t.loc[index,'Ploss'] = lc_info.t.loc[index,'loss_lim_cut_Ploss']\n",
    "            print('Final chi-square cut is %d, with %0.2f%% contamination and %0.2f%% loss.' % (final_cut, lc_info.t.loc[index,'Pcontamination'], lc_info.t.loc[index,'Ploss']))\n",
    "\n",
    "        #plot_lim_cuts(lc_info.t.loc[index], lim_cuts, contam_lim_cut, loss_lim_cut)\n",
    "\n",
    "    return lc_info\n",
    "\n",
    "if lim_to_prioritize != 'loss_lim' and lim_to_prioritize != 'contam_lim':\n",
    "    print(\"ERROR: lim_to_prioritize must be 'loss_lim' or 'contam_lim'!\")\n",
    "    sys.exit()\n",
    "\n",
    "lc_info.t['contam_lim_cut'] = None\n",
    "lc_info.t['loss_lim_cut'] = None\n",
    "\n",
    "lc_info = get_lim_cuts_loop(lc_info)\n",
    "print('\\n',lc_info.t.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Pextendedtail and Phockeystick against Contamination and Loss Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "et, hs = axes.flatten()\n",
    "plt.suptitle('Chi-Square Limits and Regions',fontsize=15, y=1)\n",
    "et.set_title('Chi-Square Limits for Extended Tail Regions')\n",
    "et.set_xlabel('Percent Data with %d<x2<%d' % (extendedtail_cut, hockeystick_cut))\n",
    "et.set_ylabel('Chi-Square Cut')\n",
    "hs.set_title('Chi-Square Limits for Hockeystick Regions')\n",
    "hs.set_xlabel('Percent Data with x2>%d' % hockeystick_cut)\n",
    "hs.set_ylabel('Chi-Square Cut')\n",
    "\n",
    "color = iter(plt.cm.rainbow(np.linspace(0, 1, len(lc_info.t))))\n",
    "\n",
    "tnsnames = []\n",
    "for index in range(0,len(lc_info.t)):\n",
    "    c = next(color)\n",
    "    tnsnames.append(mpatches.Patch(color=c, label=lc_info.t.loc[index,'tnsname']))\n",
    "\n",
    "    #et.scatter(lc_info.t.loc[index,'Pextendedtail_b'], lc_info.t.loc[index,'final_cut'], color='b')\n",
    "    #hs.scatter(lc_info.t.loc[index,'Phockeystick_b'], lc_info.t.loc[index,'final_cut'], color='b')\n",
    "\n",
    "    regionstates = get_region_table(lc_info.t.loc[index])\n",
    "    print('\\nSN %s\\n'%lc_info.t.loc[index,'tnsname'],regionstates.t.to_string())\n",
    "\n",
    "    for j in [1,2,3]:\n",
    "        if np.isnan(regionstates.t.loc[j,'start_index']) or np.isnan(regionstates.t.loc[j,'end_index']):\n",
    "            continue\n",
    "\n",
    "        print('Getting lim_cuts table for regions from i=%d to i=%d...' % (regionstates.t.loc[j,'start_index'], regionstates.t.loc[j,'end_index']))\n",
    "        lim_cuts = get_lim_cuts_table(lc_info.t.loc[index], indices=range(regionstates.t.loc[j,'start_index'],regionstates.t.loc[j,'end_index']+1))\n",
    "        print(lim_cuts.t.to_string())\n",
    "\n",
    "        contam_lim_cut, loss_lim_cut, contam_case, loss_case = get_lim_cuts(lc_info, index, lim_cuts)\n",
    "        print('Contamination cut according to given contam_limit: ',contam_lim_cut,'\\nLoss cut according to given loss_limit: ',loss_lim_cut)\n",
    "        final_cut = choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case)\n",
    "\n",
    "        if np.isnan(final_cut):\n",
    "            print('Final cut: ',final_cut)\n",
    "        else:\n",
    "            print('Final chi-square cut: %d, with %0.2f%% contamination and %0.2f%% loss.' % (final_cut, lim_cuts.t.loc[np.where(lim_cuts.t['PSF Chi-Square Cut']==final_cut)[0][0],'Contamination'], lim_cuts.t.loc[np.where(lim_cuts.t['PSF Chi-Square Cut']==final_cut)[0][0],'Loss']))\n",
    "\n",
    "        et.scatter(regionstates.t.loc[j,'Pextendedtail'], final_cut, color=c, edgecolors='black') #, label='%s b_%d'%(lc_info.t.loc[index,'tnsname'],j-1))\n",
    "        hs.scatter(regionstates.t.loc[j,'Phockeystick'], final_cut, color=c, edgecolors='black') #, label='%s b_%d'%(lc_info.t.loc[index,'tnsname'],j-1))\n",
    "\n",
    "        plot_lim_cuts(lc_info.t.loc[index], lim_cuts, contam_lim_cut, loss_lim_cut, add2title=' (b_%d)'%(j-1))\n",
    "\n",
    "#et.legend()\n",
    "hs.legend(handles=tnsnames, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "\"\"\"\n",
    "red = mpatches.Patch(color='red', label='Contamination Cut')\n",
    "green = mpatches.Patch(color='green', label='Loss Cut')\n",
    "et.legend(handles=[red, green])\n",
    "hs.legend(handles=[red, green])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample_data(lc_info):\n",
    "    sample_data = pdastrostatsclass(columns=['tnsname', 'b_Pextendedtail', 'b_Phockeystick', 'b_Pwellbehaved', 'final_cut', 'Pcontamination', 'Ploss'])\n",
    "    si = 0\n",
    "\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        regionstates = get_region_table(lc_info.t.loc[index])\n",
    "        \n",
    "        lim_cuts = get_lim_cuts_table(lc_info,index)\n",
    "        lc_info, contam_lim_cut, loss_lim_cut, contam_case, loss_case = get_lim_cuts(lc_info, index, lim_cuts)\n",
    "        print('Contamination cut according to given contam_limit: ',contam_lim_cut,'\\nLoss cut according to given loss_limit: ',loss_lim_cut)\n",
    "        lc_info.t.loc[index,'contam_case'] = contam_case\n",
    "        lc_info.t.loc[index,'loss_case'] = loss_case\n",
    "        lc_info.t.loc[index,'contam_lim_cut'] = contam_lim_cut\n",
    "        lc_info.t.loc[index,'loss_lim_cut'] = loss_lim_cut\n",
    "\n",
    "        final_cut = choose_btwn_lim_cuts(contam_lim_cut, loss_lim_cut, contam_case, loss_case)\n",
    "        lc_info.t.loc[index,'final_cut'] = final_cut\n",
    "        \n",
    "        if np.isnan(final_cut):\n",
    "            print('Final chi-square cut could not be determined.')\n",
    "        else:\n",
    "            sample_data.t.loc[si,'tnsname'] = lc_info.t.loc[index,'tnsname']\n",
    "            sample_data.t.loc[si,'final_cut'] = final_cut\n",
    "            if final_cut==contam_lim_cut:\n",
    "                sample_data.t.loc[si,'Ngood'] =  lc_info.t.loc[index,'contam_lim_cut_Ngood']\n",
    "                sample_data.t.loc[si,'Nbad'] =  lc_info.t.loc[index,'contam_lim_cut_Nbad']\n",
    "                sample_data.t.loc[si,'Nkept'] =  lc_info.t.loc[index,'contam_lim_cut_Nkept']\n",
    "                sample_data.t.loc[si,'Ncut'] =  lc_info.t.loc[index,'contam_lim_cut_Ncut']\n",
    "                sample_data.t.loc[si,'Ngood,kept'] =  lc_info.t.loc[index,'contam_lim_cut_Ngood,kept']\n",
    "                sample_data.t.loc[si,'Ngood,cut'] =  lc_info.t.loc[index,'contam_lim_cut_Ngood,cut']\n",
    "                sample_data.t.loc[si,'Nbad,kept'] =  lc_info.t.loc[index,'contam_lim_cut_Nbad,kept']\n",
    "                sample_data.t.loc[si,'Nbad,cut'] =  lc_info.t.loc[index,'contam_lim_cut_Nbad,cut']\n",
    "                sample_data.t.loc[si,'Pgood,kept'] =  lc_info.t.loc[index,'contam_lim_cut_Pgood,kept']\n",
    "                sample_data.t.loc[si,'Pgood,cut'] =  lc_info.t.loc[index,'contam_lim_cut_Pgood,cut']\n",
    "                sample_data.t.loc[si,'Pbad,kept'] =  lc_info.t.loc[index,'contam_lim_cut_Pbad,kept']\n",
    "                sample_data.t.loc[si,'Pbad,cut'] =  lc_info.t.loc[index,'contam_lim_cut_Pbad,cut']\n",
    "                sample_data.t.loc[si,'Ngood,kept/Ngood'] =  lc_info.t.loc[index,'contam_lim_cut_Ngood,kept/Ngood']\n",
    "                sample_data.t.loc[si,'Ploss'] =  lc_info.t.loc[index,'contam_lim_cut_Ploss']\n",
    "                sample_data.t.loc[si,'Pcontamination'] =  lc_info.t.loc[index,'contam_lim_cut_Pcontamination']\n",
    "            else:\n",
    "                sample_data.t.loc[si,'Ngood'] =  lc_info.t.loc[index,'loss_lim_cut_Ngood']\n",
    "                sample_data.t.loc[si,'Nbad'] =  lc_info.t.loc[index,'loss_lim_cut_Nbad']\n",
    "                sample_data.t.loc[si,'Nkept'] =  lc_info.t.loc[index,'loss_lim_cut_Nkept']\n",
    "                sample_data.t.loc[si,'Ncut'] =  lc_info.t.loc[index,'loss_lim_cut_Ncut']\n",
    "                sample_data.t.loc[si,'Ngood,kept'] =  lc_info.t.loc[index,'loss_lim_cut_Ngood,kept']\n",
    "                sample_data.t.loc[si,'Ngood,cut'] =  lc_info.t.loc[index,'loss_lim_cut_Ngood,cut']\n",
    "                sample_data.t.loc[si,'Nbad,kept'] =  lc_info.t.loc[index,'loss_lim_cut_Nbad,kept']\n",
    "                sample_data.t.loc[si,'Nbad,cut'] =  lc_info.t.loc[index,'loss_lim_cut_Nbad,cut']\n",
    "                sample_data.t.loc[si,'Pgood,kept'] =  lc_info.t.loc[index,'loss_lim_cut_Pgood,kept']\n",
    "                sample_data.t.loc[si,'Pgood,cut'] =  lc_info.t.loc[index,'loss_lim_cut_Pgood,cut']\n",
    "                sample_data.t.loc[si,'Pbad,kept'] =  lc_info.t.loc[index,'loss_lim_cut_Pbad,kept']\n",
    "                sample_data.t.loc[si,'Pbad,cut'] =  lc_info.t.loc[index,'loss_lim_cut_Pbad,cut']\n",
    "                sample_data.t.loc[si,'Ngood,kept/Ngood'] =  lc_info.t.loc[index,'loss_lim_cut_Ngood,kept/Ngood']\n",
    "                sample_data.t.loc[si,'Ploss'] =  lc_info.t.loc[index,'loss_lim_cut_Ploss']\n",
    "                sample_data.t.loc[si,'Pcontamination'] =  lc_info.t.loc[index,'loss_lim_cut_Pcontamination']\n",
    "            sample_data.t.loc[si,'b_Pextendedtail'] = regionstates.t.loc[0,'Pextendedtail']\n",
    "            sample_data.t.loc[si,'b_Phockeystick'] = regionstates.t.loc[0,'Phockeystick']\n",
    "            sample_data.t.loc[si,'b_Pwellbehaved'] = regionstates.t.loc[0,'Pwellbehaved']\n",
    "            print('Final chi-square cut is %0.2f, with %0.2f%% contamination and %0.2f%% loss.' % (final_cut, lc_info.t.loc[index,'Pcontamination'], lc_info.t.loc[index,'Ploss']))\n",
    "\n",
    "            si += 1\n",
    "    \n",
    "    print(sample_data.t.to_string())\n",
    "    sample_data.write('sample_data.txt',overwrite=True)\n",
    "\n",
    "save_sample_data(lc_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
