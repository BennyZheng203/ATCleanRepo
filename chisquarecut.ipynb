{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Run preliminary imports, set directories, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, copy, requests, time, json\n",
    "from pdastro import pdastrostatsclass, AandB, AnotB, AorB, not_AandB\n",
    "\n",
    "# for interactive sliders, etc.\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "# for getting discovery date from TNS\n",
    "from lxml import html\n",
    "from collections import OrderedDict\n",
    "from astropy.time import Time\n",
    "\n",
    "# plotting\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as matlib\n",
    "import warnings\n",
    "warnings.simplefilter('error', RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.figsize'] = [12, 7]\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Times'\n",
    "\n",
    "# ATLAS template changes\n",
    "global tchange1\n",
    "global tchange2\n",
    "tchange1 = 58417\n",
    "tchange2 = 58882\n",
    "\n",
    "#https://towardsdatascience.com/5-powerful-tricks-to-visualize-your-data-with-matplotlib-16bc33747e05\n",
    "\n",
    "def dataPlot(x, y, dx=None, dy=None, sp=None, label=None, fmt='bo', ecolor='k', elinewidth=None, barsabove = False, capsize=1, logx=False, logy=False):\n",
    "    if sp == None:\n",
    "        sp = matlib.subplot(111)\n",
    "    if dx is None and dy is None:\n",
    "        if logy:\n",
    "            if logx:\n",
    "                plot, = sp.loglog(x, y, fmt)\n",
    "            else:\n",
    "                plot, = sp.semilogy(x, y, fmt)\n",
    "        elif logx:\n",
    "            plot, = sp.semilogx(x, y, fmt)\n",
    "        else:\n",
    "            if barsabove:\n",
    "                plot, dplot,dummy = sp.errorbar(x, y, label=label, fmt=fmt, capsize=capsize, barsabove=barsabove)\n",
    "            else:\n",
    "                plot, = sp.plot(x, y, fmt)\n",
    "        return sp, plot, None\n",
    "    else:\n",
    "        if logy:\n",
    "            sp.set_yscale(\"log\", nonposx='clip')\n",
    "        if logx:\n",
    "            sp.set_xscale(\"log\", nonposx='clip')\n",
    "        plot, dplot, dummy = sp.errorbar(x, y, xerr=dx, yerr=dy, label=label, fmt=fmt, ecolor=ecolor, elinewidth=elinewidth, capsize=capsize, barsabove=barsabove)\n",
    "        return sp, plot, dplot\n",
    "\n",
    "lc_info = pdastrostatsclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your data directory (where the SN light curves are located) here:\n",
    "global dir\n",
    "dir = '/Users/sofiarest/Google Drive/My Drive/College/STScI Research Paper/atlaslc_chisquare/brightsne/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about template changes at MJDs 58417 and 58882: https://fallingstar-data.com/forcedphot/faq/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load in light curves\n",
    "Enter the TNS name for each SN you wish to load in and examine/cut in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter each SN's TNS name here:\n",
    "lc_info.t['tnsname'] = ['2020lse','2019vxm','2020jfo','2018pv','2017guh','2018oh','2017izu']\n",
    "\n",
    "# Optionally, enter each SN's discovery date (start of SN) here:\n",
    "lc_info.t['mjdstart'] = [59005.264,58818.04,58975.20256,58152.631331,58014.31,58153.49,None]\n",
    "\n",
    "# Optionally, enter a preliminary classification of the SN measurements:\n",
    "lc_info.t['classification'] = ['good','bad',None,None,None,None,None]\n",
    "\n",
    "#lc_info.t['tnsname'] = ['2020lse','2019vxm','2020jfo','2017gjn','2017glq','2017gup','2017guu','2017guv','2017haf','2017gqr','2017guh','2017gvp','2017ghu','2017hgz','2017hjw','2017hjy','2017hoq','2017hpa','2017hou','2017igf','2017isq','2017gxq','2017isj','2017iyb','2017iji','2017jav','2017iyw','2017jyl','2018cqw','2020ejm','2017izu','2019syd','2017gjd','2018jaj','2018jov','2018jaz','2018jaz','2018imd','2018kfv','2017jd','2018pc','2020fcw','2018hkq','2019wdx','2018lqy','2018gfi','2018K','2020dkm','2018yh','2020afp','2019wrz','2018gv','2018iq','2018gl','2018kp','2018oh','2018pv','2018xx','2018yu','2018aaz','2018zz','2018ajp','2018aqh','2018aoz','2018aqi','2018azu','2018aye','2018cnj','2018cqj','2018cuw','2018dda','2018dzy','2018ebk','2018ddi','2018enc','2018epx','2018etm','2018feb','2018fop','2018fli','2018fnq','2018fhx']\n",
    "#lc_info.t['mjdstart'] = [58955.26,58768.04,58925.20,57994.85992,57999.35754,58013.556,58009.58899,58011.305,58026.5,58005.1567,58014.31,58019.48,57991.4866,58036.02,58040.57399,58040.49699,58047.551,58051.34602,58050.37014,58075.59,58091.618,58013.453,58089.34568,58103.44699,58077.64,58106.24301,58105.3,58117.546,58287.2,58919.23791,58101.97699,58767.10699,57994.09,58447.48199,58460.583,58442.51023,58442.51023,58436.8626,58468.28699,57762.21399,58152.75,58936.437,58406.37,58824.38,58358.39,58373.4,58121.13589,58904.44051,58173.25,58866.66,58832.48,58133.68132,58137.49453,58131.56999,58142.36257,58153.49,58152.63133,58170.38199,58178.223,58183.56689,58180.92,58195.81,58213.44899,58210.30899,58214.07,58220.28699,58229.31,58266.61199,58282.268,58293.515,58303.96,58315.59399,58315.55199,58301.44,58332.32699,58335.605,58338.343,58346.16399,58351.56899,58343.15199,58361.08,58351.38]\n",
    "#lc_info.t['classification'] = ['good','bad','good',None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the rest of lc_info (discovery date, loading in lc, etc.)\n",
    "\n",
    "lc_info.t['lc'] = [None] * len(lc_info.t)\n",
    "\n",
    "# get the discovery date for each SN\n",
    "print('Obtaining discovery date information from TNS if needed...')\n",
    "def get_tns_data(tnsname):\n",
    "\ttry:\n",
    "\t\tget_obj = [(\"objname\",tnsname), (\"objid\",\"\"), (\"photometry\",\"1\"), (\"spectra\",\"1\")]\n",
    "\t\tget_url = 'https://www.wis-tns.org/api/get/object'\n",
    "\t\tjson_file = OrderedDict(get_obj)\n",
    "\t\tget_data = {'api_key':'2eca323a16b17d78fbc99cd6f1f801699a81a91c','data':json.dumps(json_file)}\n",
    "\t\tresponse = requests.post(get_url, data=get_data, headers={'User-Agent':'tns_marker{\"tns_id\":104739,\"type\": \"bot\", \"name\":\"Name and Redshift Retriever\"}'})\n",
    "\t\tjson_data = json.loads(response.text,object_pairs_hook=OrderedDict)\n",
    "\t\treturn json_data\n",
    "\texcept Exception as e:\n",
    "\t\treturn 'Error: \\n'+str(e)\n",
    "def get_disc_date(tnsname):\n",
    "\tjson_data = get_tns_data(tnsname)\n",
    "\tdiscoverydate = json_data['data']['reply']['discoverydate']\n",
    "\tdate = list(discoverydate.partition(' '))[0]\n",
    "\ttime = list(discoverydate.partition(' '))[2]\n",
    "\tdisc_date_format = date+'T'+time\n",
    "\tdateobjects = Time(disc_date_format, format='isot', scale='utc')\n",
    "\tdisc_date = dateobjects.mjd\n",
    "\t#print(tnsname+\" discovery date (MJD): %.4f\" % disc_date)\n",
    "\treturn disc_date\n",
    "def get_disc_date_loop(lc_info):\n",
    "\tfor index in lc_info.ix_null(colnames=['mjdstart']):\n",
    "\t\tdisc_date = get_disc_date(lc_info.t.loc[index,'tnsname'])\n",
    "\t\tlc_info.t.loc[index,'mjdstart'] = disc_date\n",
    "\treturn lc_info\n",
    "lc_info = get_disc_date_loop(lc_info)\n",
    "lc_info.write(columns=['tnsname','mjdstart','classification'])\n",
    "\n",
    "# load in the lcs from files\n",
    "print('\\nLoading in light curves...')\n",
    "def load_lc_loop(lc_info):\n",
    "\tfor index in range(0,len(lc_info.t)):\n",
    "\t\tlc_info.t.loc[index,'lc'] = pdastrostatsclass()\n",
    "\t\tfilename = dir+lc_info.t.loc[index,'tnsname']+'/'+lc_info.t.loc[index,'tnsname']+'_i000.o.lc.txt'\n",
    "\t\ttry:\n",
    "\t\t\tlc_info.t.loc[index,'lc'].load_spacesep(filename,delim_whitespace=True)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint('Could not load light curve for SN %s at %s: %s' % (lc_info.t.loc[index,'tnsname'], filename, str(e)))\n",
    "\t\t\tsys.exit()\n",
    "\t\tlc_info.t.loc[index,'lc'].t['uJy/duJy'] = lc_info.t.loc[index,'lc'].t['uJy']/lc_info.t.loc[index,'lc'].t['duJy']\n",
    "\tprint('Success')\n",
    "\treturn lc_info\n",
    "lc_info = load_lc_loop(lc_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each light curve\n",
    "def prelim_plot_lc(info):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "    ax.axhline(linewidth=1,color='k')\n",
    "    ax.set_ylabel('uJy')\n",
    "    ax.set_xlabel('MJD')\n",
    "    ax.set_title('SN '+info['tnsname']+' Light Cuve')\n",
    "    ax.axvline(x=tchange1,color='magenta', label='ATLAS template change')\n",
    "    ax.axvline(x=tchange2,color='magenta')\n",
    "    return fig, ax\n",
    "def plot_lc_mjdstart(info):\n",
    "    baseline = np.where(info['lc'].t['MJD'] < info['mjdstart'])[0]\n",
    "    duringsn = np.where(info['lc'].t['MJD'] > info['mjdstart'])[0]\n",
    "\n",
    "    fig, ax = prelim_plot_lc(info)\n",
    "    sp, plot_baseline, dplot = dataPlot(info['lc'].t.loc[baseline,'MJD'], info['lc'].t.loc[baseline,'uJy'], sp=ax)\n",
    "    matlib.setp(plot_baseline,ms=5,color='b',marker='o',label='Baseline flux')\n",
    "    sp, plot_aftersn, dplot = dataPlot(info['lc'].t.loc[duringsn,'MJD'], info['lc'].t.loc[duringsn,'uJy'], sp=ax)\n",
    "    matlib.setp(plot_aftersn,ms=5,color='c',marker='o',label='During and after SN')\n",
    "    plt.legend()\n",
    "def plot_lc_mjdstart_loop(lc_info):\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        plot_lc_mjdstart(lc_info.t.loc[index])\n",
    "plot_lc_mjdstart_loop(lc_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Make sure each light curve has bright flux and classify the SN as \"good\" or \"bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the limit for the standard deviation calculated during the 3-sigma cut\n",
    "# that will determine a SN's classification (\"good\" or \"bad\").\n",
    "global stdev_limit\n",
    "stdev_limit = 1.5\n",
    "\n",
    "# Enter the limit for the percent of measurements clipped during the 3-sigma cut \n",
    "# that will determine a SN's classification (\"good\" or \"bad\").\n",
    "global clippercent_limit\n",
    "clippercent_limit = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate rough brightness and confirm/change classification of each SN\n",
    "\n",
    "# estimate rough brightness of each SN by getting 90th percentile flux from mjdstart to mjdstart+60 and determine if SN is a good candidate\n",
    "\n",
    "def get_90th_percentile_flux(info):\n",
    "    target = AandB(np.where(info['lc'].t['MJD']>info['mjdstart'])[0], np.where(info['lc'].t['MJD']<info['mjdstart']+60)[0])\n",
    "    if len(target)==0: \n",
    "        return None\n",
    "    else:\n",
    "        return np.percentile(info['lc'].t.loc[target, 'uJy'], 90)\n",
    "\n",
    "def remove_SN(lc_info,index):\n",
    "    print('Removing SN at index %d...' % index)\n",
    "    lc_info.t = lc_info.t.drop([index])\n",
    "    lc_info.t = lc_info.t.reset_index(drop=True)\n",
    "    return lc_info\n",
    "\n",
    "def get_90th_percentile_flux_loop(lc_info):\n",
    "    index = 0\n",
    "    while index < len(lc_info.t):\n",
    "        flux = get_90th_percentile_flux(lc_info.t.loc[index])\n",
    "        if flux is None:\n",
    "            print('WARNING: For %s, 90th percentile flux not found' % (lc_info.t.loc[index,'tnsname']))\n",
    "            lc_info = remove_SN(lc_info,index)\n",
    "        elif(flux > 1000):\n",
    "            print('For %s, 90th percentile flux %0.2f is over 1000 ' % (lc_info.t.loc[index,'tnsname'], flux) + u'\\u2713')\n",
    "            index += 1\n",
    "        else:\n",
    "            print('WARNING: For %s, 90th percentile flux %0.2f is under 1000\\nRemoving SN from list? (y/n)' % (lc_info.t.loc[index,'tnsname'], flux))\n",
    "            #answer = input()\n",
    "            #if (answer == 'y'):\n",
    "            lc_info = remove_SN(lc_info,index)\n",
    "            #else:\n",
    "                #index += 1\n",
    "    return lc_info\n",
    "\n",
    "lc_info = get_90th_percentile_flux_loop(lc_info)\n",
    "print('\\nRevised sample of SNe (only SNe with high flux):')\n",
    "lc_info.write(columns=['tnsname','mjdstart','classification'])\n",
    "\n",
    "# to confirm their classifications, take the baseline uJy/duJy and apply a 3-sigma cut. \"Bad\" SNe will have: sigma > 1.5, % data cut > 5%\n",
    "\n",
    "print('\\nstdev limit: %0.1f, %% data clipped limit: %0.1f' % (stdev_limit, clippercent_limit))\n",
    "\n",
    "def sigmacut_lc(info):\n",
    "    baseline = np.where(info['lc'].t['MJD'] < info['mjdstart'])[0]\n",
    "    info['lc'].calcaverage_sigmacutloop('uJy/duJy',noisecol=None,indices=baseline,verbose=1,Nsigma=3.0,median_firstiteration=True)\n",
    "    print('stdev: ',info['lc'].statparams['stdev'],', % data clipped: ',100*info['lc'].statparams['Nclip']/len(info['lc'].t))\n",
    "    output = 'Classified as '\n",
    "    if info['lc'].statparams['stdev'] is None or info['lc'].statparams['Nclip'] is None:\n",
    "        classification = 'bad'\n",
    "    elif(info['lc'].statparams['stdev']<stdev_limit and (100*info['lc'].statparams['Nclip']/len(info['lc'].t))<clippercent_limit):\n",
    "        classification = 'good'\n",
    "    else: \n",
    "        classification = 'bad'\n",
    "    output += classification + ' SN; '\n",
    "    if(info['classification']==classification):\n",
    "        output += 'consistent with preliminary classification ' + u'\\u2713'\n",
    "    else:\n",
    "        output += 'WARNING: not consistent with preliminary classification (\\'%s\\')\\nReclassifying in table...' % info['classification']\n",
    "        info['classification'] = classification\n",
    "    print(output)\n",
    "    return info\n",
    "\n",
    "def sigmacut_lc_loop(lc_info):\n",
    "    for index in range(0, len(lc_info.t)):\n",
    "        print('Sigmacutting %s...' % lc_info.t.loc[index,'tnsname'])\n",
    "        lc_info.t.loc[index] = sigmacut_lc(lc_info.t.loc[index])\n",
    "    return lc_info\n",
    "    \n",
    "lc_info = sigmacut_lc_loop(lc_info)\n",
    "print('\\nRevised sample of bright SNe (all SNe reclassified as good or bad):')\n",
    "print(lc_info.t[['tnsname','mjdstart','classification']].to_string())\n",
    "#lc_info.write(columns=['tnsname','mjdstart','classification'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Examine each region defined by the ATLAS template changes and its chi-squares\n",
    "There are two ATLAS template changes, one at MJD = 58417 and one at MJD = 58882."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global hockeystick_cut\n",
    "global hockeystick_cut_pcutoff\n",
    "global extendedtail_cut\n",
    "global extendedtail_cut_pcutoff\n",
    "# if % data with x2 greater than hockeystick_cut is >hockeystick_cut_pcutoff%, classify as hockeystick.\n",
    "hockeystick_cut = 100\n",
    "hockeystick_cut_pcutoff = 1.0\n",
    "# if % data with x2 greater than extendedtail_cut and smaller than hockeystick_cut is >extendedtail_cut_pcutoff%, classify as extended tail.\n",
    "extendedtail_cut = 5\n",
    "extendedtail_cut_pcutoff = 1.0\n",
    "# else classify as well-behaved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide light curve into 6 MJD regions: baseline (t0, t1, t2), SN (t0, t1, t2).\n",
    "# for each MJD region, calculate the number of detections. If n < 50, disregard. If n > 50, continue.\n",
    "# hockeystick: % data with x2>100\n",
    "# extended tail: % data with 5<x2<100\n",
    "# well-behaved: % data with x2<5\n",
    "\n",
    "def get_region_table_plot_colors(regionstates):\n",
    "    colors = []\n",
    "    for state in regionstates.loc[[1,2,3,5,6,7],'state']:\n",
    "        if state == 'hockeystick':\n",
    "            colors.append('red')\n",
    "        elif state == 'extended tail':\n",
    "            colors.append('yellow')\n",
    "        else:\n",
    "            colors.append('green')\n",
    "    return colors \n",
    "\n",
    "def get_regionstates_allsne_table_colors(val):\n",
    "    if val == 'hockeystick':\n",
    "        color = 'red'\n",
    "    elif val == 'extended tail':\n",
    "        color = 'yellow'\n",
    "    elif val == 'well-behaved':\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'white'\n",
    "    return 'background-color: %s' % color\n",
    "\n",
    "def get_ranges(info):\n",
    "    baseline = np.where(info['lc'].t['MJD'] < info['mjdstart'])[0]\n",
    "    aftersnstart = AnotB(info['lc'].getindices(),baseline)\n",
    "    ranges = {}\n",
    "    ranges[0] = baseline\n",
    "    ranges[1] = AandB(baseline, np.where(info['lc'].t['MJD'] < tchange1)[0]) # b_t0\n",
    "    ranges[2] = AandB(baseline, AandB(np.where(info['lc'].t['MJD'] > tchange1)[0], np.where(info['lc'].t['MJD'] < tchange2)[0])) # b_t1\n",
    "    ranges[3] = AandB(baseline, np.where(info['lc'].t['MJD'] > tchange2)[0]) # b_t2\n",
    "    ranges[4] = aftersnstart\n",
    "    ranges[5] = AandB(aftersnstart, np.where(info['lc'].t['MJD'] < tchange1)[0]) # sn_t0\n",
    "    ranges[6] = AandB(aftersnstart, AandB(np.where(info['lc'].t['MJD'] > tchange1)[0], np.where(info['lc'].t['MJD'] < tchange2)[0])) # sn_t1\n",
    "    ranges[7] = AandB(aftersnstart, np.where(info['lc'].t['MJD'] > tchange2)[0]) # sn_t2\n",
    "    return ranges\n",
    "\n",
    "def get_region_table(info):\n",
    "    regionstates = pdastrostatsclass(columns=['region','n','start_index','end_index','mjd_start','Phockeystick','Pextendedtail','Pwellbehaved','state'])\n",
    "    regionstates.t['region'] = ['b','b_t0','b_t1','b_t2','sn','sn_t0','sn_t1','sn_t2']\n",
    "    ranges = get_ranges(info)\n",
    "\n",
    "    for index in range(0,len(ranges)):\n",
    "        regionstates.t.loc[index,'n'] = len(ranges[index])\n",
    "\n",
    "        if len(ranges[index] > 0): \n",
    "            regionstates.t.loc[index,'mjd_start'] = info['lc'].t.loc[ranges[index][0],'MJD']\n",
    "            regionstates.t.loc[index,'start_index'] = ranges[index][0]\n",
    "            regionstates.t.loc[index,'end_index'] = ranges[index][-1]\n",
    "        else:\n",
    "            regionstates.t.loc[index,'mjd_start'] = np.nan\n",
    "            regionstates.t.loc[index,'start_index'] = np.nan\n",
    "            regionstates.t.loc[index,'end_index'] = np.nan\n",
    "        if len(ranges[index]) < 50:\n",
    "            regionstates.t.loc[index,'Phockeystick'] = np.nan\n",
    "            regionstates.t.loc[index,'Pextendedtail'] = np.nan\n",
    "            regionstates.t.loc[index,'Pwellbehaved'] = np.nan\n",
    "            regionstates.t.loc[index,'state'] = np.nan\n",
    "            continue\n",
    "\n",
    "        Phockeystick_i = AandB(ranges[index], np.where(info['lc'].t['chi/N']>=hockeystick_cut)[0])\n",
    "        Pextendedtail_i = AandB(ranges[index], AandB(np.where(info['lc'].t['chi/N']>=extendedtail_cut)[0], np.where(info['lc'].t['chi/N']<hockeystick_cut)[0]))\n",
    "        Pwellbehaved_i = AandB(ranges[index], np.where(info['lc'].t['chi/N']<extendedtail_cut)[0])\n",
    "\n",
    "        regionstates.t.loc[index,'Phockeystick'] = 100.0*len(Phockeystick_i)/len(ranges[index])\n",
    "        regionstates.t.loc[index,'Pextendedtail'] = 100.0*len(Pextendedtail_i)/len(ranges[index])\n",
    "        regionstates.t.loc[index,'Pwellbehaved'] = 100.0*len(Pwellbehaved_i)/len(ranges[index])\n",
    "        \n",
    "        if(regionstates.t.loc[index,'Phockeystick'] > hockeystick_cut_pcutoff):\n",
    "            regionstates.t.loc[index,'state'] = 'hockeystick'\n",
    "        elif(regionstates.t.loc[index,'Pextendedtail'] > extendedtail_cut_pcutoff):\n",
    "            regionstates.t.loc[index,'state'] = 'extended tail'\n",
    "        else:\n",
    "            regionstates.t.loc[index,'state'] = 'well-behaved'\n",
    "\n",
    "    return regionstates\n",
    "\n",
    "def get_region_table_loop(lc_info):\n",
    "    regionstates_allsne = pd.DataFrame(columns=['tnsname','classification','b_t0','b_t1','b_t2','sn_t0','sn_t1','sn_t2'])\n",
    "    \n",
    "    # prelim plots of distribution for percent cut\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "    et, hs, eths = axes.flatten()\n",
    "    plt.suptitle('Chi-Square Distribution for All SNe',fontsize=15, y=1)\n",
    "    et.set_title('Distribution for Extended Tail')\n",
    "    et.set_ylabel('Percent Data with %d<x2<%d' % (extendedtail_cut, hockeystick_cut))\n",
    "    et.set_xlabel('Percent Data with x2<%d' % extendedtail_cut)\n",
    "    hs.set_title('Distribution for Hockeystick')\n",
    "    hs.set_ylabel('Percent Data with x2>%d' % hockeystick_cut)\n",
    "    hs.set_xlabel('Percent Data with x2<%d' % extendedtail_cut)\n",
    "    eths.set_title('Distribution for Extended Tail and Hockeystick')\n",
    "    eths.set_xlabel('Percent Data with %d<x2<%d' % (extendedtail_cut, hockeystick_cut))\n",
    "    eths.set_ylabel('Percent Data with x2>%d' % hockeystick_cut)\n",
    "\n",
    "    hockeystick_counter = 0\n",
    "    extendedtail_counter = 0\n",
    "    wellbehaved_counter = 0\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        # get region table\n",
    "        regionstates = get_region_table(lc_info.t.loc[index])\n",
    "        print('SN %s MJD region states w.r.t. PSF chi-square values:' % (lc_info.t.loc[index,'tnsname'])) \n",
    "        print(regionstates.t.to_string(),'\\n')\n",
    "\n",
    "        ix = [1,2,3,5,6,7] # don't count repeat b and sn regions\n",
    "\n",
    "        hockeystick_counter += len(np.where(regionstates.t.loc[ix,'state']=='hockeystick')[0])\n",
    "        extendedtail_counter += len(np.where(regionstates.t.loc[ix,'state']=='extended tail')[0])\n",
    "        wellbehaved_counter += len(np.where(regionstates.t.loc[ix,'state']=='well-behaved')[0])\n",
    "\n",
    "        # plot\n",
    "        colors = get_region_table_plot_colors(regionstates.t)\n",
    "        et.scatter(regionstates.t.loc[ix,'Pwellbehaved'], regionstates.t.loc[ix,'Pextendedtail'], c=colors)\n",
    "        hs.scatter(regionstates.t.loc[ix,'Pwellbehaved'], regionstates.t.loc[ix,'Phockeystick'], c=colors)\n",
    "        eths.scatter(regionstates.t.loc[ix,'Pextendedtail'], regionstates.t.loc[ix,'Phockeystick'], c=colors)\n",
    "\n",
    "        # classify each region\n",
    "        regionstates_allsne.loc[index,'tnsname'] = lc_info.t.loc[index,'tnsname']\n",
    "        regionstates_allsne.loc[index,'classification'] = lc_info.t.loc[index,'classification']\n",
    "        for i in range(0,len(regionstates.t['region'])):\n",
    "            regionstates_allsne.loc[index,regionstates.t.loc[i,'region']] = regionstates.t.loc[i,'state']\n",
    "\n",
    "        # update lc_info\n",
    "        et_colname = 'Pextendedtail_'\n",
    "        hs_colname = 'Phockeystick_'\n",
    "        for i in range(0,4):\n",
    "            lc_info.t.loc[index, et_colname+regionstates.t.loc[i,'region']] = regionstates.t.loc[i,'Pextendedtail']\n",
    "            lc_info.t.loc[index, hs_colname+regionstates.t.loc[i,'region']] = regionstates.t.loc[i,'Phockeystick']\n",
    "    \n",
    "    print('Hockeystick ranges: %d\\nExtended tail ranges: %d\\nWell-behaved ranges: %d\\n' % (hockeystick_counter, extendedtail_counter, wellbehaved_counter))\n",
    "    \n",
    "    # legends for plots of distribution for percent cut\n",
    "    red = mpatches.Patch(color='red', label='Hockeystick')\n",
    "    yellow = mpatches.Patch(color='yellow', label='Extended tail')\n",
    "    green = mpatches.Patch(color='green', label='Well-behaved')\n",
    "    et.legend(handles=[red, yellow, green])\n",
    "    hs.legend(handles=[red, yellow, green])\n",
    "    eths.legend(handles=[red, yellow, green])\n",
    "\n",
    "    return regionstates_allsne, lc_info\n",
    "\n",
    "regionstates_allsne = pdastrostatsclass(columns=['tnsname','classification','b_t0','b_t1','b_t2','sn_t0','sn_t1','sn_t2'])\n",
    "\n",
    "print('Template changes at %d and %d MJD' % (tchange1,tchange2))\n",
    "print('PSF chi-square cut for hockeystick classification: %d; PSF chi-square cut for extended tail classification: %d' % (hockeystick_cut,extendedtail_cut))\n",
    "print('PSF chi-square hockeystick cut percent cutoff: %0.1f%%; PSF chi-square extended tail cut percent cutoff: %0.1f%%\\n' % (hockeystick_cut_pcutoff, extendedtail_cut_pcutoff))\n",
    "regionstates_allsne.t, lc_info = get_region_table_loop(lc_info)\n",
    "print(lc_info.t)\n",
    "regionstates_allsne.t.style.applymap(get_regionstates_allsne_table_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Examine each region defined by the ATLAS template changes and its chi-squares in relation to S/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each well-behaved baseline region of all SNe, 3 histograms.\n",
    "# histogram 1: S/N of all measurements with x2>hockeystick_cut.\n",
    "# histogram 2: S/N of all measurements with hockeystick_cut>x2>extendedtail_cut.\n",
    "# histogram 3: S/N of all measurements with extendedtail_cut>x2.\n",
    "# repeat for each extended tail baseline region and for each hockeystick baseline region.\n",
    "\n",
    "def get_state_expected_n(state):\n",
    "    counter = 0\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        regionstates = get_region_table(lc_info.t.loc[index])\n",
    "        for b_region_i in range(0,3):\n",
    "            if regionstates.t.loc[b_region_i,'state'] == state:\n",
    "                counter += regionstates.t.loc[b_region_i,'n']\n",
    "    return counter\n",
    "\n",
    "def get_state_x2_expected_n(state, x2_lowlim=None, x2_uplim=None):\n",
    "    counter = 0\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        regionstates = get_region_table(lc_info.t.loc[index])\n",
    "        for b_region_i in range(0,3):\n",
    "            if regionstates.t.loc[b_region_i,'state'] == state:\n",
    "                s = regionstates.t.loc[b_region_i,'start_index']\n",
    "                e = regionstates.t.loc[b_region_i,'end_index']\n",
    "                if not(x2_lowlim is None) and not(x2_uplim is None):\n",
    "                    counter += len(AandB(np.where(lc_info.t.loc[index,'lc'].t.loc[range(s,e+1),'chi/N']>= x2_lowlim)[0], np.where(lc_info.t.loc[index,'lc'].t.loc[range(s,e+1),'chi/N']<= x2_uplim)[0]))\n",
    "                elif not(x2_lowlim is None):\n",
    "                    counter += len(np.where(lc_info.t.loc[index,'lc'].t.loc[range(s,e+1),'chi/N']>= x2_lowlim)[0])\n",
    "                elif not(x2_uplim is None):\n",
    "                    counter += len(np.where(lc_info.t.loc[index,'lc'].t.loc[range(s,e+1),'chi/N']<= x2_uplim)[0])\n",
    "    return counter\n",
    "\n",
    "def get_state_baseline_allsne(state):\n",
    "    state_baseline_allsne = pdastrostatsclass()\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        #print('SN ', lc_info.t.loc[index,'tnsname'])\n",
    "        regionstates = get_region_table(lc_info.t.loc[index])\n",
    "        #print(regionstates.t.to_string())\n",
    "        # for each baseline region, check if state matches and if so add to state_baseline_allsne table\n",
    "        for b_region_i in range(0,3):\n",
    "            #print(b_region_i,regionstates.t.loc[b_region_i,'state'])\n",
    "            if regionstates.t.loc[b_region_i,'state'] == state:\n",
    "                #print('passed, adding: ', regionstates.t.loc[b_region_i,'start_index'], regionstates.t.loc[b_region_i,'end_index'])\n",
    "                # add measurements within that region to state_baseline_allsne table\n",
    "                state_baseline_allsne.t = pd.concat([state_baseline_allsne.t, \n",
    "                                                    lc_info.t.loc[index,'lc'].t.loc[range(regionstates.t.loc[b_region_i,'start_index'], 1+regionstates.t.loc[b_region_i,'end_index'])]], \n",
    "                                                    ignore_index=True)\n",
    "    state_baseline_allsne.t['uJy/duJy'] = state_baseline_allsne.t['uJy']/state_baseline_allsne.t['duJy']\n",
    "    state_baseline_allsne.t = state_baseline_allsne.t.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    return state_baseline_allsne\n",
    "\n",
    "def plot_stn_hist(state, state_baseline_allsne, hist, x2_lowlim=None, x2_uplim=None):\n",
    "    indices = state_baseline_allsne.ix_inrange(colnames=['chi/N'], lowlim=x2_lowlim, uplim=x2_uplim)\n",
    "    print('Expected %d indices and found %d indices for chi-square range ' % (get_state_x2_expected_n(state,x2_lowlim,x2_uplim), len(indices)), x2_lowlim, '-', x2_uplim)\n",
    "\n",
    "    #print(state_baseline_allsne.t.loc[indices, 'uJy/duJy'].to_string())\n",
    "    hist.hist(state_baseline_allsne.t.loc[indices, 'uJy/duJy'], bins=30) #, range=(0,20))\n",
    "    \n",
    "    title = 'For data with '\n",
    "    if not(x2_uplim is None): title += '%0.1f>' % x2_uplim\n",
    "    title += 'x2'\n",
    "    if not(x2_lowlim is None): title += '>%0.1f' % x2_lowlim\n",
    "    hist.set_title(title)\n",
    "    hist.set_xlabel('uJy/duJy')\n",
    "\n",
    "def plot_region_stn_hist(state):\n",
    "    print('\\nPlotting S/N Histograms for %s MJD Regions' % state.title())\n",
    "    state_baseline_allsne = get_state_baseline_allsne(state)\n",
    "    #print(state_baseline_allsne.t)\n",
    "    print('Expected indices for this state: %d' % get_state_expected_n(state))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "    plt.suptitle('S/N Frequencies for Chi-Square Ranges in %s MJD Regions' % state.title(), fontsize=15, y=1)\n",
    "    hs, et, wb = axes.flatten()\n",
    "\n",
    "    plot_stn_hist(state, state_baseline_allsne, hs, x2_lowlim=hockeystick_cut)\n",
    "    plot_stn_hist(state, state_baseline_allsne, et, x2_lowlim=extendedtail_cut, x2_uplim=hockeystick_cut)\n",
    "    plot_stn_hist(state, state_baseline_allsne, wb, x2_uplim=extendedtail_cut)\n",
    "\n",
    "\n",
    "plot_region_stn_hist('well-behaved') \n",
    "plot_region_stn_hist('extended tail')\n",
    "plot_region_stn_hist('hockeystick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Recalculate cut limits for each SN based on contamination and other factors (to potentially replace Step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the contamination limit (contamination = Nbad,kept/Nkept must be <= contam_lim%):\n",
    "global contam_lim\n",
    "contam_lim = 10.0\n",
    "\n",
    "# Enter the loss limit (loss = Ngood,cut/Ngood must be >= loss_lim%):\n",
    "global loss_lim\n",
    "loss_lim = 10.0\n",
    "\n",
    "# Enter the limit to prioritize (must be 'loss_lim' or 'contam_lim', based on which is more important):\n",
    "global lim_to_prioritize\n",
    "lim_to_prioritize = 'contam_lim'\n",
    "\n",
    "# Enter the parameters for the chi-square cut (minimum cut, maximum cut, and step):\n",
    "global cut_start\n",
    "global cut_stop\n",
    "global cut_step\n",
    "cut_start = 5 # this is inclusive\n",
    "cut_stop = 50 # this is inclusive\n",
    "cut_step = 1\n",
    "\n",
    "# Enter the signal-to-noise cut that will determine a \"good\" measurement from\n",
    "# a \"bad\" measurement:\n",
    "global stn_cut\n",
    "stn_cut = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. static cut at x2=50; get percent cut before and after mjdstart\n",
    "# 2. for different x2 cuts decreasing from cut_stop and cut at stn_cut\n",
    "#    and for only baseline calculate contam% and loss%\n",
    "\n",
    "def plot_lim_cuts(info, lim_cuts, contam_lim_cut, loss_lim_cut):\n",
    "    plt.figure()\n",
    "    plt.title('SN '+info['tnsname']+' Dynamic PSF Chi-Square Cut (Baseline Only)')\n",
    "    plt.axhline(linewidth=1,color='k')\n",
    "    plt.axvline(linewidth=1,color='k')\n",
    "    plt.xlabel('PSF Chi-Square Cut')\n",
    "    plt.ylabel('% of Measurements')\n",
    "\n",
    "    plt.axhline(loss_lim,linewidth=1,color='r',linestyle='--',label='Loss Limit')\n",
    "    sp, loss, dloss = dataPlot(lim_cuts.t['PSF Chi-Square Cut'], lim_cuts.t['Loss'])\n",
    "    matlib.setp(loss,ms=5,color='r',marker='o',label='Loss')\n",
    "    plt.axvline(x=loss_lim_cut,color='r',label='Loss Cut')\n",
    "    plt.axvspan(loss_lim_cut, 50, alpha=0.2, color='r')\n",
    "\n",
    "    plt.axhline(contam_lim,linewidth=1,color='g',linestyle='--',label='Contamination Limit')\n",
    "    sp, contam, dcontam = dataPlot(lim_cuts.t['PSF Chi-Square Cut'], lim_cuts.t['Contamination'])\n",
    "    matlib.setp(contam,ms=5,color='g',marker='o',label='Contamination')\n",
    "    plt.axvline(x=contam_lim_cut,color='g',label='Contamination Cut')\n",
    "    plt.axvspan(5, contam_lim_cut, alpha=0.2, color='g')\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "def get_lim_cuts(lc_info, index, lim_cuts):\n",
    "    contam_lim_cut = None\n",
    "    loss_lim_cut = None\n",
    "\n",
    "    sortby_loss = lim_cuts.t.iloc[(lim_cuts.t['Loss']).argsort()].reset_index()\n",
    "    min_loss = sortby_loss.loc[0,'Loss']\n",
    "    max_loss = sortby_loss.loc[len(sortby_loss)-1,'Loss']\n",
    "    # if all loss below lim, loss_lim_cut is min cut\n",
    "    if min_loss < loss_lim and max_loss < loss_lim:\n",
    "        lc_info.t.loc[index,'loss_case'] = 'below lim'\n",
    "        loss_lim_cut = lim_cuts.t.loc[0,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all loss above lim, loss_lim_cut is min cut with min% loss\n",
    "        if min_loss > loss_lim and max_loss > loss_lim:\n",
    "            lc_info.t.loc[index,'loss_case'] = 'above lim'\n",
    "            a = np.where(lim_cuts.t['Loss'] == min_loss)[0]\n",
    "        # else if loss crosses lim at some point, loss_lim_cut is min cut with max% loss <= loss_lim\n",
    "        else:\n",
    "            lc_info.t.loc[index,'loss_case'] = 'crosses lim'\n",
    "            valid_cuts = sortby_loss[sortby_loss['Loss'] <= loss_lim]\n",
    "            a = np.where(lim_cuts.t['Loss'] == valid_cuts.loc[len(valid_cuts)-1,'Loss'])[0]\n",
    "        # sort those indices by cuts\n",
    "        b = lim_cuts.t.iloc[a]\n",
    "        c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "        loss_lim_cut = c.loc[0,'PSF Chi-Square Cut']\n",
    "\n",
    "    sortby_contam = lim_cuts.t.iloc[(lim_cuts.t['Contamination']).argsort()].reset_index()\n",
    "    min_contam = sortby_contam.loc[0,'Contamination']\n",
    "    max_contam = sortby_contam.loc[len(sortby_contam)-1,'Contamination']\n",
    "    # if all contam below lim, contam_lim_cut is max cut\n",
    "    if min_contam < contam_lim and max_contam < contam_lim:\n",
    "        lc_info.t.loc[index,'contam_case'] = 'below lim'\n",
    "        contam_lim_cut = lim_cuts.t.loc[len(lim_cuts.t)-1,'PSF Chi-Square Cut']\n",
    "    else:\n",
    "        # else if all contam above lim, contam_lim_cut is max cut with min% contam\n",
    "        if min_contam > contam_lim and max_contam > contam_lim:\n",
    "            lc_info.t.loc[index,'contam_case'] = 'above lim'\n",
    "            a = np.where(lim_cuts.t['Contamination'] == min_contam)[0]\n",
    "        # else if contam crosses lim at some point, contam_lim_cut is max cut with max% contam <= contam_lim\n",
    "        else:\n",
    "            lc_info.t.loc[index,'contam_case'] = 'crosses lim'\n",
    "            valid_cuts = sortby_contam[sortby_contam['Contamination'] <= contam_lim]\n",
    "            a = np.where(lim_cuts.t['Contamination'] == valid_cuts.loc[len(valid_cuts)-1,'Contamination'])[0]\n",
    "        # sort those indices by cuts\n",
    "        b = lim_cuts.t.iloc[a]\n",
    "        c = b.iloc[(b['PSF Chi-Square Cut']).argsort()].reset_index()\n",
    "        contam_lim_cut = c.loc[len(c)-1,'PSF Chi-Square Cut']\n",
    "\n",
    "    return lc_info, contam_lim_cut, loss_lim_cut\n",
    "\n",
    "def choose_btwn_lim_cuts(info, loss_lim_cut, contam_lim_cut):\n",
    "    # case 1 and 1: final_cut = 5\n",
    "    # case 1 and 2: take limit of case 2\n",
    "    # case 1 and 3: take limit of case 3\n",
    "    # case 2 and 2: print lims don't work\n",
    "    # case 2 and 3: choose_btwn_lim_cuts\n",
    "    # case 3 and 3: choose_btwn_lim_cuts\n",
    "\n",
    "    case1 = info['loss_case'] == 'below lim' or info['contam_case'] == 'below lim'\n",
    "    case2 = info['loss_case'] == 'above lim' or info['contam_case'] == 'above lim'\n",
    "    case3 = info['loss_case'] == 'crosses lim' or info['contam_case'] == 'crosses lim'\n",
    "\n",
    "    final_cut = None\n",
    "    if case1 and not case2 and not case3: # 1 and 1\n",
    "        final_cut = 5\n",
    "    elif case1: # 1\n",
    "        if case2: # and 2\n",
    "            final_cut = loss_lim_cut if info['loss_case'] == 'above lim' else contam_lim_cut\n",
    "        else: # and 3\n",
    "            final_cut = loss_lim_cut if info['loss_case'] == 'crosses lim' else contam_lim_cut\n",
    "    elif case2 and not case3: # 2 and 2\n",
    "        print('ERROR: chi-square loss_lim_cut >= %d and contam_lim_cut <= %d both fall above limits %0.2f%% and %0.2f%%! Try setting less strict limits. Setting final cut to nan.' % (loss_lim_cut, contam_lim_cut, loss_lim, contam_lim))\n",
    "        final_cut = np.nan\n",
    "    else: # 2 and 3 or 3 and 3\n",
    "        if loss_lim_cut > contam_lim_cut:\n",
    "            print('WARNING: chi-square loss_lim_cut >= %d and contam_lim_cut <= %d do not overlap! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %d...' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %d... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "        else:\n",
    "            print('Valid chi-square cut range from %d to %d! ' % (loss_lim_cut, contam_lim_cut))\n",
    "            if lim_to_prioritize == 'contam_lim':\n",
    "                print('Prioritizing %s and setting to %d... ' % (lim_to_prioritize, loss_lim_cut))\n",
    "                final_cut = loss_lim_cut\n",
    "            else:\n",
    "                print('Prioritizing %s and setting to %d... ' % (lim_to_prioritize, contam_lim_cut))\n",
    "                final_cut = contam_lim_cut\n",
    "    return final_cut\n",
    "\n",
    "def get_lim_cuts_table(info, indices=None):\n",
    "    print('\\nCutting %s...' % info['tnsname'])\n",
    "    print('abs(uJy/duJy) cut at: %0.2f \\nx2 cut from %0.2f to %0.2f inclusive, with step size %d' % (stn_cut,cut_start,cut_stop,cut_step))\n",
    "\n",
    "    lim_cuts = pdastrostatsclass(columns=['PSF Chi-Square Cut', 'N', 'Ngood', 'Nbad', 'Nkept', 'Ncut', 'Ngood,kept', 'Ngood,cut', 'Nbad,kept', 'Nbad,cut',\n",
    "                                        'Pgood,kept', 'Pgood,cut', 'Pbad,kept', 'Pbad,cut', 'Ngood,kept/Ngood', 'Loss', 'Contamination'])\n",
    "\n",
    "    if indices is None:\n",
    "        indices = np.where(info['lc'].t['MJD'] < info['mjdstart'])[0]\n",
    "    \n",
    "    # static cut at x2=50\n",
    "    x2cut_50 = np.where(info['lc'].t['chi/N'] < 50)[0]\n",
    "    print('Static chi square cut at 50: %0.2f%% cut for baseline' % (100*len(AnotB(indices,x2cut_50))/len(indices)))\n",
    "\n",
    "    # good baseline measurement indices\n",
    "    b_stncut_i = AandB(indices, np.where(abs(info['lc'].t['uJy/duJy']) < stn_cut)[0])\n",
    "    # for different x2 cuts decreasing from 50 and cut at abs(s/n)=stn_cut\n",
    "    for cut in range(cut_start,cut_stop+1,cut_step):\n",
    "        # kept baseline measurement indices\n",
    "        b_x2cut_i = AandB(indices, np.where(info['lc'].t['chi/N'] < cut)[0])\n",
    "\n",
    "        df = pd.DataFrame([[cut,\n",
    "                            len(indices), # N\n",
    "                            len(b_stncut_i), # Ngood\n",
    "                            len(AnotB(indices,b_stncut_i)), # Nbad\n",
    "                            len(b_x2cut_i), # Nkept\n",
    "                            len(AnotB(indices,b_x2cut_i)), # Ncut\n",
    "                            len(AandB(b_stncut_i,b_x2cut_i)), # Ngood,kept\n",
    "                            len(AnotB(b_stncut_i,b_x2cut_i)), # Ngood,cut\n",
    "                            len(AnotB(b_x2cut_i,b_stncut_i)), # Nbad,kept\n",
    "                            len(AandB(AnotB(indices,b_stncut_i),AnotB(indices,b_x2cut_i))), # Nbad,cut\n",
    "                            100*len(AandB(b_stncut_i,b_x2cut_i))/len(indices), # Ngood,kept/Nbaseline\n",
    "                            100*len(AnotB(b_stncut_i,b_x2cut_i))/len(indices), # Ngood,cut/Nbaseline \n",
    "                            100*len(AnotB(b_x2cut_i,b_stncut_i))/len(indices), # Nbad,kept/Nbaseline\n",
    "                            100*len(AandB(AnotB(indices,b_stncut_i),AnotB(indices,b_x2cut_i)))/len(indices), # Nbad,cut/Nbaseline\n",
    "                            100*len(AandB(b_stncut_i,b_x2cut_i))/len(b_stncut_i), # Ngood,kept/Ngood\n",
    "                            100*len(AnotB(b_stncut_i,b_x2cut_i))/len(b_stncut_i), # Ngood,cut/Ngood = Loss\n",
    "                            100*len(AnotB(b_x2cut_i,b_stncut_i))/len(b_x2cut_i)]], # Nbad,kept/Nkept = Contamination\n",
    "                            columns=['PSF Chi-Square Cut', 'N', 'Ngood', 'Nbad', 'Nkept', 'Ncut', 'Ngood,kept', 'Ngood,cut', 'Nbad,kept', 'Nbad,cut',\n",
    "                                        'Pgood,kept', 'Pgood,cut', 'Pbad,kept', 'Pbad,cut', 'Ngood,kept/Ngood', 'Loss', 'Contamination'])\n",
    "        lim_cuts.t = pd.concat([lim_cuts.t,df],ignore_index=True)\n",
    "    return lim_cuts\n",
    "\n",
    "def get_lim_cuts_loop(lc_info):\n",
    "    for index in range(0,len(lc_info.t)):\n",
    "        lim_cuts = get_lim_cuts_table(lc_info.t.loc[index])\n",
    "        print(lim_cuts.t.to_string())\n",
    "\n",
    "        lc_info, contam_lim_cut, loss_lim_cut = get_lim_cuts(lc_info, index, lim_cuts)\n",
    "        print('Contamination cut according to given contam_limit: ',contam_lim_cut,'\\nLoss cut according to given loss_limit: ',loss_lim_cut)\n",
    "        final_cut = choose_btwn_lim_cuts(lc_info.t.loc[index], loss_lim_cut, contam_lim_cut)\n",
    "        if np.isnan(final_cut):\n",
    "            print('Final chi-square cut could not be determined.')\n",
    "        else:\n",
    "            print('Final chi-square cut is %d, with %0.2f%% contamination and %0.2f%% loss.' % (final_cut, lim_cuts.t.loc[np.where(lim_cuts.t['PSF Chi-Square Cut']==final_cut)[0][0],'Contamination'], lim_cuts.t.loc[np.where(lim_cuts.t['PSF Chi-Square Cut']==final_cut)[0][0],'Loss']))\n",
    "\n",
    "        lc_info.t.loc[index,'contam_lim_cut'] = contam_lim_cut\n",
    "        lc_info.t.loc[index,'loss_lim_cut'] = loss_lim_cut\n",
    "        lc_info.t.loc[index,'final_cut'] = final_cut\n",
    "\n",
    "        plot_lim_cuts(lc_info.t.loc[index], lim_cuts, contam_lim_cut, loss_lim_cut)\n",
    "\n",
    "    return lc_info\n",
    "\n",
    "if lim_to_prioritize != 'loss_lim' and lim_to_prioritize != 'contam_lim':\n",
    "    print(\"ERROR: lim_to_prioritize must be 'loss_lim' or 'contam_lim'!\")\n",
    "    sys.exit()\n",
    "\n",
    "lc_info.t['contam_lim_cut'] = None\n",
    "lc_info.t['loss_lim_cut'] = None\n",
    "\n",
    "lc_info = get_lim_cuts_loop(lc_info)\n",
    "print('\\n',lc_info.t.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Plot Pextendedtail and Phockeystick against Contamination and Ngood,cut/Ngood Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "et, hs = axes.flatten()\n",
    "plt.suptitle('Chi-Square Limits and Regions',fontsize=15, y=1)\n",
    "et.set_title('Chi-Square Limits for Extended Tail Regions')\n",
    "et.set_xlabel('Percent Data with %d<x2<%d' % (extendedtail_cut, hockeystick_cut))\n",
    "et.set_ylabel('Chi-Square Cut')\n",
    "hs.set_title('Chi-Square Limits for Hockeystick Regions')\n",
    "hs.set_xlabel('Percent Data with x2>%d' % hockeystick_cut)\n",
    "hs.set_ylabel('Chi-Square Cut')\n",
    "\n",
    "for index in range(0,len(lc_info.t)):\n",
    "    et.scatter(lc_info.t.loc[index,'Pextendedtail_b'], lc_info.t.loc[index,'final_cut'], color='b')\n",
    "    hs.scatter(lc_info.t.loc[index,'Phockeystick_b'], lc_info.t.loc[index,'final_cut'], color='b')\n",
    "    \n",
    "    \"\"\"\n",
    "    regionstates = get_region_table(lc_info.t.loc[index])\n",
    "    print(regionstates.t.to_string())\n",
    "\n",
    "    for j in [1,2,3]:\n",
    "        if np.isnan(regionstates.t.loc[j,'start_index']) or np.isnan(regionstates.t.loc[j,'end_index']):\n",
    "            continue\n",
    "\n",
    "        print('Getting limits for regions from i=%d to i=%d...' % (regionstates.t.loc[j,'start_index'], regionstates.t.loc[j,'end_index']))\n",
    "        lim_cuts = get_lim_cuts_table(lc_info.t.loc[index], indices=range(regionstates.t.loc[j,'start_index'],regionstates.t.loc[j,'end_index']+1))\n",
    "        contam_lim_cut, loss_lim_cut = get_lim_cuts(lim_cuts)\n",
    "        print('Contamination cut according to given contam_limit: ',contam_lim_cut,'\\nLoss cut according to given loss_limit: ',loss_lim_cut)\n",
    "        final_cut = choose_btwn_lim_cuts(loss_lim_cut, contam_lim_cut)\n",
    "        print('Final cut: ',final_cut)\n",
    "\n",
    "        et.scatter(regionstates.t.loc[j,'Pextendedtail'], final_cut, color='b')\n",
    "        hs.scatter(regionstates.t.loc[j,'Phockeystick'], final_cut, color='b')\n",
    "    \"\"\"\n",
    "    \n",
    "\"\"\"\n",
    "red = mpatches.Patch(color='red', label='Contamination Cut')\n",
    "green = mpatches.Patch(color='green', label='Loss Cut')\n",
    "et.legend(handles=[red, green])\n",
    "hs.legend(handles=[red, green])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
